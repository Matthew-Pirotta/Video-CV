{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import imageio.v3 as iio # pip install imageio[ffmpeg]\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996542c1",
   "metadata": {},
   "source": [
    "## 2. Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64824c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(video_path, frame_interval=10, display_frames=True):\n",
    "    # Load frames from a video file at specified intervals.\n",
    "\n",
    "    frame_count = 0\n",
    "    frames = []  # List to store frames for visualization\n",
    "\n",
    "    try:\n",
    "        print(f\"Opening video file: {video_path}\")\n",
    "        \n",
    "        # Iterate through frames in the video file\n",
    "        for frame in iio.imiter(video_path):\n",
    "            if frame_count % frame_interval == 0:\n",
    "                frames.append(frame)  # Store frame in list\n",
    "                \n",
    "                # Display frame if display_frames is True\n",
    "                if display_frames:\n",
    "                    plt.imshow(frame)\n",
    "                    plt.title(f'Frame {frame_count}')\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                \n",
    "            frame_count += 1\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Video file not found at {video_path}\")\n",
    "        return [], 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the video: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "    print(f\"\\nFinished processing video.\")\n",
    "    print(f\"Total frames iterated: {frame_count}.\")\n",
    "    \n",
    "    return frames\n",
    "\n",
    "frames = load_video_frames('../videos/bedroomvideo.mp4', frame_interval=10, display_frames=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679cd5a",
   "metadata": {},
   "source": [
    "## 5. Essential/ Fundamental Matrix Computation\n",
    "\n",
    "Both matrices describe the epipolar geometry between two images of the same scene taken from different viewpoints. Epipolar geometry defines the constraint that corresponding points must satisfy. If you have a point x in the first image, its corresponding point x' in the second image must lie on a specific line called the epiline. Both matrices capture this relationship, but they differ based on camera calibration.\n",
    "\n",
    "The Fundamental Matrix acts as a bridge between the 2D pixel coordinates of corresponding points in two different images, capturing the geometric constraints imposed by the 3D scene and the camera positions, even when you don't know the camera's exact internal details. It's a fundamental tool for tasks like finding correct feature matches, estimating camera motion, and ultimately reconstructing the 3D scene.\n",
    "\n",
    "- Fundamental Matrix (F):\n",
    "  - **What it is**: A 3x3 matrix that relates corresponding points between two images in pixel coordinates.\n",
    "  - **Information encoded**: It contains information about the camera's relative rotation and translation (extrinsic parameters) and the intrinsic parameters (like focal length, principal point) of both cameras.\n",
    "  - **Equation**: It satisfies the epipolar constraint: x'^T * F * x = 0, where x and x' are the homogeneous coordinates of the matching points in pixels.\n",
    "  - **When to use**: Use the Fundamental Matrix when the cameras are uncalibrated, meaning you don't know their intrinsic parameters.\n",
    "  - **Computation**: Typically requires at least 8 pairs of corresponding points (using the 8-point algorithm) or 7 pairs (7-point algorithm). OpenCV's function often uses robust methods like RANSAC or LMedS which handle outliers well using many more points.\n",
    "\n",
    "If you are recording video with a phone and have not performed a specific camera calibration procedure to find its intrinsic matrix (K), then using the Fundamental Matrix (F) is the recommended approach. You treat the camera as uncalibrated.\n",
    "\n",
    "- Essential Matrix (E):\n",
    "  - **What it is**: A 3x3 matrix that relates corresponding points between two images in normalized image coordinates (independent of camera intrinsics).\n",
    "  - **Information encoded**: It contains only information about the camera's relative rotation (R) and translation (t), up to a scale factor. It does not include camera intrinsic information.\n",
    "  - **Equation**: It satisfies the epipolar constraint in normalized coordinates: x_norm'^T * E * x_norm = 0.\n",
    "  - **When to use**: Use the Essential Matrix when the cameras are calibrated, meaning you know their intrinsic parameters (focal length, principal point - often represented in a camera matrix K).\n",
    "  - **Computation**: Requires at least 5 pairs of corresponding points (using the 5-point algorithm), though robust methods in OpenCV use more points.\n",
    "  - **Relation to F**: E = K'^T * F * K, where K and K' are the camera intrinsic matrices for the two views. If the camera is the same for both views, K' = K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fef4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fundamental_matrix(pts1, pts2, method=cv2.FM_RANSAC, ransacReprojThreshold=1.0, confidence=0.999, maxIters=2000):\n",
    "    \"\"\"\n",
    "    Computes the Fundamental Matrix (F) and filters inliers between two sets of points.\n",
    "\n",
    "    Args:\n",
    "        pts1 (np.ndarray): NumPy array of N points from image 1, shape (N, 1, 2), dtype=float32/64.\n",
    "        pts2 (np.ndarray): NumPy array of N points from image 2, shape (N, 1, 2), dtype=float32/64.\n",
    "        method (int, optional): Method for computation (e.g., cv2.FM_RANSAC, cv2.FM_LMEDS).\n",
    "                                Defaults to cv2.FM_RANSAC.\n",
    "        ransacReprojThreshold (float, optional): RANSAC reprojection threshold (pixels). Defaults to 1.0.\n",
    "        confidence (float, optional): RANSAC confidence level. Defaults to 0.999.\n",
    "        maxIters (int, optional): Maximum RANSAC iterations. Defaults to 2000.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - F (np.ndarray or None): The computed 3x3 Fundamental Matrix, or None if failed.\n",
    "            - pts1_inliers (np.ndarray or None): Inlier points from image 1, or None if failed.\n",
    "            - pts2_inliers (np.ndarray or None): Inlier points from image 2, or None if failed.\n",
    "            - status_mask (np.ndarray or None): The mask indicating inliers (1) and outliers (0).\n",
    "    \"\"\"\n",
    "    if pts1 is None or pts2 is None or len(pts1) < 8 or len(pts2) < 8:\n",
    "        print(\"Error: Not enough points to compute Fundamental Matrix.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    try:\n",
    "        # Compute the Fundamental Matrix using the chosen method\n",
    "        F, mask = cv2.findFundamentalMat(pts1, pts2,\n",
    "                                         method=method,\n",
    "                                         ransacReprojThreshold=ransacReprojThreshold,\n",
    "                                         confidence=confidence,\n",
    "                                         maxIters=maxIters)\n",
    "\n",
    "        if F is None or mask is None:\n",
    "            print(\"Warning: findFundamentalMat returned None.\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        # Filter points using the mask to get inliers\n",
    "        pts1_inliers = pts1[mask.ravel() == 1]\n",
    "        pts2_inliers = pts2[mask.ravel() == 1]\n",
    "        num_inliers = len(pts1_inliers)\n",
    "        num_total = len(pts1)\n",
    "\n",
    "        print(f\"Fundamental Matrix computed. Inliers: {num_inliers} / {num_total}\")\n",
    "\n",
    "        return F, pts1_inliers, pts2_inliers, mask\n",
    "\n",
    "    except cv2.error as e:\n",
    "        print(f\"OpenCV Error during findFundamentalMat: {e}\")\n",
    "        return None, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40336fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code\n",
    "# 1. Assume you have your matched points (replace with your actual data)\n",
    "num_points = 150\n",
    "pts1_matched = np.random.rand(num_points, 1, 2).astype(np.float32) * 500\n",
    "translation_demo = np.array([30, 10], dtype=np.float32).reshape(1, 1, 2)\n",
    "noise_demo = np.random.randn(num_points, 1, 2).astype(np.float32) * 3\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(num_points, size=int(num_points * 0.2), replace=False)\n",
    "pts2_matched = pts1_matched + translation_demo + noise_demo\n",
    "pts2_matched[outlier_indices] = np.random.rand(len(outlier_indices), 1, 2).astype(np.float32) * 500\n",
    "\n",
    "# 2. Call the function\n",
    "fundamental_matrix, pts1_inliers, pts2_inliers, inlier_mask = compute_fundamental_matrix(\n",
    "    pts1_matched,\n",
    "    pts2_matched,\n",
    "    ransacReprojThreshold=1.5, # Example threshold adjustment\n",
    "    confidence=0.99\n",
    ")\n",
    "\n",
    "# 3. Check the result and use the outputs\n",
    "if fundamental_matrix is not None:\n",
    "    print(\"Successfully computed F and filtered inliers.\")\n",
    "    print(f\"Shape of pts1_inliers: {pts1_inliers.shape}\")\n",
    "    print(f\"Shape of pts2_inliers: {pts2_inliers.shape}\")\n",
    "\n",
    "    # Now you can pass 'fundamental_matrix', 'pts1_inliers', 'pts2_inliers'\n",
    "    # to the next stage (e.g., camera pose estimation)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to compute Fundamental Matrix or find sufficient inliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c28f1",
   "metadata": {},
   "source": [
    "## 6. Camera Pose Estimation\n",
    "\n",
    "The goal of this stage is to determine the relative motion between the two camera views where you matched features. This motion is described by:\n",
    "\n",
    "- **Rotation (R)**: A 3x3 matrix describing how the camera orientation changed between the two shots.\n",
    "- **Translation (t)**: A 3x1 vector describing how the camera position changed between the two shots. Note that the translation vector t can only be determined up to a scale factor. This means you know the direction of motion but not the absolute distance moved.\n",
    "\n",
    "- Input Matrix: cv2.recoverPose technically requires the Essential Matrix (E) as its main input, not the Fundamental Matrix (F). It also needs the corresponding inlier points from the previous stage (pts1_inliers, pts2_inliers) and the camera intrinsic matrix (K).\n",
    "- Handling the Uncalibrated Case (Starting with F): Since you computed F (because the phone camera was treated as uncalibrated), you need a way to get E to use recoverPose. The relationship is E = K^T * F * K. But K is unknown!\n",
    "  - The Workaround: You need to assume a plausible K matrix. A common approach is:\n",
    "Set the principal point (cx, cy) to the image center (e.g., width/2, height/2).\n",
    "  - Estimate or guess the focal length (fx, fy). Sometimes fx = fy = image_width is used as a starting guess, or a typical value for phone cameras (e.g., 500-1000 pixels) might be assumed.\n",
    "  - Compute E: Calculate E = K_assumed.T @ F @ K_assumed.\n",
    "  - Use recoverPose: Call cv2.recoverPose using this computed E, the same assumed K, and your inlier points.\n",
    "  - Important: You must document this assumption about K in your report. The resulting pose (especially translation t) will be relative to the scale defined by your assumed K and F.\n",
    "- Chirality Problem: Mathematically, decomposing the E matrix yields four possible solutions for the rotation (R) and translation (t). However, only one of these solutions is physically correct – the one where the reconstructed 3D points lie in front of both cameras.\n",
    "  - How recoverPose Solves It: The cv2.recoverPose function handles this automatically! It takes your inlier points (pts1_inliers, pts2_inliers) and the K matrix, triangulates the points for each of the four possible (R, t) combinations, and counts how many points end up in front of both camera views. It then returns the R and t corresponding to the hypothesis with the most positive depth points, effectively resolving the chirality ambiguity. Your report should explain this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814312b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_camera_pose(pts1, pts2, F, K_assumed):\n",
    "    \"\"\"\n",
    "    Estimates the relative camera pose (Rotation and Translation) from the\n",
    "    Fundamental Matrix (F) and corresponding inlier points using cv2.recoverPose.\n",
    "\n",
    "    Args:\n",
    "        pts1 (np.ndarray): Inlier points from image 1, shape (N, 1, 2) or (N, 2).\n",
    "        pts2 (np.ndarray): Inlier points from image 2, shape (N, 1, 2) or (N, 2).\n",
    "        F (np.ndarray): The 3x3 Fundamental Matrix computed previously.\n",
    "        K_assumed (np.ndarray): The assumed 3x3 camera intrinsic matrix.\n",
    "                           Crucial for converting F to E and for chirality check.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - R (np.ndarray or None): The 3x3 estimated Rotation matrix, or None if failed.\n",
    "            - t (np.ndarray or None): The 3x1 estimated Translation vector (up to scale), or None if failed.\n",
    "            - points_valid_mask (np.ndarray or None): Mask indicating points used by recoverPose for the final check.\n",
    "    \"\"\"\n",
    "    if F is None or K_assumed is None or pts1 is None or pts2 is None or len(pts1) < 5:\n",
    "         print(\"Error: Insufficient input for pose estimation.\")\n",
    "         return None, None, None\n",
    "\n",
    "    # Ensure points are in the correct shape (N, 2) for recoverPose input consistency check\n",
    "    # Although recoverPose can sometimes handle (N, 1, 2), ensuring (N, 2) is safer.\n",
    "    pts1_rp = np.reshape(pts1, (-1, 2)).astype(np.float64)\n",
    "    pts2_rp = np.reshape(pts2, (-1, 2)).astype(np.float64)\n",
    "\n",
    "    try:\n",
    "        # 1. Compute Essential Matrix from F and assumed K\n",
    "        E = K_assumed.T @ F @ K_assumed\n",
    "        print(\"Computed Essential Matrix (E) from F and assumed K.\")\n",
    "        # print(\"E:\\n\", E) # Uncomment to view\n",
    "\n",
    "        # 2. Recover Pose (R, t) using cv2.recoverPose\n",
    "        # This function internally handles the chirality check using the points and K\n",
    "        points, R, t, mask_pose = cv2.recoverPose(E, pts1_rp, pts2_rp, K_assumed)\n",
    "\n",
    "        num_valid_points = cv2.countNonZero(mask_pose) if mask_pose is not None else 0\n",
    "        print(f\"Pose recovered. Number of points consistent with the pose: {num_valid_points} / {len(pts1_rp)}\")\n",
    "\n",
    "        if num_valid_points < 5: # Need at least a few points for a reliable pose\n",
    "             print(\"Warning: Very few points consistent with the recovered pose. Result might be unreliable.\")\n",
    "             # Depending on requirements, you might return None here\n",
    "             # return None, None, None\n",
    "\n",
    "        return R, t, mask_pose # R is 3x3, t is 3x1\n",
    "\n",
    "    except cv2.error as e:\n",
    "        print(f\"OpenCV Error during recoverPose: {e}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during pose estimation: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66618cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code\n",
    "\n",
    "# 1. Assume you have results from the previous stage:\n",
    "# fundamental_matrix, pts1_inliers, pts2_inliers (from compute_fundamental_matrix)\n",
    "\n",
    "# --- Placeholder results (replace with your actual data) ---\n",
    "# F_computed = np.random.rand(3, 3) # Replace with actual F\n",
    "# pts1_in = np.random.rand(50, 1, 2).astype(np.float32) * 500 # Replace with actual inliers\n",
    "# pts2_in = np.random.rand(50, 1, 2).astype(np.float32) * 500 # Replace with actual inliers\n",
    "# If the compute_fundamental_matrix function was just run:\n",
    "F_computed = fundamental_matrix\n",
    "pts1_in = pts1_inliers\n",
    "pts2_in = pts2_inliers\n",
    "# -- Placeholder end ---\n",
    "\n",
    "# 2. Define your assumed K matrix (Example!)\n",
    "#    You MUST justify your choice/estimation of K in your report.\n",
    "image_width = 640 # Example image width\n",
    "image_height = 480 # Example image height\n",
    "assumed_cx = image_width / 2\n",
    "assumed_cy = image_height / 2\n",
    "assumed_focal = image_width # A common starting guess if unknown\n",
    "K_assumed = np.array([\n",
    "    [assumed_focal, 0, assumed_cx],\n",
    "    [0, assumed_focal, assumed_cy],\n",
    "    [0, 0, 1]\n",
    "], dtype=np.float64)\n",
    "print(\"\\nUsing Assumed K matrix:\\n\", K_assumed)\n",
    "\n",
    "# 3. Call the pose estimation function\n",
    "if F_computed is not None and pts1_in is not None and pts2_in is not None:\n",
    "    R_estimated, t_estimated, pose_mask = estimate_camera_pose(pts1_in, pts2_in, F_computed, K_assumed)\n",
    "\n",
    "    # 4. Check results\n",
    "    if R_estimated is not None and t_estimated is not None:\n",
    "        print(\"\\nSuccessfully estimated camera pose.\")\n",
    "        print(\"Rotation (R):\\n\", R_estimated)\n",
    "        print(\"Translation (t) (up to scale):\\n\", t_estimated)\n",
    "        # You now have the relative rotation R and translation t between the two camera views.\n",
    "    else:\n",
    "        print(\"\\nFailed to estimate camera pose.\")\n",
    "else:\n",
    "    print(\"\\nSkipping pose estimation due to missing F or inlier points from previous stage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a361aa2",
   "metadata": {},
   "source": [
    "## 7. 3D Point Triangulation and Scene Visualisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
