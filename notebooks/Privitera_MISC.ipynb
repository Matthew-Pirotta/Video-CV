{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import imageio.v3 as iio # pip install imageio[ffmpeg]\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d # pip install open3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996542c1",
   "metadata": {},
   "source": [
    "## 2. Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64824c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening video file: ../videos/bedroomvideo.mp4\n",
      "\n",
      "Finished processing video.\n",
      "Total frames iterated: 237.\n"
     ]
    }
   ],
   "source": [
    "def load_video_frames(video_path, frame_interval=10, display_frames=True):\n",
    "    # Load frames from a video file at specified intervals.\n",
    "\n",
    "    frame_count = 0\n",
    "    frames = []  # List to store frames for visualization\n",
    "\n",
    "    try:\n",
    "        print(f\"Opening video file: {video_path}\")\n",
    "        \n",
    "        # Iterate through frames in the video file\n",
    "        for frame in iio.imiter(video_path):\n",
    "            if frame_count % frame_interval == 0:\n",
    "                frames.append(frame)  # Store frame in list\n",
    "                \n",
    "                # Display frame if display_frames is True\n",
    "                if display_frames:\n",
    "                    plt.imshow(frame)\n",
    "                    plt.title(f'Frame {frame_count}')\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                \n",
    "            frame_count += 1\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Video file not found at {video_path}\")\n",
    "        return [], 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the video: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "    print(f\"\\nFinished processing video.\")\n",
    "    print(f\"Total frames iterated: {frame_count}.\")\n",
    "    \n",
    "    return frames\n",
    "\n",
    "frames = load_video_frames('../videos/bedroomvideo.mp4', frame_interval=10, display_frames=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679cd5a",
   "metadata": {},
   "source": [
    "## 5. Essential/ Fundamental Matrix Computation\n",
    "\n",
    "Both matrices describe the epipolar geometry between two images of the same scene taken from different viewpoints. Epipolar geometry defines the constraint that corresponding points must satisfy. If you have a point x in the first image, its corresponding point x' in the second image must lie on a specific line called the epiline. Both matrices capture this relationship, but they differ based on camera calibration.\n",
    "\n",
    "The Fundamental Matrix acts as a bridge between the 2D pixel coordinates of corresponding points in two different images, capturing the geometric constraints imposed by the 3D scene and the camera positions, even when you don't know the camera's exact internal details. It's a fundamental tool for tasks like finding correct feature matches, estimating camera motion, and ultimately reconstructing the 3D scene.\n",
    "\n",
    "- Fundamental Matrix (F):\n",
    "  - **What it is**: A 3x3 matrix that relates corresponding points between two images in pixel coordinates.\n",
    "  - **Information encoded**: It contains information about the camera's relative rotation and translation (extrinsic parameters) and the intrinsic parameters (like focal length, principal point) of both cameras.\n",
    "  - **Equation**: It satisfies the epipolar constraint: x'^T * F * x = 0, where x and x' are the homogeneous coordinates of the matching points in pixels.\n",
    "  - **When to use**: Use the Fundamental Matrix when the cameras are uncalibrated, meaning you don't know their intrinsic parameters.\n",
    "  - **Computation**: Typically requires at least 8 pairs of corresponding points (using the 8-point algorithm) or 7 pairs (7-point algorithm). OpenCV's function often uses robust methods like RANSAC or LMedS which handle outliers well using many more points.\n",
    "\n",
    "If you are recording video with a phone and have not performed a specific camera calibration procedure to find its intrinsic matrix (K), then using the Fundamental Matrix (F) is the recommended approach. You treat the camera as uncalibrated.\n",
    "\n",
    "- Essential Matrix (E):\n",
    "  - **What it is**: A 3x3 matrix that relates corresponding points between two images in normalized image coordinates (independent of camera intrinsics).\n",
    "  - **Information encoded**: It contains only information about the camera's relative rotation (R) and translation (t), up to a scale factor. It does not include camera intrinsic information.\n",
    "  - **Equation**: It satisfies the epipolar constraint in normalized coordinates: x_norm'^T * E * x_norm = 0.\n",
    "  - **When to use**: Use the Essential Matrix when the cameras are calibrated, meaning you know their intrinsic parameters (focal length, principal point - often represented in a camera matrix K).\n",
    "  - **Computation**: Requires at least 5 pairs of corresponding points (using the 5-point algorithm), though robust methods in OpenCV use more points.\n",
    "  - **Relation to F**: E = K'^T * F * K, where K and K' are the camera intrinsic matrices for the two views. If the camera is the same for both views, K' = K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the fundamental matrix from matches keypoints\n",
    "def compute_fundamental_matrix(matches, keypoints1, keypoints2, method = cv2.FM_RANSAC, ransac_threshold = 3.0, confidence = 0.99):\n",
    "  # Extract coordinates of matches keypoints\n",
    "  points1 = np.float([keypoints1[m.queryIdx].pt for m in matches])\n",
    "  points2 = np.float([keypoints2[m.queryIdx].pt for m in matches])\n",
    "  \n",
    "  # Compute the fundamental matrix\n",
    "  fundamental_matrix, inlier_mask = cv2.findFundamentalMat(points1, points2, method = method, ransacReprojThreshold = ransac_threshold, confidence = confidence)\n",
    "  \n",
    "  # Convert mask to binary array for easier filtering\n",
    "  if fundamental_matrix is None or fundamental_matrix.shape != (3,3):\n",
    "    raise ValueError(\"Failed to compute a valid fundamental matrix\")\n",
    "  \n",
    "  return fundamental_matrix, inlier_mask\n",
    "\n",
    "# Visualise epipolar lines to verify the fundamental matrix\n",
    "def visualise_epipolar_lines(img1, img2, points1, points2, fundamental_matrix, sample_size = 20):\n",
    "  # Sample points if there are too many\n",
    "  if len(points1) > sample_size:\n",
    "    indices = np.random.choice(len(points1), sample_size, replace = False)\n",
    "    pts1 = points1[indices]\n",
    "    pts2 = points2[indices]\n",
    "  else:\n",
    "    pts1 = points1\n",
    "    pts2 = points2\n",
    "    \n",
    "  # Create a figure to display epipolar lines\n",
    "  _, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 8))\n",
    "    \n",
    "  # Display the first image\n",
    "  ax1.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "  ax1.set_title('Epipolar Lines on Image 1')\n",
    "  ax1.axis('off')\n",
    "    \n",
    "  # Display the second image\n",
    "  ax2.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "  ax2.set_title('Epipolar Lines on Image 2')\n",
    "  ax2.axis('off')\n",
    "    \n",
    "  # Draw epipolar lines on both images\n",
    "  for i in range(len(pts1)):\n",
    "    # Draw points\n",
    "    ax1.plot(pts1[i, 0], pts1[i, 1], 'ro', markersize = 6)\n",
    "    ax2.plot(pts2[i, 0], pts2[i, 1], 'ro', markersize = 6)\n",
    "        \n",
    "    # Compute epipolar line in second image for point in first image\n",
    "    line2 = cv2.computeCorrespondEpilines(pts1[i].reshape(-1, 1, 2), 1, fundamental_matrix)\n",
    "    line2 = line2.reshape(-1)\n",
    "        \n",
    "    # Draw epipolar line\n",
    "    x0, y0 = 0, int(-line2[2] / line2[1])\n",
    "    x1, y1 = img2.shape[1], int(-(line2[2] + line2[0] * img2.shape[1]) / line2[1])\n",
    "    ax2.plot([x0, x1], [y0, y1], 'g-')\n",
    "        \n",
    "    # Compute epipolar line in first image for point in second image\n",
    "    line1 = cv2.computeCorrespondEpilines(pts2[i].reshape(-1, 1, 2), 2, fundamental_matrix)\n",
    "    line1 = line1.reshape(-1)\n",
    "        \n",
    "    # Draw epipolar line\n",
    "    x0, y0 = 0, int(-line1[2] / line1[1])\n",
    "    x1, y1 = img1.shape[1], int(-(line1[2] + line1[0] * img1.shape[1]) / line1[1])\n",
    "    ax1.plot([x0, x1], [y0, y1], 'g-')\n",
    "    \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  \n",
    "  # Return the current Figure object that is active\n",
    "  return plt.gcf()\n",
    "\n",
    "# Calculate the epipolar geometry error to evaluate the quality of the fundamental matrix\n",
    "def epipolar_error(points1, points2, fundamental_matrix):\n",
    "  # Convert each point to homogeneous coordinates\n",
    "  homogeneous_points1 = np.hstack((points1, np.ones((points1.shape[0],1))))\n",
    "  homogeneous_points2 = np.hstack((points2, np.ones((points2.shape[0],1))))\n",
    "  \n",
    "  # Calculate epipolar lines for points in image 1\n",
    "  lines2 = np.dot(homogeneous_points1, fundamental_matrix.T)\n",
    "  # Normalise lines\n",
    "  norms2 = np.sqrt(lines2[:, 0]**2 + lines2[:, 1]**2)\n",
    "  lines2 = lines2 / norms2.reshape(-1,1)\n",
    "  # Calculate the distance from points in image 2 to their corresponding epipolar lines\n",
    "  dist2 = np.abs(np.sum(lines2 * homogeneous_points2, axis = 1))\n",
    "  \n",
    "  # Calculate epipolar lines for points in image 2\n",
    "  lines1 = np.dot(homogeneous_points2, fundamental_matrix)\n",
    "  # Normalise lines\n",
    "  norms1 = np.sqrt(lines1[:, 0]**2 + lines1[:, 1]**2)\n",
    "  lines1 = lines1 / norms1.reshape(-1,1)\n",
    "  # Calculate the distance from points in image 1 to their corresponding epipolar lines\n",
    "  dist1 = np.abs(np.sum(lines1 * homogeneous_points1, axis = 1))\n",
    "  \n",
    "  metrics = {\n",
    "    \"mean_error\": (np.mean(dist1) + np.mean(dist2)) / 2,\n",
    "    \"max_error\": max(np.max(dist1), np.max(dist2)),\n",
    "    \"std_error\": (np.std(dist1) + np.std(dist2)) / 2\n",
    "  }\n",
    "  \n",
    "  return metrics\n",
    "\n",
    "# Main function to process the fundamental computation step\n",
    "def process_fundamental_matrix(imgs, matches, keypoints1, keypoints2, visualise = True):\n",
    "  # Compute fundamental matrix\n",
    "  print(\"Computing fundamental matrix...\")\n",
    "  F, inlier_mask = compute_fundamental_matrix(matches, keypoints1, keypoints2)\n",
    "    \n",
    "  # Filter matches based on inlier mask\n",
    "  inlier_matches = [m for i, m in enumerate(matches) if inlier_mask[i]]\n",
    "  print(f\"Inlier matches: {len(inlier_matches)} out of {len(matches)} ({len(inlier_matches) / len(matches) * 100:.2f}%)\")\n",
    "    \n",
    "  # Extract coordinates of inlier keypoints\n",
    "  inlier_points1 = np.float32([keypoints1[m.queryIdx].pt for m in inlier_matches])\n",
    "  inlier_points2 = np.float32([keypoints2[m.trainIdx].pt for m in inlier_matches])\n",
    "    \n",
    "  # Calculate error metrics\n",
    "  error_metrics = epipolar_error(inlier_points1, inlier_points2, F)\n",
    "  print(f\"Mean epipolar error: {error_metrics['mean_error']:.4f} pixels\")\n",
    "        \n",
    "  # Visualize epipolar lines if requested\n",
    "  if visualise and len(imgs) >= 2:\n",
    "      visualise_epipolar_lines(imgs[0], imgs[1], inlier_points1, inlier_points2, F)\n",
    "    \n",
    "  # Prepare results\n",
    "  results = {\n",
    "      \"fundamental_matrix\": F,\n",
    "      \"inlier_mask\": inlier_mask,\n",
    "      \"inlier_matches\": inlier_matches,\n",
    "      \"inlier_points1\": inlier_points1,\n",
    "      \"inlier_points2\": inlier_points2,\n",
    "      \"error_metrics\": error_metrics,\n",
    "  }\n",
    "    \n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c28f1",
   "metadata": {},
   "source": [
    "## 6. Camera Pose Estimation\n",
    "\n",
    "The goal of this stage is to determine the relative motion between the two camera views where you matched features. This motion is described by:\n",
    "\n",
    "- **Rotation (R)**: A 3x3 matrix describing how the camera orientation changed between the two shots.\n",
    "- **Translation (t)**: A 3x1 vector describing how the camera position changed between the two shots. Note that the translation vector t can only be determined up to a scale factor. This means you know the direction of motion but not the absolute distance moved.\n",
    "\n",
    "- Input Matrix: cv2.recoverPose technically requires the Essential Matrix (E) as its main input, not the Fundamental Matrix (F). It also needs the corresponding inlier points from the previous stage (pts1_inliers, pts2_inliers) and the camera intrinsic matrix (K).\n",
    "- Handling the Uncalibrated Case (Starting with F): Since you computed F (because the phone camera was treated as uncalibrated), you need a way to get E to use recoverPose. The relationship is E = K^T * F * K. But K is unknown!\n",
    "  - The Workaround: You need to assume a plausible K matrix. A common approach is:\n",
    "Set the principal point (cx, cy) to the image center (e.g., width/2, height/2).\n",
    "  - Estimate or guess the focal length (fx, fy). Sometimes fx = fy = image_width is used as a starting guess, or a typical value for phone cameras (e.g., 500-1000 pixels) might be assumed.\n",
    "  - Compute E: Calculate E = K_assumed.T @ F @ K_assumed.\n",
    "  - Use recoverPose: Call cv2.recoverPose using this computed E, the same assumed K, and your inlier points.\n",
    "  - Important: You must document this assumption about K in your report. The resulting pose (especially translation t) will be relative to the scale defined by your assumed K and F.\n",
    "- Chirality Problem: Mathematically, decomposing the E matrix yields four possible solutions for the rotation (R) and translation (t). However, only one of these solutions is physically correct – the one where the reconstructed 3D points lie in front of both cameras.\n",
    "  - How recoverPose Solves It: The cv2.recoverPose function handles this automatically! It takes your inlier points (pts1_inliers, pts2_inliers) and the K matrix, triangulates the points for each of the four possible (R, t) combinations, and counts how many points end up in front of both camera views. It then returns the R and t corresponding to the hypothesis with the most positive depth points, effectively resolving the chirality ambiguity. Your report should explain this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a361aa2",
   "metadata": {},
   "source": [
    "## 7. 3D Point Triangulation and Scene Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb294880",
   "metadata": {},
   "source": [
    "- Concept: Triangulation is the process of determining the 3D coordinates of a point in space. You can do this if you have observed the projection of that point in at least two images taken from different, known camera viewpoints. Given the 2D coordinates of the point in each image (pts1, pts2) and the camera poses (relative rotation R and translation t between the views, plus the camera intrinsics K), you can find the intersection of the \"rays\" that go from each camera center through its corresponding 2D image point. This intersection point is the estimated 3D location of the point.\n",
    "- Projection Matrices (P1, P2): To use cv2.triangulatePoints, you need the 3x4 projection matrix for each camera view. This matrix maps 3D world points (in homogeneous coordinates) to 2D image points (in homogeneous coordinates).\n",
    "  - Camera 1 (Reference): We typically assume the first camera is at the origin of the world coordinate system. Its projection matrix P1 is formed using the intrinsic matrix K and a standard pose [I | 0] (Identity rotation, Zero translation): P1 = K @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "  - Camera 2 (Relative Pose): The second camera's projection matrix P2 is formed using the same K (assuming the same camera or the assumed K from Stage 6) and the relative rotation R and translation t calculated in Stage 6: P2 = K @ np.hstack((R, t))\n",
    "- cv2.triangulatePoints: This OpenCV function takes the two projection matrices (P1, P2) and the corresponding 2D points from both images (pts1, pts2) as input.\n",
    "  - Input Format: Note that cv2.triangulatePoints expects the 2D points as 2xN arrays (2 rows, N columns). You'll likely need to transpose your (N, 1, 2) or (N, 2) point arrays.\n",
    "  - Output Format: It returns the 3D points in homogeneous coordinates as a 4xN array (4 rows, N columns). To get the standard 3D Cartesian coordinates, you need to divide the first three rows by the fourth row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a85872",
   "metadata": {},
   "source": [
    "Explanation: Scene Visualisation\n",
    "\n",
    "- Goal: To display the calculated 3D points to visually inspect the reconstructed scene structure.\n",
    "- Tools: Your project allows libraries like Matplotlib, Open3D, or CloudCompare.\n",
    "- Interactivity: The visualization must be interactive, allowing you to rotate, pan, and zoom the 3D view.\n",
    "  - Matplotlib (mplot3d): Can create basic 3D scatter plots. Interactivity (rotation/zoom) often works well in specific environments like Jupyter Notebook using the %matplotlib notebook backend.\n",
    "  - Open3D: A library specifically designed for 3D data processing and visualization. It generally provides more robust and feature-rich interactive visualization windows suitable for point clouds. This is likely the better choice to meet the \"interactive\" requirement robustly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
