{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fec69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import imageio.v3 as iio # pip install imageio[ffmpeg]\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d # pip install open3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996542c1",
   "metadata": {},
   "source": [
    "## 2. Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64824c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening video file: ../videos/bedroomvideo.mp4\n",
      "\n",
      "Finished processing video.\n",
      "Total frames iterated: 237.\n"
     ]
    }
   ],
   "source": [
    "def load_video_frames(video_path, frame_interval=10, display_frames=True):\n",
    "    # Load frames from a video file at specified intervals.\n",
    "\n",
    "    frame_count = 0\n",
    "    frames = []  # List to store frames for visualization\n",
    "\n",
    "    try:\n",
    "        print(f\"Opening video file: {video_path}\")\n",
    "        \n",
    "        # Iterate through frames in the video file\n",
    "        for frame in iio.imiter(video_path):\n",
    "            if frame_count % frame_interval == 0:\n",
    "                frames.append(frame)  # Store frame in list\n",
    "                \n",
    "                # Display frame if display_frames is True\n",
    "                if display_frames:\n",
    "                    plt.imshow(frame)\n",
    "                    plt.title(f'Frame {frame_count}')\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                \n",
    "            frame_count += 1\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Video file not found at {video_path}\")\n",
    "        return [], 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the video: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "    print(f\"\\nFinished processing video.\")\n",
    "    print(f\"Total frames iterated: {frame_count}.\")\n",
    "    \n",
    "    return frames\n",
    "\n",
    "frames = load_video_frames('../videos/bedroomvideo.mp4', frame_interval=10, display_frames=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679cd5a",
   "metadata": {},
   "source": [
    "## 5. Essential/ Fundamental Matrix Computation\n",
    "\n",
    "Both matrices describe the epipolar geometry between two images of the same scene taken from different viewpoints. Epipolar geometry defines the constraint that corresponding points must satisfy. If you have a point x in the first image, its corresponding point x' in the second image must lie on a specific line called the epiline. Both matrices capture this relationship, but they differ based on camera calibration.\n",
    "\n",
    "The Fundamental Matrix acts as a bridge between the 2D pixel coordinates of corresponding points in two different images, capturing the geometric constraints imposed by the 3D scene and the camera positions, even when you don't know the camera's exact internal details. It's a fundamental tool for tasks like finding correct feature matches, estimating camera motion, and ultimately reconstructing the 3D scene.\n",
    "\n",
    "- Fundamental Matrix (F):\n",
    "  - **What it is**: A 3x3 matrix that relates corresponding points between two images in pixel coordinates.\n",
    "  - **Information encoded**: It contains information about the camera's relative rotation and translation (extrinsic parameters) and the intrinsic parameters (like focal length, principal point) of both cameras.\n",
    "  - **Equation**: It satisfies the epipolar constraint: x'^T * F * x = 0, where x and x' are the homogeneous coordinates of the matching points in pixels.\n",
    "  - **When to use**: Use the Fundamental Matrix when the cameras are uncalibrated, meaning you don't know their intrinsic parameters.\n",
    "  - **Computation**: Typically requires at least 8 pairs of corresponding points (using the 8-point algorithm) or 7 pairs (7-point algorithm). OpenCV's function often uses robust methods like RANSAC or LMedS which handle outliers well using many more points.\n",
    "\n",
    "If you are recording video with a phone and have not performed a specific camera calibration procedure to find its intrinsic matrix (K), then using the Fundamental Matrix (F) is the recommended approach. You treat the camera as uncalibrated.\n",
    "\n",
    "- Essential Matrix (E):\n",
    "  - **What it is**: A 3x3 matrix that relates corresponding points between two images in normalized image coordinates (independent of camera intrinsics).\n",
    "  - **Information encoded**: It contains only information about the camera's relative rotation (R) and translation (t), up to a scale factor. It does not include camera intrinsic information.\n",
    "  - **Equation**: It satisfies the epipolar constraint in normalized coordinates: x_norm'^T * E * x_norm = 0.\n",
    "  - **When to use**: Use the Essential Matrix when the cameras are calibrated, meaning you know their intrinsic parameters (focal length, principal point - often represented in a camera matrix K).\n",
    "  - **Computation**: Requires at least 5 pairs of corresponding points (using the 5-point algorithm), though robust methods in OpenCV use more points.\n",
    "  - **Relation to F**: E = K'^T * F * K, where K and K' are the camera intrinsic matrices for the two views. If the camera is the same for both views, K' = K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fef4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fundamental_matrix(pts1, pts2, method=cv2.FM_RANSAC, ransacReprojThreshold=1.0, confidence=0.999, maxIters=2000):\n",
    "    \"\"\"\n",
    "    Computes the Fundamental Matrix (F) and filters inliers between two sets of points.\n",
    "\n",
    "    Args:\n",
    "        pts1 (np.ndarray): NumPy array of N points from image 1, shape (N, 1, 2), dtype=float32/64.\n",
    "        pts2 (np.ndarray): NumPy array of N points from image 2, shape (N, 1, 2), dtype=float32/64.\n",
    "        method (int, optional): Method for computation (e.g., cv2.FM_RANSAC, cv2.FM_LMEDS).\n",
    "                                Defaults to cv2.FM_RANSAC.\n",
    "        ransacReprojThreshold (float, optional): RANSAC reprojection threshold (pixels). Defaults to 1.0.\n",
    "        confidence (float, optional): RANSAC confidence level. Defaults to 0.999.\n",
    "        maxIters (int, optional): Maximum RANSAC iterations. Defaults to 2000.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - F (np.ndarray or None): The computed 3x3 Fundamental Matrix, or None if failed.\n",
    "            - pts1_inliers (np.ndarray or None): Inlier points from image 1, or None if failed.\n",
    "            - pts2_inliers (np.ndarray or None): Inlier points from image 2, or None if failed.\n",
    "            - status_mask (np.ndarray or None): The mask indicating inliers (1) and outliers (0).\n",
    "    \"\"\"\n",
    "    if pts1 is None or pts2 is None or len(pts1) < 8 or len(pts2) < 8:\n",
    "        print(\"Error: Not enough points to compute Fundamental Matrix.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    try:\n",
    "        # Compute the Fundamental Matrix using the chosen method\n",
    "        F, mask = cv2.findFundamentalMat(pts1, pts2,\n",
    "                                         method=method,\n",
    "                                         ransacReprojThreshold=ransacReprojThreshold,\n",
    "                                         confidence=confidence,\n",
    "                                         maxIters=maxIters)\n",
    "\n",
    "        if F is None or mask is None:\n",
    "            print(\"Warning: findFundamentalMat returned None.\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        # Filter points using the mask to get inliers\n",
    "        pts1_inliers = pts1[mask.ravel() == 1]\n",
    "        pts2_inliers = pts2[mask.ravel() == 1]\n",
    "        num_inliers = len(pts1_inliers)\n",
    "        num_total = len(pts1)\n",
    "\n",
    "        print(f\"Fundamental Matrix computed. Inliers: {num_inliers} / {num_total}\")\n",
    "\n",
    "        return F, pts1_inliers, pts2_inliers, mask\n",
    "\n",
    "    except cv2.error as e:\n",
    "        print(f\"OpenCV Error during findFundamentalMat: {e}\")\n",
    "        return None, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40336fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fundamental Matrix computed. Inliers: 65 / 150\n",
      "Successfully computed F and filtered inliers.\n",
      "Shape of pts1_inliers: (65, 1, 2)\n",
      "Shape of pts2_inliers: (65, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Driver code\n",
    "# 1. Assume you have your matched points (replace with your actual data)\n",
    "num_points = 150\n",
    "pts1_matched = np.random.rand(num_points, 1, 2).astype(np.float32) * 500\n",
    "translation_demo = np.array([30, 10], dtype=np.float32).reshape(1, 1, 2)\n",
    "noise_demo = np.random.randn(num_points, 1, 2).astype(np.float32) * 3\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(num_points, size=int(num_points * 0.2), replace=False)\n",
    "pts2_matched = pts1_matched + translation_demo + noise_demo\n",
    "pts2_matched[outlier_indices] = np.random.rand(len(outlier_indices), 1, 2).astype(np.float32) * 500\n",
    "\n",
    "# 2. Call the function\n",
    "fundamental_matrix, pts1_inliers, pts2_inliers, inlier_mask = compute_fundamental_matrix(\n",
    "    pts1_matched,\n",
    "    pts2_matched,\n",
    "    ransacReprojThreshold=1.5, # Example threshold adjustment\n",
    "    confidence=0.99\n",
    ")\n",
    "\n",
    "# 3. Check the result and use the outputs\n",
    "if fundamental_matrix is not None:\n",
    "    print(\"Successfully computed F and filtered inliers.\")\n",
    "    print(f\"Shape of pts1_inliers: {pts1_inliers.shape}\")\n",
    "    print(f\"Shape of pts2_inliers: {pts2_inliers.shape}\")\n",
    "\n",
    "    # Now you can pass 'fundamental_matrix', 'pts1_inliers', 'pts2_inliers'\n",
    "    # to the next stage (e.g., camera pose estimation)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to compute Fundamental Matrix or find sufficient inliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c28f1",
   "metadata": {},
   "source": [
    "## 6. Camera Pose Estimation\n",
    "\n",
    "The goal of this stage is to determine the relative motion between the two camera views where you matched features. This motion is described by:\n",
    "\n",
    "- **Rotation (R)**: A 3x3 matrix describing how the camera orientation changed between the two shots.\n",
    "- **Translation (t)**: A 3x1 vector describing how the camera position changed between the two shots. Note that the translation vector t can only be determined up to a scale factor. This means you know the direction of motion but not the absolute distance moved.\n",
    "\n",
    "- Input Matrix: cv2.recoverPose technically requires the Essential Matrix (E) as its main input, not the Fundamental Matrix (F). It also needs the corresponding inlier points from the previous stage (pts1_inliers, pts2_inliers) and the camera intrinsic matrix (K).\n",
    "- Handling the Uncalibrated Case (Starting with F): Since you computed F (because the phone camera was treated as uncalibrated), you need a way to get E to use recoverPose. The relationship is E = K^T * F * K. But K is unknown!\n",
    "  - The Workaround: You need to assume a plausible K matrix. A common approach is:\n",
    "Set the principal point (cx, cy) to the image center (e.g., width/2, height/2).\n",
    "  - Estimate or guess the focal length (fx, fy). Sometimes fx = fy = image_width is used as a starting guess, or a typical value for phone cameras (e.g., 500-1000 pixels) might be assumed.\n",
    "  - Compute E: Calculate E = K_assumed.T @ F @ K_assumed.\n",
    "  - Use recoverPose: Call cv2.recoverPose using this computed E, the same assumed K, and your inlier points.\n",
    "  - Important: You must document this assumption about K in your report. The resulting pose (especially translation t) will be relative to the scale defined by your assumed K and F.\n",
    "- Chirality Problem: Mathematically, decomposing the E matrix yields four possible solutions for the rotation (R) and translation (t). However, only one of these solutions is physically correct – the one where the reconstructed 3D points lie in front of both cameras.\n",
    "  - How recoverPose Solves It: The cv2.recoverPose function handles this automatically! It takes your inlier points (pts1_inliers, pts2_inliers) and the K matrix, triangulates the points for each of the four possible (R, t) combinations, and counts how many points end up in front of both camera views. It then returns the R and t corresponding to the hypothesis with the most positive depth points, effectively resolving the chirality ambiguity. Your report should explain this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "814312b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_camera_pose(pts1, pts2, F, K_assumed):\n",
    "    \"\"\"\n",
    "    Estimates the relative camera pose (Rotation and Translation) from the\n",
    "    Fundamental Matrix (F) and corresponding inlier points using cv2.recoverPose.\n",
    "\n",
    "    Args:\n",
    "        pts1 (np.ndarray): Inlier points from image 1, shape (N, 1, 2) or (N, 2).\n",
    "        pts2 (np.ndarray): Inlier points from image 2, shape (N, 1, 2) or (N, 2).\n",
    "        F (np.ndarray): The 3x3 Fundamental Matrix computed previously.\n",
    "        K_assumed (np.ndarray): The assumed 3x3 camera intrinsic matrix.\n",
    "                           Crucial for converting F to E and for chirality check.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - R (np.ndarray or None): The 3x3 estimated Rotation matrix, or None if failed.\n",
    "            - t (np.ndarray or None): The 3x1 estimated Translation vector (up to scale), or None if failed.\n",
    "            - points_valid_mask (np.ndarray or None): Mask indicating points used by recoverPose for the final check.\n",
    "    \"\"\"\n",
    "    if F is None or K_assumed is None or pts1 is None or pts2 is None or len(pts1) < 5:\n",
    "         print(\"Error: Insufficient input for pose estimation.\")\n",
    "         return None, None, None\n",
    "\n",
    "    # Ensure points are in the correct shape (N, 2) for recoverPose input consistency check\n",
    "    # Although recoverPose can sometimes handle (N, 1, 2), ensuring (N, 2) is safer.\n",
    "    pts1_rp = np.reshape(pts1, (-1, 2)).astype(np.float64)\n",
    "    pts2_rp = np.reshape(pts2, (-1, 2)).astype(np.float64)\n",
    "\n",
    "    try:\n",
    "        # 1. Compute Essential Matrix from F and assumed K\n",
    "        E = K_assumed.T @ F @ K_assumed\n",
    "        print(\"Computed Essential Matrix (E) from F and assumed K.\")\n",
    "        # print(\"E:\\n\", E) # Uncomment to view\n",
    "\n",
    "        # 2. Recover Pose (R, t) using cv2.recoverPose\n",
    "        # This function internally handles the chirality check using the points and K\n",
    "        points, R, t, mask_pose = cv2.recoverPose(E, pts1_rp, pts2_rp, K_assumed)\n",
    "\n",
    "        num_valid_points = cv2.countNonZero(mask_pose) if mask_pose is not None else 0\n",
    "        print(f\"Pose recovered. Number of points consistent with the pose: {num_valid_points} / {len(pts1_rp)}\")\n",
    "\n",
    "        if num_valid_points < 5: # Need at least a few points for a reliable pose\n",
    "             print(\"Warning: Very few points consistent with the recovered pose. Result might be unreliable.\")\n",
    "             # Depending on requirements, you might return None here\n",
    "             # return None, None, None\n",
    "\n",
    "        return R, t, mask_pose # R is 3x3, t is 3x1\n",
    "\n",
    "    except cv2.error as e:\n",
    "        print(f\"OpenCV Error during recoverPose: {e}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during pose estimation: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66618cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Assumed K matrix:\n",
      " [[640.   0. 320.]\n",
      " [  0. 640. 240.]\n",
      " [  0.   0.   1.]]\n",
      "Computed Essential Matrix (E) from F and assumed K.\n",
      "Pose recovered. Number of points consistent with the pose: 10 / 65\n",
      "\n",
      "Successfully estimated camera pose.\n",
      "Rotation (R):\n",
      " [[ 0.99907164  0.00923915  0.04207716]\n",
      " [-0.00984555  0.99985031  0.01422728]\n",
      " [-0.04193941 -0.01462834  0.99901306]]\n",
      "Translation (t) (up to scale):\n",
      " [[ 0.15971083]\n",
      " [-0.2872955 ]\n",
      " [-0.94443303]]\n"
     ]
    }
   ],
   "source": [
    "# Driver code\n",
    "\n",
    "# 1. Assume you have results from the previous stage:\n",
    "# fundamental_matrix, pts1_inliers, pts2_inliers (from compute_fundamental_matrix)\n",
    "\n",
    "# --- Placeholder results (replace with your actual data) ---\n",
    "# F_computed = np.random.rand(3, 3) # Replace with actual F\n",
    "# pts1_in = np.random.rand(50, 1, 2).astype(np.float32) * 500 # Replace with actual inliers\n",
    "# pts2_in = np.random.rand(50, 1, 2).astype(np.float32) * 500 # Replace with actual inliers\n",
    "# If the compute_fundamental_matrix function was just run:\n",
    "F_computed = fundamental_matrix\n",
    "pts1_in = pts1_inliers\n",
    "pts2_in = pts2_inliers\n",
    "# -- Placeholder end ---\n",
    "\n",
    "# 2. Define your assumed K matrix (Example!)\n",
    "#    You MUST justify your choice/estimation of K in your report.\n",
    "image_width = 640 # Example image width\n",
    "image_height = 480 # Example image height\n",
    "assumed_cx = image_width / 2\n",
    "assumed_cy = image_height / 2\n",
    "assumed_focal = image_width # A common starting guess if unknown\n",
    "K_assumed = np.array([\n",
    "    [assumed_focal, 0, assumed_cx],\n",
    "    [0, assumed_focal, assumed_cy],\n",
    "    [0, 0, 1]\n",
    "], dtype=np.float64)\n",
    "print(\"\\nUsing Assumed K matrix:\\n\", K_assumed)\n",
    "\n",
    "# 3. Call the pose estimation function\n",
    "if F_computed is not None and pts1_in is not None and pts2_in is not None:\n",
    "    R_estimated, t_estimated, pose_mask = estimate_camera_pose(pts1_in, pts2_in, F_computed, K_assumed)\n",
    "\n",
    "    # 4. Check results\n",
    "    if R_estimated is not None and t_estimated is not None:\n",
    "        print(\"\\nSuccessfully estimated camera pose.\")\n",
    "        print(\"Rotation (R):\\n\", R_estimated)\n",
    "        print(\"Translation (t) (up to scale):\\n\", t_estimated)\n",
    "        # You now have the relative rotation R and translation t between the two camera views.\n",
    "    else:\n",
    "        print(\"\\nFailed to estimate camera pose.\")\n",
    "else:\n",
    "    print(\"\\nSkipping pose estimation due to missing F or inlier points from previous stage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a361aa2",
   "metadata": {},
   "source": [
    "## 7. 3D Point Triangulation and Scene Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb294880",
   "metadata": {},
   "source": [
    "- Concept: Triangulation is the process of determining the 3D coordinates of a point in space. You can do this if you have observed the projection of that point in at least two images taken from different, known camera viewpoints. Given the 2D coordinates of the point in each image (pts1, pts2) and the camera poses (relative rotation R and translation t between the views, plus the camera intrinsics K), you can find the intersection of the \"rays\" that go from each camera center through its corresponding 2D image point. This intersection point is the estimated 3D location of the point.\n",
    "- Projection Matrices (P1, P2): To use cv2.triangulatePoints, you need the 3x4 projection matrix for each camera view. This matrix maps 3D world points (in homogeneous coordinates) to 2D image points (in homogeneous coordinates).\n",
    "  - Camera 1 (Reference): We typically assume the first camera is at the origin of the world coordinate system. Its projection matrix P1 is formed using the intrinsic matrix K and a standard pose [I | 0] (Identity rotation, Zero translation): P1 = K @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "  - Camera 2 (Relative Pose): The second camera's projection matrix P2 is formed using the same K (assuming the same camera or the assumed K from Stage 6) and the relative rotation R and translation t calculated in Stage 6: P2 = K @ np.hstack((R, t))\n",
    "- cv2.triangulatePoints: This OpenCV function takes the two projection matrices (P1, P2) and the corresponding 2D points from both images (pts1, pts2) as input.\n",
    "  - Input Format: Note that cv2.triangulatePoints expects the 2D points as 2xN arrays (2 rows, N columns). You'll likely need to transpose your (N, 1, 2) or (N, 2) point arrays.\n",
    "  - Output Format: It returns the 3D points in homogeneous coordinates as a 4xN array (4 rows, N columns). To get the standard 3D Cartesian coordinates, you need to divide the first three rows by the fourth row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16209bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_points(pts1, pts2, K, R, t):\n",
    "    \"\"\"\n",
    "    Triangulates 3D points from corresponding 2D points in two views.\n",
    "\n",
    "    Args:\n",
    "        pts1 (np.ndarray): Inlier points from image 1, shape (N, 2) or (N, 1, 2), dtype=float64.\n",
    "        pts2 (np.ndarray): Inlier points from image 2, shape (N, 2) or (N, 1, 2), dtype=float64.\n",
    "        K (np.ndarray): The 3x3 camera intrinsic matrix (assumed or calibrated).\n",
    "        R (np.ndarray): The 3x3 estimated Rotation matrix (from view 1 to view 2).\n",
    "        t (np.ndarray): The 3x1 estimated Translation vector (from view 1 to view 2).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray or None: Array of triangulated 3D points in Cartesian coordinates (N, 3),\n",
    "                           or None if input is invalid or triangulation fails.\n",
    "    \"\"\"\n",
    "    if pts1 is None or pts2 is None or K is None or R is None or t is None:\n",
    "        print(\"Error: Missing inputs for triangulation.\")\n",
    "        return None\n",
    "    if len(pts1) != len(pts2):\n",
    "        print(\"Error: Number of points in pts1 and pts2 must match.\")\n",
    "        return None\n",
    "    if len(pts1) == 0:\n",
    "        print(\"Error: No points provided for triangulation.\")\n",
    "        return None\n",
    "\n",
    "    # Reshape points to (N, 2) if needed and ensure correct type (float64 preferred by some examples)\n",
    "    pts1_tri = np.reshape(pts1, (-1, 2)).astype(np.float64)\n",
    "    pts2_tri = np.reshape(pts2, (-1, 2)).astype(np.float64)\n",
    "\n",
    "    # Construct projection matrices P = K @ [R|t]\n",
    "    # P1: Camera 1 at origin\n",
    "    P1 = K @ np.hstack((np.eye(3, dtype=np.float64), np.zeros((3, 1), dtype=np.float64)))\n",
    "    # P2: Camera 2 pose relative to Camera 1\n",
    "    P2 = K @ np.hstack((R.astype(np.float64), t.astype(np.float64)))\n",
    "\n",
    "    try:\n",
    "        # Triangulate points - Input points need to be 2xN\n",
    "        # Output is 4xN homogeneous coordinates\n",
    "        points_4d_hom = cv2.triangulatePoints(P1, P2, pts1_tri.T, pts2_tri.T)\n",
    "\n",
    "        # Convert from homogeneous to Cartesian coordinates (divide by 4th element)\n",
    "        # Avoid division by zero or very small numbers\n",
    "        w = points_4d_hom[3, :]\n",
    "        # Set points with invalid W to NaN or handle appropriately\n",
    "        # points_3d = points_4d_hom[:3, :] / w # Basic conversion\n",
    "\n",
    "        # Safer conversion: handle potential zero W\n",
    "        points_3d = np.zeros((3, points_4d_hom.shape[1]))\n",
    "        valid_w = np.abs(w) > 1e-6 # Threshold to avoid division by near-zero\n",
    "        points_3d[:, valid_w] = points_4d_hom[:3, valid_w] / w[valid_w]\n",
    "        # Decide how to handle invalid points (e.g., filter them out later)\n",
    "        # For now, we keep the structure, invalid points might be at origin or NaN\n",
    "\n",
    "        # Transpose to get shape (N, 3)\n",
    "        points_3d = points_3d.T\n",
    "        num_valid = np.sum(valid_w)\n",
    "        print(f\"Successfully triangulated points. {num_valid} points with valid depth.\")\n",
    "        return points_3d\n",
    "\n",
    "    except cv2.error as e:\n",
    "        print(f\"OpenCV Error during triangulatePoints: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during triangulation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a85872",
   "metadata": {},
   "source": [
    "Explanation: Scene Visualisation\n",
    "\n",
    "- Goal: To display the calculated 3D points to visually inspect the reconstructed scene structure.\n",
    "- Tools: Your project allows libraries like Matplotlib, Open3D, or CloudCompare.\n",
    "- Interactivity: The visualization must be interactive, allowing you to rotate, pan, and zoom the 3D view.\n",
    "  - Matplotlib (mplot3d): Can create basic 3D scatter plots. Interactivity (rotation/zoom) often works well in specific environments like Jupyter Notebook using the %matplotlib notebook backend.\n",
    "  - Open3D: A library specifically designed for 3D data processing and visualization. It generally provides more robust and feature-rich interactive visualization windows suitable for point clouds. This is likely the better choice to meet the \"interactive\" requirement robustly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b419db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d_scene_open3d(points_3d, point_colors=None, window_name=\"Open3D Scene Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualizes the 3D point cloud interactively using Open3D.\n",
    "    Opens a separate, blocking window.\n",
    "    Args:\n",
    "        points_3d (list or np.ndarray): A list of [x, y, z] points.\n",
    "        point_colors (list or np.ndarray, optional): Optional RGB color values (0.0-1.0).\n",
    "        window_name (str, optional): Title for the visualization window.\n",
    "    \"\"\"\n",
    "    if not len(points_3d):\n",
    "        print(\"No points to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Create point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.asarray(points_3d))\n",
    "    \n",
    "    if point_colors is not None:\n",
    "        pcd.colors = o3d.utility.Vector3dVector(np.asarray(point_colors))\n",
    "    else:\n",
    "        pcd.paint_uniform_color([0.5, 0.5, 0.5])  # Default to gray\n",
    "    \n",
    "    # Create coordinate frame to visualize the origin\n",
    "    coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5)\n",
    "    \n",
    "    try:\n",
    "        # Try using the visualization approach directly from the documentation\n",
    "        from open3d.visualization import draw_geometries\n",
    "        draw_geometries(\n",
    "            [pcd, coordinate_frame],\n",
    "            window_name=window_name,\n",
    "            width=800, \n",
    "            height=600\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")\n",
    "        print(\"Saving point cloud to file instead.\")\n",
    "        temp_file = \"point_cloud_3d.ply\"\n",
    "        o3d.io.write_point_cloud(temp_file, pcd)\n",
    "        print(f\"Point cloud saved to {temp_file}. Please open it with an external viewer.\")\n",
    "    \n",
    "    print(\"Visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc875e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Triangulation ---\n",
      "Successfully triangulated points. 50 points with valid depth.\n",
      "\n",
      "--- Starting Visualization ---\n",
      "Visualization complete.\n"
     ]
    }
   ],
   "source": [
    "# Driver code\n",
    "\n",
    "# 1. Assume you have results from previous stages:\n",
    "# R_estimated, t_estimated (from estimate_camera_pose)\n",
    "# pts1_inliers, pts2_inliers (inliers from F or E computation)\n",
    "# K_matrix (assumed or calibrated K used in pose estimation)\n",
    "\n",
    "# --- Placeholder results (replace with your actual data) ---\n",
    "# Use the variables from the previous example call if available\n",
    "# R_est = R_estimated\n",
    "# t_est = t_estimated\n",
    "# pts1_for_tri = pts1_inliers\n",
    "# pts2_for_tri = pts2_inliers\n",
    "# K_matrix = K_assumed\n",
    "# Example values if previous variables not available:\n",
    "R_est_demo = np.array([[0.99, 0.01, 0.0], [-0.01, 0.99, 0.0], [0.0, 0.0, 1.0]])\n",
    "t_est_demo = np.array([[0.1], [0.0], [0.01]])\n",
    "K_matrix_demo = np.array([[800.0, 0, 320.0], [0, 800.0, 240.0], [0, 0, 1.0]])\n",
    "# Generate some dummy points consistent with a simple scene\n",
    "N_pts_demo = 50\n",
    "true_3d = np.random.rand(N_pts_demo, 3) * np.array([2, 2, 5]) + np.array([-1, -1, 3]) # Points in front\n",
    "P1_demo = K_matrix_demo @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "P2_demo = K_matrix_demo @ np.hstack((R_est_demo, t_est_demo))\n",
    "pts1_demo = (P1_demo @ np.hstack((true_3d, np.ones((N_pts_demo, 1)))).T).T\n",
    "pts2_demo = (P2_demo @ np.hstack((true_3d, np.ones((N_pts_demo, 1)))).T).T\n",
    "pts1_demo = (pts1_demo[:, :2] / pts1_demo[:, 2:]) + np.random.randn(N_pts_demo, 2) * 0.5 # Add noise\n",
    "pts2_demo = (pts2_demo[:, :2] / pts2_demo[:, 2:]) + np.random.randn(N_pts_demo, 2) * 0.5 # Add noise\n",
    "\n",
    "R_est = R_est_demo\n",
    "t_est = t_est_demo\n",
    "pts1_for_tri = pts1_demo\n",
    "pts2_for_tri = pts2_demo\n",
    "K_matrix = K_matrix_demo\n",
    "# --- End Placeholder ---\n",
    "\n",
    "# 2. Triangulate the points\n",
    "if R_est is not None and t_est is not None and K_matrix is not None and pts1_for_tri is not None and pts2_for_tri is not None:\n",
    "    print(\"\\n--- Starting Triangulation ---\")\n",
    "    points_3d_scene = triangulate_points(pts1_for_tri, pts2_for_tri, K_matrix, R_est, t_est)\n",
    "\n",
    "    if points_3d_scene is not None and len(points_3d_scene) > 0:\n",
    "        print(\"\\n--- Starting Visualization ---\")\n",
    "        visualize_3d_scene_open3d(points_3d_scene)\n",
    "    else:\n",
    "        print(\"Triangulation failed or no points to visualize.\")\n",
    "else:\n",
    "     print(\"\\nSkipping triangulation and visualization due to missing inputs from previous stages.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
