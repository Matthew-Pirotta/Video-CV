{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb680e4d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bcbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "import open3d as o3d # 3D scene visualization\n",
    "import plyfile # Export point cloud\n",
    "import os # File operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228a99b",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57fa357",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatchResult = namedtuple(\"MatchResult\", [\"frame_pair\", \"kp1\", \"kp2\", \"pts1\", \"pts2\", \"matches\"])\n",
    "Frame = np.ndarray\n",
    "\n",
    "def get_next_frame_index(frames, current_frame_index):\n",
    "    next_frame_index = current_frame_index + 1\n",
    "\n",
    "    if next_frame_index >= len(frames):\n",
    "        raise ValueError(\"Next frame index out of bounds\")\n",
    "    \n",
    "    return next_frame_index\n",
    "\n",
    "def convert_frames_to_grayscale(*frames):\n",
    "    gray_frames = []\n",
    "    for frame in frames:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frames.append(gray)\n",
    "    \n",
    "    return tuple(gray_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61a469",
   "metadata": {},
   "source": [
    "# 2. Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850110fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_indices(video_path:str, num_frames:int) -> list:\n",
    "    # Count total frames by iterating once\n",
    "    total_frames = sum(1 for _ in iio.imiter(video_path))\n",
    "\n",
    "    if num_frames > total_frames:\n",
    "        raise ValueError(f\"Requested {num_frames} frames, but video has only {total_frames} frames.\")\n",
    "\n",
    "    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "    return indices.tolist()\n",
    "\n",
    "def load_specific_frames(video_path: str, selected_indices: list, display_frames=True) -> list:\n",
    "    frames = []\n",
    "    selected_set = set(selected_indices)\n",
    "    collected = 0\n",
    "\n",
    "    for frame_count, frame in enumerate(iio.imiter(video_path)):\n",
    "        if frame_count in selected_set:\n",
    "            frames.append(frame)\n",
    "            collected += 1\n",
    "\n",
    "            if display_frames:\n",
    "                plt.imshow(frame)\n",
    "                plt.title(f'Frame {frame_count}')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "    print(f\"\\nFinished loading selected frames.\")\n",
    "    print(f\"Total frames processed: {frame_count + 1}\")\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d647f",
   "metadata": {},
   "source": [
    "# 3. Feature Detection and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955719a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_orb(gray):\n",
    "    orb = cv2.ORB_create(nfeatures=1500) \n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_sift(gray):\n",
    "    sift = cv2.SIFT_create(nfeatures=1500, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6)\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_fast(gray):\n",
    "    fast = cv2.FastFeatureDetector_create(threshold=37, nonmaxSuppression=True)\n",
    "    keypoints = fast.detect(gray, None)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.compute(gray, keypoints)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_shi_tomasi(gray):\n",
    "    corners = cv2.goodFeaturesToTrack(gray, maxCorners=3000, qualityLevel=0.01, minDistance=10, blockSize=3)\n",
    "    keypoints = []\n",
    "    if corners is not None:\n",
    "        keypoints = [cv2.KeyPoint(float(x), float(y), 1) for [[x, y]] in corners]\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_harris(gray, block_size=2, ksize=3, k=0.04, threshold_ratio=0.07): \n",
    "    gray_f32 = np.float32(gray)\n",
    "    dst = cv2.cornerHarris(gray_f32, block_size, ksize, k)\n",
    "    dst = cv2.dilate(dst, None)\n",
    "    threshold = threshold_ratio * dst.max()\n",
    "    corners = np.argwhere(dst > threshold)\n",
    "    keypoints = [cv2.KeyPoint(float(pt[1]), float(pt[0]), 1) for pt in corners]\n",
    "\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70779144",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for matplotlib plots \n",
    "DETECTOR_COLORS_MPL = {\n",
    "    \"orb\": 'green',\n",
    "    \"sift\": 'blue',\n",
    "    \"fast\": 'red',\n",
    "    \"shi_tomasi\": 'cyan',\n",
    "    \"harris\": 'magenta'\n",
    "}\n",
    "\n",
    "DETECTOR_COLORS = {\n",
    "    \"ORB\": (0, 255, 0),          \n",
    "    \"SIFT\": (255, 0, 0),         \n",
    "    \"FAST\": (0, 0, 255),         \n",
    "    \"Shi-Tomasi\": (255, 255, 0), \n",
    "    \"Harris\": (255, 0, 255)      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24720f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_detector_pairs(frames, matches: list, detector_name: str):\n",
    "    color = DETECTOR_COLORS.get(detector_name, (0, 255, 0))\n",
    "    keypoint_draw_flags = (\n",
    "        cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS\n",
    "        if detector_name in [\"SIFT\"]\n",
    "        else cv2.DrawMatchesFlags_DEFAULT\n",
    "    )\n",
    "\n",
    "    for match_result in matches:\n",
    "        i, j = match_result.frame_pair\n",
    "\n",
    "        img1 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "        img2 = cv2.cvtColor(frames[j], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        img_kp1 = cv2.drawKeypoints(img1, match_result.kp1, None, color=color, flags=keypoint_draw_flags)\n",
    "        img_kp2 = cv2.drawKeypoints(img2, match_result.kp2, None, color=color, flags=keypoint_draw_flags)\n",
    "\n",
    "        # Show keypoints on individual images\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        axes[0].imshow(img_kp1, cmap='gray')\n",
    "        axes[0].set_title(f\"{detector_name} - Frame {i}, Keypoints: {len(match_result.kp1)}\")\n",
    "\n",
    "        axes[1].imshow(img_kp2, cmap='gray')\n",
    "        axes[1].set_title(f\"{detector_name} - Frame {j}, Keypoints: {len(match_result.kp2)}\")\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        plt.suptitle(f\"{detector_name} Keypoints: Frame {i} ↔ Frame {j}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18681a9",
   "metadata": {},
   "source": [
    "## Combined Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keypoints_over_frames(results):\n",
    "    indices = results['frame_indices']\n",
    "\n",
    "    # Keypoints\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, data in results.items():\n",
    "        if name != 'frame_indices':\n",
    "            color = DETECTOR_COLORS_MPL.get(name, 'green')\n",
    "            plt.plot(indices, data['keypoint_counts'], label=f'{name.upper()} Keypoints', color=color)\n",
    "    plt.title(\"Keypoints per Frame\")\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Number of Keypoints\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_computationTime_over_frames(results):\n",
    "    # Computation time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, data in results.items():\n",
    "        if name != 'frame_indices':\n",
    "            color = DETECTOR_COLORS_MPL.get(name, 'green')\n",
    "            plt.plot(indices, data['computation_times'], label=f'{name.upper()} Time', color=color)\n",
    "    plt.title(\"Computation Time per Frame\")\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_keypoints_and_computTimes(results):\n",
    "\n",
    "    plot_keypoints_over_frames(results)\n",
    "    plot_computationTime_over_frames(results)\n",
    "\n",
    "    # Print averages\n",
    "    for name, data in results.items():\n",
    "        if name != 'frame_indices':\n",
    "            print(f\"Average {name.upper()}: {np.mean(data['keypoint_counts']):.1f} keypoints, {np.mean(data['computation_times']):.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_detectors_per_frame(frames):\n",
    "    detectors = {\n",
    "        \"ORB\": detect_orb,\n",
    "        \"SIFT\": detect_sift,\n",
    "        \"FAST\": detect_fast,\n",
    "        \"Shi-Tomasi\": detect_shi_tomasi,\n",
    "        \"Harris\": detect_harris\n",
    "    }\n",
    "\n",
    "    for idx in range(len(frames)):\n",
    "        if idx < 0 or idx >= len(frames):\n",
    "            print(f\"Skipping invalid frame index {idx}\")\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(frames[idx], cv2.COLOR_BGR2GRAY)\n",
    "        frame_results = {}\n",
    "\n",
    "        for name, fn in detectors.items():\n",
    "            keypoints, _ = fn(gray)\n",
    "                \n",
    "            color = DETECTOR_COLORS.get(name, (0, 255, 0))  # default green if missing\n",
    "            img_with_kp = cv2.drawKeypoints(frames[idx], keypoints, None, color)\n",
    "            frame_results[name] = (img_with_kp, len(keypoints))\n",
    "\n",
    "        # Plot all detector results in 3x3 grid\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "\n",
    "        # Flatten axes for easy iteration\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Plot the 5 detector images\n",
    "        for ax, (name, (img, count)) in zip(axes, frame_results.items()):\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            ax.set_title(f\"{name} ({count})\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        # Hide the remaining axes (empty slots)\n",
    "        for ax in axes[len(frame_results):]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Detector Comparison - Frame {idx}\", fontsize=16)\n",
    "        plt.subplots_adjust(left=0.05, right=0.95, top=0.90, bottom=0.05, wspace=0.05, hspace=0.1)\n",
    "        plt.show()\n",
    "\n",
    "def calculate_repeatability(keypoints1, keypoints2, homography, threshold=3):\n",
    "    \"\"\"\n",
    "    Calculates the repeatability score between two sets of keypoints given a homography.\n",
    "\n",
    "    Args:\n",
    "        keypoints1 (list): List of cv2.KeyPoint objects from the first image.\n",
    "        keypoints2 (list): List of cv2.KeyPoint objects from the second image.\n",
    "        homography (np.ndarray): The 3x3 homography matrix transforming points from image 1 to image 2.\n",
    "        threshold (int): The maximum pixel distance to consider a keypoint as a match.\n",
    "\n",
    "    Returns:\n",
    "        float: The repeatability score.\n",
    "        int: The number of corresponding keypoints found.\n",
    "    \"\"\"\n",
    "    if not keypoints1 or not keypoints2 or homography is None:\n",
    "        return 0.0, 0\n",
    "\n",
    "    # Get coordinates from keypoint objects\n",
    "    points1 = np.float32([kp.pt for kp in keypoints1]).reshape(-1, 1, 2)\n",
    "    points2 = np.float32([kp.pt for kp in keypoints2])\n",
    "\n",
    "    # Project keypoints from image 1 to image 2\n",
    "    projected_points1 = cv2.perspectiveTransform(points1, homography)\n",
    "    projected_points1 = projected_points1.reshape(-1, 2)\n",
    "\n",
    "    correspondences = 0\n",
    "    for pt1 in projected_points1:\n",
    "        # Calculate the distance from the projected point to all keypoints in the second image\n",
    "        distances = np.linalg.norm(points2 - pt1, axis=1)\n",
    "        # Check if the minimum distance is within the threshold\n",
    "        if np.min(distances) < threshold:\n",
    "            correspondences += 1\n",
    "\n",
    "    # Repeatability is the ratio of correspondences to the minimum number of keypoints\n",
    "    repeatability_score = correspondences / min(len(keypoints1), len(keypoints2))\n",
    "    return repeatability_score, correspondences\n",
    "\n",
    "def benchmark_detectors_with_repeatability(frames):\n",
    "    \"\"\"\n",
    "    Benchmarks detectors for keypoint count, computation time, and repeatability.\n",
    "    \"\"\"\n",
    "    detectors = {\n",
    "        \"ORB\": detect_orb,\n",
    "        \"SIFT\": detect_sift,\n",
    "        \"FAST\": detect_fast,\n",
    "        \"Shi-Tomasi\": detect_shi_tomasi,\n",
    "        \"Harris\": detect_harris\n",
    "    }\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    results = {name: {'keypoint_counts': [], 'computation_times': [], 'repeatability': []} for name in detectors}\n",
    "    results['frame_indices'] = list(range(len(frames)))\n",
    "\n",
    "    # --- Part 1: Gather per-frame stats ---\n",
    "    for i, frame in enumerate(frames):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        for name, detector_fn in detectors.items():\n",
    "            start = time.time()\n",
    "            keypoints, _ = detector_fn(gray)\n",
    "            elapsed = time.time() - start\n",
    "            results[name]['keypoint_counts'].append(len(keypoints))\n",
    "            results[name]['computation_times'].append(elapsed)\n",
    "\n",
    "    # --- Part 2: Calculate repeatability between consecutive frames ---\n",
    "    sift = cv2.SIFT_create()\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2)\n",
    "\n",
    "    for i in range(len(frames) - 1):\n",
    "        frame1 = frames[i]\n",
    "        frame2 = frames[i+1]\n",
    "        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find a \"ground-truth\" homography between the frames using a reliable method\n",
    "        kp1_h, des1_h = sift.detectAndCompute(gray1, None)\n",
    "        kp2_h, des2_h = sift.detectAndCompute(gray2, None)\n",
    "        \n",
    "        homography = None\n",
    "        if des1_h is not None and des2_h is not None and len(des1_h) > 10 and len(des2_h) > 10:\n",
    "            matches = bf.knnMatch(des1_h, des2_h, k=2)\n",
    "            good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "            \n",
    "            if len(good_matches) > 10: # Minimum matches to find homography\n",
    "                src_pts = np.float32([kp1_h[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "                dst_pts = np.float32([kp2_h[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "                homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Now, calculate repeatability for each of our target detectors\n",
    "        for name, detector_fn in detectors.items():\n",
    "            # Detect keypoints for each frame with the current detector\n",
    "            keypoints1, _ = detector_fn(gray1)\n",
    "            keypoints2, _ = detector_fn(gray2)\n",
    "\n",
    "            score, _ = calculate_repeatability(keypoints1, keypoints2, homography)\n",
    "            # We append the score for the first frame of the pair\n",
    "            results[name]['repeatability'].append(score)\n",
    "\n",
    "    # Since repeatability is calculated between pairs, the last frame has no score.\n",
    "    for name in detectors:\n",
    "        results[name]['repeatability'].append(0.0) \n",
    "\n",
    "    # --- Part 3: Plot the results ---\n",
    "    plot_all_benchmarks(results) # A new comprehensive plot function\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_all_benchmarks(results):\n",
    "    \"\"\"Plots keypoint counts, computation times, and repeatability.\"\"\"\n",
    "    detector_names = list(results.keys())\n",
    "    detector_names.remove('frame_indices')\n",
    "    frame_indices = results['frame_indices']\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 18), sharex=True)\n",
    "    \n",
    "    # Plot Keypoint Counts\n",
    "    for name in detector_names:\n",
    "        ax1.plot(frame_indices, results[name]['keypoint_counts'], linestyle='-', label=name)\n",
    "    ax1.set_ylabel('Number of Keypoints')\n",
    "    ax1.set_title('Keypoint Detection Count per Frame')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot Computation Times\n",
    "    for name in detector_names:\n",
    "        ax2.plot(frame_indices, results[name]['computation_times'], linestyle='-', label=name)\n",
    "    ax2.set_ylabel('Computation Time (seconds)')\n",
    "    ax2.set_title('Computation Time per Frame')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot Repeatability\n",
    "    for name in detector_names:\n",
    "        # We plot up to the second to last frame, as the last has no subsequent pair\n",
    "        ax3.plot(frame_indices[:-1], results[name]['repeatability'][:-1], linestyle='-', label=name)\n",
    "    ax3.set_xlabel('Frame Index')\n",
    "    ax3.set_ylabel('Repeatability Score')\n",
    "    ax3.set_title('Repeatability Score between Consecutive Frames')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63df05",
   "metadata": {},
   "source": [
    "# 4. Feature Matching and Outlier Rejection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460af133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matcher(detector_type: str):\n",
    "    detector_type = detector_type.lower()\n",
    "    if detector_type in ['sift', 'surf', 'fast']:\n",
    "        # FLANN for float descriptors\n",
    "        index_params = dict(algorithm=1, trees=5)  # KDTree\n",
    "        search_params = dict(checks=50)\n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        descriptor_type = 'float'\n",
    "    elif detector_type in ['orb', 'brief', 'shi-tomasi', 'harris']:\n",
    "        # Brute-force for binary descriptors\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "        descriptor_type = 'binary'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported detector type: {detector_type}\")\n",
    "    \n",
    "    return matcher, descriptor_type\n",
    "\n",
    "def match_features(frames, detector_type, detect_func, ratio_thresh=0.75):\n",
    "    matcher, descriptor_type = get_matcher(detector_type)\n",
    "    match_results = []\n",
    "\n",
    "    for start_idx in range(len(frames)-1):\n",
    "        end_idx = start_idx + 1\n",
    "\n",
    "        if start_idx >= len(frames) or end_idx >= len(frames):\n",
    "            continue\n",
    "\n",
    "        gray1, gray2 = convert_frames_to_grayscale(frames[start_idx], frames[end_idx])\n",
    "        \n",
    "\n",
    "        kp1, des1 = detect_func(gray1)\n",
    "        kp2, des2 = detect_func(gray2)\n",
    "\n",
    "\n",
    "        if des1 is None or des2 is None:\n",
    "            continue\n",
    "        \n",
    "        if descriptor_type == 'float':\n",
    "            des1 = des1.astype(np.float32)\n",
    "            des2 = des2.astype(np.float32)\n",
    "\n",
    "        knn_matches = matcher.knnMatch(des1, des2, k=2)\n",
    "        \n",
    "        good_matches = []\n",
    "        for match_pair in knn_matches:\n",
    "            if len(match_pair) == 2:\n",
    "                m, n = match_pair\n",
    "                if m.distance < ratio_thresh * n.distance:\n",
    "                    good_matches.append(m)\n",
    "\n",
    "        if len(good_matches) < 10:\n",
    "            continue\n",
    "\n",
    "        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
    "\n",
    "        result = MatchResult(\n",
    "            frame_pair=(start_idx, end_idx),\n",
    "            kp1=kp1,\n",
    "            kp2=kp2,\n",
    "            pts1=pts1,\n",
    "            pts2=pts2,\n",
    "            matches=good_matches\n",
    "        )\n",
    "\n",
    "        match_results.append(result)\n",
    "        print(f\"[{detector_type.upper()}] Matched {start_idx} → {end_idx} : {len(good_matches)} matches\")\n",
    "\n",
    "    return match_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860deb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANSAC removes outlier matches to improve geometric consistency\n",
    "# Higher threshold = more lenient (accepts points further from predicted position)\n",
    "def apply_ransac_filter(match_result:MatchResult, reproj_thresh=4.0)-> MatchResult:\n",
    "    H, mask = cv2.findHomography(match_result.pts1, match_result.pts2, cv2.RANSAC, reproj_thresh)\n",
    "    mask = mask.ravel()\n",
    "\n",
    "    inlier_matches = [m for m, keep in zip(match_result.matches, mask) if keep]\n",
    "    pts1_inliers = match_result.pts1[mask == 1]\n",
    "    pts2_inliers = match_result.pts2[mask == 1]\n",
    "\n",
    "    return MatchResult(\n",
    "        frame_pair=match_result.frame_pair,\n",
    "        kp1=match_result.kp1,\n",
    "        kp2=match_result.kp2,\n",
    "        pts1=pts1_inliers,\n",
    "        pts2=pts2_inliers,\n",
    "        matches=inlier_matches\n",
    "    )\n",
    "\n",
    "def plot_before_after_ransac(img1:Frame, img2:Frame, match_before: MatchResult, match_after: MatchResult) -> None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "    # Before RANSAC\n",
    "    img_before = cv2.drawMatches(\n",
    "        img1, match_before.kp1,\n",
    "        img2, match_before.kp2,\n",
    "        match_before.matches,\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_DEFAULT\n",
    "    )\n",
    "    axes[0].imshow(img_before)\n",
    "    axes[0].set_title(f\"Before RANSAC - Matches: {len(match_before.matches)}\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # After RANSAC\n",
    "    img_after = cv2.drawMatches(\n",
    "        img1, match_after.kp1,\n",
    "        img2, match_after.kp2,\n",
    "        match_after.matches,\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_DEFAULT\n",
    "    )\n",
    "    axes[1].imshow(img_after)\n",
    "    axes[1].set_title(f\"After RANSAC - Inliers: {len(match_after.matches)}\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    fig.suptitle(f\"Frames:{match_before.frame_pair}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213ae0e",
   "metadata": {},
   "source": [
    "# 5. Essential/Fundamental Matrix Computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the fundamental matrix from matched keypoints\n",
    "def compute_fundamental_matrix(matches, keypoints1, keypoints2, method = cv2.FM_RANSAC, ransac_threshold = 3.0, confidence = 0.99):\n",
    "  # Extract coordinates of matched keypoints\n",
    "  points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])\n",
    "  points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])\n",
    "  \n",
    "  # Compute the fundamental matrix\n",
    "  fundamental_matrix, inlier_mask = cv2.findFundamentalMat(points1, points2, method = method, ransacReprojThreshold = ransac_threshold, confidence = confidence)\n",
    "  \n",
    "  # Convert mask to binary array for easier filtering\n",
    "  if fundamental_matrix is None or fundamental_matrix.shape != (3,3):\n",
    "    raise ValueError(\"Failed to compute a valid fundamental matrix\")\n",
    "  \n",
    "  return fundamental_matrix, inlier_mask\n",
    "\n",
    "# Clip epipolar lines to image boundaries\n",
    "def draw_epipolar_line_clipped(img_shape, line):\n",
    "  h, w = img_shape[:2]\n",
    "    \n",
    "  # Find intersections with image borders\n",
    "  points = []\n",
    "    \n",
    "  # Left border (x = 0)\n",
    "  if abs(line[1]) > 1e-6:\n",
    "    y = -line[2] / line[1]\n",
    "    if 0 <= y <= h:\n",
    "      points.append((0, int(y)))\n",
    "    \n",
    "  # Right border (x = w-1)\n",
    "  if abs(line[1]) > 1e-6:\n",
    "    y = -(line[2] + line[0] * (w-1)) / line[1]\n",
    "    if 0 <= y <= h:\n",
    "      points.append((w-1, int(y)))\n",
    "    \n",
    "  # Top border (y = 0)\n",
    "  if abs(line[0]) > 1e-6:\n",
    "    x = -line[2] / line[0]\n",
    "    if 0 <= x <= w:\n",
    "      points.append((int(x), 0))\n",
    "    \n",
    "  # Bottom border (y = h-1)\n",
    "  if abs(line[0]) > 1e-6:\n",
    "    x = -(line[2] + line[1] * (h-1)) / line[0]\n",
    "    if 0 <= x <= w:\n",
    "      points.append((int(x), h-1))\n",
    "    \n",
    "  # Remove duplicate points and return first two\n",
    "  unique_points = []\n",
    "  for p in points:\n",
    "    if not any(abs(p[0]-up[0]) < 2 and abs(p[1]-up[1]) < 2 for up in unique_points):\n",
    "      unique_points.append(p)\n",
    "    \n",
    "  return unique_points[:2] if len(unique_points) >= 2 else []\n",
    "\n",
    "# Visualise epipolar lines to verify the fundamental matrix\n",
    "def visualise_epipolar_lines(img1, img2, points1, points2, fundamental_matrix, sample_size = 20):\n",
    "  # Sample points if there are too many\n",
    "  if len(points1) > sample_size:\n",
    "    indices = np.random.choice(len(points1), sample_size, replace = False)\n",
    "    pts1 = points1[indices]\n",
    "    pts2 = points2[indices]\n",
    "  else:\n",
    "    pts1 = points1\n",
    "    pts2 = points2\n",
    "\n",
    "  # Create a new figure for each call\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 8))\n",
    "\n",
    "  # Convert images to RGB for matplotlib\n",
    "  img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "  img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  # Display the first image\n",
    "  ax1.imshow(img1_rgb) # Use RGB image\n",
    "  ax1.set_title('Epipolar Lines on Image 1')\n",
    "  ax1.axis('off')\n",
    "\n",
    "  # Display the second image\n",
    "  ax2.imshow(img2_rgb) # Use RGB image\n",
    "  ax2.set_title('Epipolar Lines on Image 2')\n",
    "  ax2.axis('off')\n",
    "\n",
    "  # Draw epipolar lines on both images\n",
    "  for i in range(len(pts1)):\n",
    "    # Draw points\n",
    "    ax1.plot(pts1[i, 0], pts1[i, 1], 'ro', markersize = 6)\n",
    "    ax2.plot(pts2[i, 0], pts2[i, 1], 'ro', markersize = 6)\n",
    "\n",
    "    # Compute epipolar line in second image for point in first image\n",
    "    line2 = cv2.computeCorrespondEpilines(pts1[i].reshape(-1, 1, 2), 1, fundamental_matrix)\n",
    "    line2 = line2.reshape(-1)\n",
    "\n",
    "    # Get clipped line endpoints for image 2\n",
    "    endpoints2 = draw_epipolar_line_clipped(img2.shape, line2)\n",
    "    if len(endpoints2) == 2:\n",
    "        ax2.plot([endpoints2[0][0], endpoints2[1][0]], [endpoints2[0][1], endpoints2[1][1]], 'g-', linewidth=1)\n",
    "\n",
    "    # Compute epipolar line in first image for point in second image\n",
    "    line1 = cv2.computeCorrespondEpilines(pts2[i].reshape(-1, 1, 2), 2, fundamental_matrix)\n",
    "    line1 = line1.reshape(-1)\n",
    "\n",
    "    # Get clipped line endpoints for image 1\n",
    "    endpoints1 = draw_epipolar_line_clipped(img1.shape, line1)\n",
    "    if len(endpoints1) == 2:\n",
    "        ax1.plot([endpoints1[0][0], endpoints1[1][0]], [endpoints1[0][1], endpoints1[1][1]], 'g-', linewidth=1)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  plt.close(fig)\n",
    "\n",
    "# Calculate the epipolar geometry error to evaluate the quality of the fundamental matrix\n",
    "'''\n",
    "Tips on Evaluating Epipolar Error:\n",
    "\n",
    "- Ideal: As close to 0 as possible (perfect geometry).\n",
    "- Practical: For real-world data, a mean epipolar error of less than 1 pixel is considered very good. Values up to 2-3\n",
    "pixels are often acceptable, depending on image resolution, noise, and feature localization accuracy.\n",
    "- If error > 5 pixels: This usually indicates problems with matching, calibration, or outliers.\n",
    "'''\n",
    "def epipolar_error(points1, points2, fundamental_matrix):\n",
    "  # Convert each point to homogeneous coordinates\n",
    "  homogeneous_points1 = np.hstack((points1, np.ones((points1.shape[0],1))))\n",
    "  homogeneous_points2 = np.hstack((points2, np.ones((points2.shape[0],1))))\n",
    "  \n",
    "  # Calculate epipolar lines for points in image 1\n",
    "  lines2 = np.dot(homogeneous_points1, fundamental_matrix.T)\n",
    "  # Normalise lines\n",
    "  norms2 = np.sqrt(lines2[:, 0]**2 + lines2[:, 1]**2)\n",
    "  lines2 = lines2 / norms2.reshape(-1,1)\n",
    "  # Calculate the distance from points in image 2 to their corresponding epipolar lines\n",
    "  dist2 = np.abs(np.sum(lines2 * homogeneous_points2, axis = 1))\n",
    "  \n",
    "  # Calculate epipolar lines for points in image 2\n",
    "  lines1 = np.dot(homogeneous_points2, fundamental_matrix)\n",
    "  # Normalise lines\n",
    "  norms1 = np.sqrt(lines1[:, 0]**2 + lines1[:, 1]**2)\n",
    "  lines1 = lines1 / norms1.reshape(-1,1)\n",
    "  # Calculate the distance from points in image 1 to their corresponding epipolar lines\n",
    "  dist1 = np.abs(np.sum(lines1 * homogeneous_points1, axis = 1))\n",
    "  \n",
    "  metrics = {\n",
    "    \"mean_error\": (np.mean(dist1) + np.mean(dist2)) / 2,\n",
    "    \"max_error\": max(np.max(dist1), np.max(dist2)),\n",
    "    \"std_error\": (np.std(dist1) + np.std(dist2)) / 2\n",
    "  }\n",
    "  \n",
    "  return metrics\n",
    "\n",
    "# Main function to compute the fundamental matrix\n",
    "def process_fundamental_matrix(imgs, matches, keypoints1, keypoints2, visualise = True):\n",
    "  # Compute fundamental matrix\n",
    "  print(\"Computing fundamental matrix...\")\n",
    "  F, inlier_mask = compute_fundamental_matrix(matches, keypoints1, keypoints2)\n",
    "    \n",
    "  # Filter matches based on inlier mask\n",
    "  inlier_matches = [m for i, m in enumerate(matches) if inlier_mask[i]]\n",
    "  print(f\"Inlier matches: {len(inlier_matches)} out of {len(matches)} ({len(inlier_matches) / len(matches) * 100:.2f}%)\")\n",
    "    \n",
    "  # Extract coordinates of inlier keypoints\n",
    "  inlier_points1 = np.float32([keypoints1[m.queryIdx].pt for m in inlier_matches])\n",
    "  inlier_points2 = np.float32([keypoints2[m.trainIdx].pt for m in inlier_matches])\n",
    "    \n",
    "  # Calculate error metrics\n",
    "  error_metrics = epipolar_error(inlier_points1, inlier_points2, F)\n",
    "  print(f\"Mean epipolar error: {error_metrics['mean_error']:.4f} pixels\")\n",
    "        \n",
    "  # Visualise epipolar lines if requested\n",
    "  if visualise and len(imgs) >= 2:\n",
    "      visualise_epipolar_lines(imgs[0], imgs[1], inlier_points1, inlier_points2, F)\n",
    "    \n",
    "  # Prepare results\n",
    "  results = {\n",
    "      \"fundamental_matrix\": F,\n",
    "      \"inlier_mask\": inlier_mask,\n",
    "      \"inlier_matches\": inlier_matches,\n",
    "      \"inlier_points1\": inlier_points1,\n",
    "      \"inlier_points2\": inlier_points2,\n",
    "      \"error_metrics\": error_metrics,\n",
    "  }\n",
    "    \n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1123511",
   "metadata": {},
   "source": [
    "# 6. Camera Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CameraIntrinsics:\n",
    "  fx: float\n",
    "  fy: float\n",
    "  cx: float\n",
    "  cy: float\n",
    "  \n",
    "  def matrix(self):\n",
    "    return np.array([\n",
    "      [self.fx, 0, self.cx],\n",
    "      [0, self.fy, self.cy],\n",
    "      [0, 0, 1]\n",
    "    ],dtype = np.float64)\n",
    "    \n",
    "# Estimate approximate intrinsics for a phone camera\n",
    "def estimate_camera_intrinsics(image_shape):\n",
    "  height, width = image_shape\n",
    "  \n",
    "  fov_degrees = 75\n",
    "  fov_radians = np.radians(fov_degrees)\n",
    "  f = (width / 2.0) / np.tan(fov_radians / 2.0)\n",
    "  \n",
    "  # Principal point at image center\n",
    "  cx = width / 2\n",
    "  cy = height / 2\n",
    "  \n",
    "  print(f\"Intrinsics fx = fy = {f:.1f}, cx = {cx:.1f}, cy = {cy:.1f}\")\n",
    "  return CameraIntrinsics(f, f, cx, cy)\n",
    "\n",
    "# Compute the essential matrix from the fundamental matrix using the formula:\n",
    "# E = K^T * F * K\n",
    "def convert_fundamental_to_essential_matrix(F, K):\n",
    "  E = K.T @ F @ K\n",
    "  return E\n",
    "\n",
    "# Recover relative camera pose (R, t) from the essential matrix\n",
    "def recover_camera_pose(E, pts1, pts2, K):\n",
    "  pts1 = np.asarray(pts1, dtype = np.float32)\n",
    "  pts2 = np.asarray(pts2, dtype = np.float32)\n",
    "  \n",
    "  # Make sure points have correct shape\n",
    "  if pts1.ndim == 1:\n",
    "    pts1 = pts1.reshape(-1,2)\n",
    "  if pts2.ndim == 1:\n",
    "    pts2 = pts2.reshape(-1,2)\n",
    "  \n",
    "  try:\n",
    "    # Chirality problem - Funtion automatically tests all 4 possible combinations of R and t\n",
    "    inliers, R, t, mask = cv2.recoverPose(E, pts1, pts2, K)\n",
    "    print(f\"Pose Inliers: {inliers}/{len(pts1)}\")\n",
    "    \n",
    "    if inliers < 5:\n",
    "      print(\"Very few inliers for pose recovery\")\n",
    "    \n",
    "    return R, t, mask\n",
    "  except cv2.error as e:\n",
    "    print(f\"OpenCV error in pose recovery: {e}\")\n",
    "     # Return identity transformation as fallback\n",
    "    return np.eye(3), np.zeros((3, 1)), np.zeros(len(pts1), dtype = np.uint8)\n",
    "\n",
    "# Validity checks on the recovered pose\n",
    "def validate_pose(R, t):\n",
    "  metrics = {}\n",
    "  det_R = np.linalg.det(R)\n",
    "  metrics['det_R'] = det_R\n",
    "  metrics['is_rotation_valid'] = np.abs(det_R - 1.0) < 1e-6\n",
    "\n",
    "  orthogonality_error = np.linalg.norm(R @ R.T - np.eye(3))\n",
    "  metrics['orthogonality_error'] = orthogonality_error\n",
    "  metrics['is_orthogonal'] = orthogonality_error < 1e-6\n",
    "\n",
    "  t_mag = np.linalg.norm(t)\n",
    "  metrics['translation_magnitude'] = t_mag\n",
    "\n",
    "  trace = np.trace(R)\n",
    "  trace = np.clip(trace, -1.0, 3.0)\n",
    "  angle_rad = np.arccos((trace - 1) / 2)\n",
    "  metrics['rotation_angle_deg'] = np.degrees(angle_rad)\n",
    "\n",
    "  return metrics\n",
    "\n",
    "# Simple 3D plot of the initial and recovered camera pose\n",
    "def visualise_camera_poses(R, t):\n",
    "  try:\n",
    "    fig = plt.figure(figsize = (10,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "    cam1 = np.array([0, 0, 0])\n",
    "    cam2 = -R.T @ t.flatten()  # Camera center in world coordinates\n",
    "    \n",
    "    if cam2.ndim > 1:\n",
    "      cam2 = cam2.flatten()\n",
    "    \n",
    "    ax.scatter(cam1[0], cam1[1], cam1[2], c = 'blue', s = 50, label = 'Camera 1 (origin)')\n",
    "    ax.scatter(cam2[0], cam2[1], cam2[2], c = 'red', s = 50, label = 'Camera 2')\n",
    "    ax.plot([cam1[0], cam2[0]], [cam1[1], cam2[1]], [cam1[2], cam2[2]], 'k--', linewidth = 2, label = 'Baseline')\n",
    "\n",
    "    # Add coordinate axes for camera orientations\n",
    "    axes_length = 0.3\n",
    "        \n",
    "    # Camera 1 axes (identity rotation)\n",
    "    ax.plot([0, axes_length], [0, 0], [0, 0], 'r-', alpha = 0.7, linewidth = 2)  # X-axis\n",
    "    ax.plot([0, 0], [0, axes_length], [0, 0], 'g-', alpha = 0.7, linewidth = 2)  # Y-axis\n",
    "    ax.plot([0, 0], [0, 0], [0, axes_length], 'b-', alpha = 0.7, linewidth = 2)  # Z-axis\n",
    "        \n",
    "    # Camera 2 axes (rotated)\n",
    "    axes = axes_length * R.T  # Camera coordinate axes in world frame\n",
    "    origin = cam2\n",
    "        \n",
    "    x_end = origin + axes[:, 0]\n",
    "    y_end = origin + axes[:, 1] \n",
    "    z_end = origin + axes[:, 2]\n",
    "        \n",
    "    ax.plot([origin[0], x_end[0]], [origin[1], x_end[1]], [origin[2], x_end[2]], 'r-', alpha = 0.7, linewidth = 2)\n",
    "    ax.plot([origin[0], y_end[0]], [origin[1], y_end[1]], [origin[2], y_end[2]], 'g-', alpha = 0.7, linewidth = 2)\n",
    "    ax.plot([origin[0], z_end[0]], [origin[1], z_end[1]], [origin[2], z_end[2]], 'b-', alpha = 0.7, linewidth = 2)\n",
    "\n",
    "    ax.set_xlabel('X') # red line\n",
    "    ax.set_ylabel('Y') # green line\n",
    "    ax.set_zlabel('Z') # blue line\n",
    "    ax.set_title('Camera Poses')\n",
    "    ax.legend()\n",
    "        \n",
    "    # Set equal aspect ratio\n",
    "    max_range = max(np.abs(cam2).max(), 0.5)\n",
    "    ax.set_xlim([-max_range, max_range])\n",
    "    ax.set_ylim([-max_range, max_range])\n",
    "    ax.set_zlim([-max_range, max_range])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  except Exception as e:\n",
    "    print(f\"Visualisation error: {e}\")\n",
    "    print(\"Skipping visualisation process\")\n",
    "\n",
    "# Takes in fundamental matrix and inlier points computed from stage 5\n",
    "def run_pose_estimation(F, pts1, pts2, frame_shape, visualise = True):\n",
    "  print(\"Camera Pose Estimation: \")\n",
    "  \n",
    "  if F is None or F.shape != (3,3):\n",
    "    raise ValueError(\"Invalid fundamental matrix\")\n",
    "  \n",
    "  if len(pts1) < 5 or len(pts2) < 5:\n",
    "    raise ValueError(\"Needs at least 5 point correspondences\")\n",
    "  \n",
    "  try:\n",
    "    # Estimate the intrinsics\n",
    "    intrinsics = estimate_camera_intrinsics(frame_shape)\n",
    "    K = intrinsics.matrix()\n",
    "    \n",
    "    # Deduce the essential matrix\n",
    "    E = convert_fundamental_to_essential_matrix(F, K)\n",
    "    print(\"Essential Matrix\\n\", E)\n",
    "    \n",
    "    # Check if essential matrix is valid using SVD\n",
    "    U, S, Vt = np.linalg.svd(E)\n",
    "    print(f\"Essential matrix singular values: {S}\")\n",
    "    \n",
    "    # Recover pose\n",
    "    R, t, mask = recover_camera_pose(E, pts1, pts2, K)\n",
    "    \n",
    "    # Validation step\n",
    "    pose_metrics = validate_pose(R, t)\n",
    "    print(\"\\nPose Validation: \")\n",
    "    for k, v in pose_metrics.items():\n",
    "      print(f\"{k}: {v}\")\n",
    "      \n",
    "    if not pose_metrics['is_rotation_valid']:\n",
    "      print(\"Invalid rotation matrix\")\n",
    "      \n",
    "    if pose_metrics['rotation_angle_deg'] > 90:\n",
    "      print(f\"Large rotation angle {pose_metrics['rotation_angle_deg']:.1f}\")\n",
    "    \n",
    "    # Check for degenerate \n",
    "    if pose_metrics['rotation_angle_deg'] < 0.1:\n",
    "      print(\"Very small rotation - possible degenerate case\")\n",
    "    \n",
    "    # Visualise the initial and recovered camera pose\n",
    "    if visualise:\n",
    "      visualise_camera_poses(R, t)\n",
    "      \n",
    "    return {\n",
    "      \"R\": R,\n",
    "      \"t\": t,\n",
    "      \"K\": K,\n",
    "      \"E\": E,\n",
    "      \"mask\": mask,\n",
    "      \"metrics\": pose_metrics\n",
    "    }\n",
    "  except Exception as e:\n",
    "    print(f\"Error in pose estimation: {e}\")\n",
    "    return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to integrate stages 4-6 together\n",
    "def run_pose_estimation_from_matches(match_result, frame_shape, fundamental_matrix, visualise):\n",
    "  return run_pose_estimation(F = fundamental_matrix, pts1 = match_result.pts1, pts2 = match_result.pts2, frame_shape = frame_shape, visualise=visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a4d20",
   "metadata": {},
   "source": [
    "# 7. 3D Point Triangulation and Scene Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954360ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_points(K, camera1_pose, camera2_pose, points1, points2):\n",
    "    # Triangulate points in 3D space from 2D correspondences and camera parameters\n",
    "\n",
    "    # Input validation\n",
    "    if points1.shape[0] != points2.shape[0]:\n",
    "        print(\"Error: Input point arrays must have the same number of points.\")\n",
    "        return np.array([])\n",
    "\n",
    "    # Deconstruct poses\n",
    "    R1, t1 = camera1_pose\n",
    "    R2, t2 = camera2_pose\n",
    "\n",
    "    # Compute projection matrices P = K[R|t]\n",
    "    P1 = K @ np.hstack((R1, t1))\n",
    "    P2 = K @ np.hstack((R2, t2))\n",
    "        \n",
    "    # Triangulate points using OpenCV\n",
    "    # cv2.triangulatePoints requires points in (2, N) format, so we transpose our (N, 2) inputs\n",
    "    points_4d_hom = cv2.triangulatePoints(P1, P2, points1.T, points2.T)\n",
    "    \n",
    "    # Convert homogeneous coordinates (x,y,z,w) to Euclidean coordinates (x,y,z)\n",
    "    points_3d = (points_4d_hom[:3, :] / points_4d_hom[3, :]).T\n",
    "    \n",
    "    return points_3d\n",
    "\n",
    "def get_pixel_colors(img, coordinate_list):\n",
    "    # Initialize empty array for color values\n",
    "    pixel_values = []\n",
    "    image_height, image_width = img.shape[:2]\n",
    "    \n",
    "    # Process each coordinate\n",
    "    for coord in coordinate_list:\n",
    "        col, row = map(lambda x: int(x + 0.5), coord)  # Alternative rounding approach\n",
    "        \n",
    "        # Validate coordinates\n",
    "        is_valid = (0 <= col < image_width) and (0 <= row < image_height)\n",
    "        \n",
    "        if is_valid:\n",
    "            # Normalize RGB values to [0,1] range\n",
    "            rgb = img[row, col].astype(float) / 255\n",
    "        else:\n",
    "            rgb = np.array([0.5, 0.5, 0.5])  # Gray color for invalid points\n",
    "            \n",
    "        pixel_values.append(rgb)\n",
    "    \n",
    "    return np.array(pixel_values)\n",
    "\n",
    "def visualize_reconstruction(points_3d, colors, window_name):\n",
    "    # Create a point cloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    # Flip Y and Z coordinates for correct orientation\n",
    "    points_3d = points_3d.copy()\n",
    "    points_3d[:, 1] = -points_3d[:, 1] # Flip Y coordinate\n",
    "    points_3d[:, 2] = -points_3d[:, 2] # Flip Z coordinate\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(points_3d)\n",
    "    # Add RGB colors to the points\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    print(\"\\nVisualizing point cloud.\")\n",
    "    print(\"Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\")\n",
    "    print(\"Press 'q' to close the window.\")\n",
    "\n",
    "    # Display the 3D point cloud\n",
    "    o3d.visualization.draw_geometries([pcd], window_name=window_name)\n",
    "\n",
    "def bundle_adjust_poses(camera_poses, all_points_3d, all_matches, K, max_iterations=10):\n",
    "    # Iterative bundle adjustment to refine camera poses and 3D points.\n",
    "    print(f\"Starting bundle adjustment with {len(camera_poses)} poses...\")\n",
    "    \n",
    "    # Return early if no 3D points exist\n",
    "    if not all_points_3d:\n",
    "        print(\"No 3D points provided for bundle adjustment!\")\n",
    "        return camera_poses, []\n",
    "    \n",
    "    # Create copies to avoid modifying inputs\n",
    "    refined_poses = camera_poses.copy()\n",
    "    refined_points_3d = [points.copy() for points in all_points_3d]  # Keep as list of arrays\n",
    "    \n",
    "    # Iteratively refine poses up to max_iterations times\n",
    "    for iter_idx in range(max_iterations):\n",
    "        total_reproj_error = 0\n",
    "        pose_updates = 0\n",
    "        \n",
    "        # Iterate through each camera pose after first reference camera\n",
    "        for cam_idx in range(1, len(camera_poses)):\n",
    "            # Skip if we don't have matching data for this camera\n",
    "            if cam_idx - 1 >= len(all_matches):\n",
    "                continue\n",
    "                \n",
    "            # Get corresponding 2D-3D point pairs for this camera\n",
    "            match_data = all_matches[cam_idx - 1]\n",
    "            frame_points_3d = refined_points_3d[cam_idx - 1]  # Use refined points\n",
    "            image_points = match_data.pts2.astype(np.float32)\n",
    "            object_points = frame_points_3d.astype(np.float32)\n",
    "            \n",
    "            # Need at least 6 points for reliable pose estimation\n",
    "            if len(object_points) < 6:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Initialize with current pose estimate\n",
    "                R_current, t_current = refined_poses[cam_idx]\n",
    "                rvec_init, _ = cv2.Rodrigues(R_current)\n",
    "                tvec_init = t_current.flatten()\n",
    "                \n",
    "                # Use PnP RANSAC to optimize the camera pose\n",
    "                success, rvec_refined, tvec_refined, inliers = cv2.solvePnPRansac(\n",
    "                    object_points.reshape(-1, 1, 3),\n",
    "                    image_points.reshape(-1, 1, 2),\n",
    "                    K,\n",
    "                    None,\n",
    "                    rvec_init,\n",
    "                    tvec_init,\n",
    "                    useExtrinsicGuess=True,\n",
    "                    iterationsCount=100,\n",
    "                    reprojectionError=2.0,\n",
    "                    confidence=0.99,\n",
    "                    flags=cv2.SOLVEPNP_ITERATIVE\n",
    "                )\n",
    "                \n",
    "                # Only update pose if we have a good solution with sufficient inliers\n",
    "                if success and inliers is not None and len(inliers) > len(object_points) * 0.5:\n",
    "                    R_refined, _ = cv2.Rodrigues(rvec_refined)\n",
    "                    t_refined = tvec_refined.reshape(3, 1)\n",
    "                    \n",
    "                    refined_poses[cam_idx] = (R_refined, t_refined)\n",
    "                    pose_updates += 1\n",
    "                    \n",
    "            except cv2.error as e:\n",
    "                print(f\"Camera {cam_idx}: PnP failed - {e}\")\n",
    "                continue\n",
    "                \n",
    "        # Early stopping if very few poses were updated\n",
    "        if pose_updates < len(camera_poses) * 0.2:\n",
    "            print(\"Early stopping due to few pose updates\")\n",
    "            break\n",
    "    \n",
    "    return refined_poses, refined_points_3d\n",
    "\n",
    "def analyze_reconstruction_quality(points_3d, points1, points2, P1, P2, baseline_length):\n",
    "    # Function to analyze quality metrics for 3D reconstruction\n",
    "\n",
    "    # Convert 3D points to homogeneous coordinates by adding 1 as fourth dimension\n",
    "    # Project 3D points back into 2D using both camera matrices\n",
    "    points_3d_hom = np.hstack([points_3d, np.ones((len(points_3d), 1))])\n",
    "    reproj1 = (P1 @ points_3d_hom.T)\n",
    "    reproj2 = (P2 @ points_3d_hom.T)\n",
    "\n",
    "    # Convert back from homogeneous to euclidean coordinates\n",
    "    reproj1 = (reproj1[:2, :] / reproj1[2, :]).T\n",
    "    reproj2 = (reproj2[:2, :] / reproj2[2, :]).T\n",
    "    \n",
    "    # Calculate reprojection error as L2 distance between original and reprojected points\n",
    "    error1 = np.linalg.norm(reproj1 - points1, axis=1)\n",
    "    error2 = np.linalg.norm(reproj2 - points2, axis=1)\n",
    "    mean_reproj_error = (np.mean(error1) + np.mean(error2)) / 2\n",
    "    \n",
    "    # Calculate distances of reconstructed 3D points from origin\n",
    "    distances = np.linalg.norm(points_3d, axis=1)\n",
    "        \n",
    "    # Print statistics\n",
    "    print(f\"\\nReconstruction Quality Metrics:\")\n",
    "    print(f\"Points: {len(points_3d)}\")\n",
    "    print(f\"Reprojection error: {mean_reproj_error:.2f} pixels\")\n",
    "    print(f\"Baseline: {baseline_length:.3f}\")\n",
    "    print(f\"Scale ratio: {np.max(distances)/baseline_length:.1f}x\")\n",
    "    \n",
    "    # Return dictionary with key metrics\n",
    "    return {\n",
    "        'mean_reprojection_error': mean_reproj_error,\n",
    "        'num_points': len(points_3d),\n",
    "        'scale_ratio': np.max(distances)/baseline_length\n",
    "    }\n",
    "\n",
    "def save_point_cloud_ply(filename, points, colors):\n",
    "    # Save 3D points with colors to a .ply file\n",
    "\n",
    "    # Create reconstructions directory if it doesn't exist\n",
    "    reconstruction_dir = 'reconstructions'\n",
    "    os.makedirs(reconstruction_dir, exist_ok=True)\n",
    "\n",
    "    # Join path with filename\n",
    "    filepath = os.path.join(reconstruction_dir, filename)\n",
    "\n",
    "    # Flip coordinates for correct orientation\n",
    "    points = points.copy()\n",
    "    points[:, 1] = -points[:, 1]  # Flip Y coordinate\n",
    "    points[:, 2] = -points[:, 2]  # Flip Z coordinate\n",
    "\n",
    "    # Ensure points and colors are float32 / uint8\n",
    "    points = points.astype(np.float32)\n",
    "    colors = (colors * 255).astype(np.uint8)  # If colors were normalized [0,1]\n",
    "\n",
    "    # Create a structured array\n",
    "    vertex_data = np.array(\n",
    "        [tuple(p) + tuple(c) for p, c in zip(points, colors)],\n",
    "        dtype=[\n",
    "            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "            ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    el = plyfile.PlyElement.describe(vertex_data, 'vertex')\n",
    "    plyfile.PlyData([el]).write(filepath)\n",
    "    print(f\"Saved point cloud to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_scene(pose_estimations, ransac_results, Frames, detector_name, use_bundle_adjustment=True, visualize=True):\n",
    "    # Initialize first camera pose\n",
    "    R_global = np.eye(3)\n",
    "    t_global = np.zeros((3, 1))\n",
    "    camera_poses = [(R_global, t_global)] # Store camera poses as list of (R,t) tuples, starting with initial pose\n",
    "\n",
    "    # Lists to store 3D points and their RGB colors\n",
    "    all_points_3d = []\n",
    "    all_colors = []\n",
    "\n",
    "    # List to store reconstruction quality metrics\n",
    "    all_quality_metrics = []\n",
    "\n",
    "    # Get camera intrinsic matrix K from first pose estimation result\n",
    "    if not pose_estimations:\n",
    "        print(\"No pose estimations available to proceed with triangulation.\")\n",
    "        exit()\n",
    "        \n",
    "    K = pose_estimations[0]['K']\n",
    "\n",
    "    # Process each pose estimation sequentially to build up 3D reconstruction\n",
    "    for i, pose_result in enumerate(pose_estimations):\n",
    "        # Extract relative pose (R,t) between current frame pair\n",
    "        R_rel = pose_result.get(\"R\")\n",
    "        t_rel = pose_result.get(\"t\")\n",
    "\n",
    "        # Skip if invalid pose\n",
    "        if R_rel is None or t_rel is None:\n",
    "            print(f\"Skipping pair {ransac_results[i].frame_pair} due to invalid pose.\")\n",
    "            continue\n",
    "\n",
    "        # Get current RANSAC result\n",
    "        ransac_res = ransac_results[i]\n",
    "        \n",
    "        # Calculate global pose\n",
    "        R_new_global = R_global @ R_rel\n",
    "        t_new_global = t_global + R_global @ t_rel\n",
    "        \n",
    "        # Initial 3D point triangulation\n",
    "        points_3d = triangulate_points(\n",
    "            K,\n",
    "            (R_global, t_global),\n",
    "            (R_new_global, t_new_global),\n",
    "            ransac_res.pts1,\n",
    "            ransac_res.pts2\n",
    "        )\n",
    "        \n",
    "        # Get RGB color values for each 3D point from first frame\n",
    "        frame_idx_1, _ = ransac_res.frame_pair\n",
    "        colors = get_pixel_colors(Frames[frame_idx_1], ransac_res.pts1)\n",
    "\n",
    "        # Store results for this frame pair\n",
    "        all_points_3d.append(points_3d)\n",
    "        all_colors.append(colors)\n",
    "        \n",
    "        print(f\"Triangulated {len(points_3d)} points.\")\n",
    "\n",
    "        # Update global pose for next iteration\n",
    "        R_global = R_new_global\n",
    "        t_global = t_new_global\n",
    "        camera_poses.append((R_global, t_global))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # Apply bundle adjustment\n",
    "    if use_bundle_adjustment and len(camera_poses) > 1 and all_points_3d:\n",
    "        print(\"\\nApplying bundle adjustment...\")\n",
    "        refined_poses, refined_points_3d = bundle_adjust_poses(\n",
    "            camera_poses, all_points_3d, ransac_results, K\n",
    "        )\n",
    "        \n",
    "        # Use refined results\n",
    "        camera_poses = refined_poses\n",
    "        all_points_3d = refined_points_3d\n",
    "        \n",
    "        print(\"Bundle adjustment completed.\")\n",
    "\n",
    "    # Calculate reconstruction quality metrics \n",
    "    for i in range(len(ransac_results)):\n",
    "        if i + 1 >= len(camera_poses):\n",
    "            continue\n",
    "            \n",
    "        ransac_res = ransac_results[i]\n",
    "        R1, t1 = camera_poses[i]\n",
    "        R2, t2 = camera_poses[i + 1]\n",
    "        \n",
    "        # Use existing points if available, otherwise triangulate\n",
    "        if i < len(all_points_3d):\n",
    "            points_3d = all_points_3d[i]\n",
    "        else:\n",
    "            points_3d = triangulate_points(\n",
    "                K, (R1, t1), (R2, t2),\n",
    "                ransac_res.pts1, ransac_res.pts2\n",
    "            )\n",
    "            all_points_3d.append(points_3d)\n",
    "        \n",
    "        # Get colors if not already stored\n",
    "        if i >= len(all_colors):\n",
    "            colors = get_pixel_colors(Frames[ransac_res.frame_pair[0]], ransac_res.pts1)\n",
    "            all_colors.append(colors)\n",
    "        \n",
    "        # Construct projection matrices for quality analysis\n",
    "        P1 = K @ np.hstack((R1, t1))\n",
    "        P2 = K @ np.hstack((R2, t2))\n",
    "        \n",
    "        # Analyze quality\n",
    "        quality = analyze_reconstruction_quality(\n",
    "            points_3d,\n",
    "            ransac_res.pts1,\n",
    "            ransac_res.pts2,\n",
    "            P1, P2,\n",
    "            np.linalg.norm(t2 - t1)\n",
    "        )\n",
    "        \n",
    "        all_quality_metrics.append(quality)\n",
    "        \n",
    "    # Compute overall reconstruction quality metrics across all frame pairs\n",
    "    if all_quality_metrics:\n",
    "        avg_error = np.mean([m['mean_reprojection_error'] for m in all_quality_metrics])\n",
    "        avg_scale = np.mean([m['scale_ratio'] for m in all_quality_metrics])\n",
    "        total_points = sum([m['num_points'] for m in all_quality_metrics])\n",
    "\n",
    "        print(f\"\\n{detector_name} Final Reconstruction Quality Summary:\")\n",
    "        print(f\"Average Reprojection Error: {avg_error:.2f} pixels\")\n",
    "        print(f\"Average Scale Ratio: {avg_scale:.1f}x\")\n",
    "        print(f\"Total Reconstructed Points: {total_points}\")\n",
    "\n",
    "    # Combine all frame pairs results into final point cloud\n",
    "    if all_points_3d:\n",
    "        final_points_3d = np.concatenate(all_points_3d, axis=0)\n",
    "        final_colors = np.concatenate(all_colors, axis=0)\n",
    "        \n",
    "        # Display final combined point cloud\n",
    "        print(f\"\\nTotal reconstructed points: {len(final_points_3d)}\")\n",
    "        \n",
    "        if visualize:\n",
    "            visualize_reconstruction(final_points_3d, final_colors, window_name=f'{detector_name} 3D Scene Reconstruction')\n",
    "        save_point_cloud_ply(f\"{detector_name}_reconstruction.ply\", final_points_3d, final_colors)\n",
    "\n",
    "    else:\n",
    "        print(\"Could not reconstruct any 3D points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310a735",
   "metadata": {},
   "source": [
    "# Main Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variables\n",
    "VIDEO_PATH = \"../videos/Video_Selection_CV.mp4\"\n",
    "NUMBER_OF_FRAME_PAIRS = 11\n",
    "\n",
    "# Output Toggles\n",
    "PLOT_DETECTOR_BENCHMARK_COMPARISONS = True\n",
    "PLOT_FEATURE_DETECTOR_PAIRS = True\n",
    "PLOT_RANSAC_FILTERING = True\n",
    "PLOT_FUNDAMENTAL_MATRIX_COMPUTATION = True\n",
    "PLOT_CAMERA_POSE_ESTIMATIONS = True\n",
    "VISUALIZE_3D_RECONTRUCTION = True # Toggle Open3D interactive window display (point clouds export is always enabled) \n",
    "\n",
    "USE_BUNDLE_ADJUSTMENT = True\n",
    "\n",
    "ALGORITHM_NAME_FUNC_MAP = {\"ORB\" : detect_orb,\n",
    "                            \"SIFT\": detect_sift,\n",
    "                            \"FAST\": detect_fast,\n",
    "                            \"SHI-TOMASI\": detect_shi_tomasi,\n",
    "                            \"HARRIS\": detect_harris}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame preparation\n",
    "indices = get_frame_indices(VIDEO_PATH, NUMBER_OF_FRAME_PAIRS)\n",
    "print(f\"Overlapping indices: {indices}\")\n",
    "Frames = load_specific_frames(VIDEO_PATH, selected_indices=indices, display_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_DETECTOR_BENCHMARK_COMPARISONS:\n",
    "    benchmark_results = benchmark_detectors_with_repeatability(Frames)\n",
    "    compare_detectors_per_frame(Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19988e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each detector type and function pair to process feature matching\n",
    "for detector_type, detector_func in ALGORITHM_NAME_FUNC_MAP.items():\n",
    "    print(f\"\\nRunning:{detector_type}, func:{detector_func.__name__}\")\n",
    "    match_results = match_features(Frames, detector_type, detector_func)\n",
    "\n",
    "    # Check if any valid matches were found\n",
    "    if not match_results:\n",
    "        raise RuntimeError(\"No valid matches found!\")\n",
    "    \n",
    "    if PLOT_FEATURE_DETECTOR_PAIRS:\n",
    "        display_feature_detector_pairs(Frames, match_results, detector_type)  \n",
    "\n",
    "    # Apply RANSAC filtering to each match result\n",
    "    ransac_results = [apply_ransac_filter(res, reproj_thresh=16) for res in match_results]\n",
    "\n",
    "    print(f\"Number of match results: {len(match_results)}\")\n",
    "\n",
    "    # Visualize matches before and after RANSAC filtering\n",
    "    for idx in range(len(ransac_results)):        \n",
    "        before = match_results[idx]\n",
    "        after = ransac_results[idx]\n",
    "        i, j = before.frame_pair\n",
    "        if PLOT_RANSAC_FILTERING:\n",
    "            print(f\"\\nVisualising Matches: Frame Pair ({i}, {j})\")\n",
    "            plot_before_after_ransac(Frames[i], Frames[j], before, after)\n",
    "  \n",
    "    # Fundamental Matrix Computation and Epipolar Geometry\n",
    "    print(\"\\nFundamental Matrix and Epipolar Geometry:\")\n",
    "    fundamental_results_list = []\n",
    "\n",
    "    # Process each RANSAC result to compute fundamental matrix\n",
    "    for idx in range(len(ransac_results)):\n",
    "        res = ransac_results[idx]\n",
    "        i, j = res.frame_pair\n",
    "        print(f\"\\nFundamental Matrix Computation for Frame Pair ({i}, {j})\")\n",
    "        imgs = [Frames[i], Frames[j]]\n",
    "        fundamental_result = process_fundamental_matrix(imgs, res.matches, res.kp1, res.kp2, visualise=PLOT_FUNDAMENTAL_MATRIX_COMPUTATION)\n",
    "        fundamental_results_list.append(fundamental_result)\n",
    "\n",
    "    # Camera Pose Estimation\n",
    "    print(\"\\nCamera Pose Estimation:\")\n",
    "    pose_estimations = []\n",
    "    frame_shape = (Frames[0].shape[0], Frames[0].shape[1])  # Get frame dimensions\n",
    "\n",
    "    # Process each fundamental matrix result and RANSAC result\n",
    "    for idx, (fundamental_data, res) in enumerate(zip(fundamental_results_list, ransac_results)):\n",
    "        print(f\"\\nPose Estimation for Frame Pair {res.frame_pair}\")\n",
    "        pose_result = run_pose_estimation_from_matches(\n",
    "            match_result = res, \n",
    "            frame_shape = frame_shape, \n",
    "            fundamental_matrix = fundamental_data['fundamental_matrix'],\n",
    "            visualise=PLOT_CAMERA_POSE_ESTIMATIONS\n",
    "        )\n",
    "        pose_estimations.append(pose_result)\n",
    "\n",
    "    # 3D Point Triangulation and Scene Visualisation\n",
    "    print(\"\\n3D Point Triangulation and Scene Visualisation:\")\n",
    "    reconstruct_scene(pose_estimations, ransac_results, Frames, detector_name=detector_type, use_bundle_adjustment=USE_BUNDLE_ADJUSTMENT, visualize=VISUALIZE_3D_RECONTRUCTION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
