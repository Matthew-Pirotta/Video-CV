{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d555ec12",
   "metadata": {},
   "source": [
    "# Principles of Computer Vision for AI\n",
    "\n",
    "**Group Project**\n",
    "\n",
    "**ARI 2129 - Principles of Computer Vision for AI**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb680e4d",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "Libraries for computer vision, 3D visualization, and point cloud processing are imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27bcbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # Image processing and computer vision\n",
    "import imageio.v3 as iio # Reading image data\n",
    "import matplotlib.pyplot as plt # Data visualization and plotting\n",
    "import numpy as np # Numerical computations and array operations\n",
    "import time # Performance measurement\n",
    "import open3d as o3d # 3D scene visualization\n",
    "import plyfile # Point cloud exportations\n",
    "import os # File operations \n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61a469",
   "metadata": {},
   "source": [
    "## Frame Extraction\n",
    "\n",
    "This section introduces functions for frame extraction, index management, and color space conversion to prepare our video frames for feature detection and matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "850110fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatchResult = namedtuple(\"MatchResult\", [\"frame_pair\", \"kp1\", \"kp2\", \"pts1\", \"pts2\", \"matches\"])\n",
    "DetectorResults = dict[int, tuple[list, np.ndarray]]\n",
    "Frame = np.ndarray\n",
    "\n",
    "def get_next_frame_index(frames, current_frame_index):\n",
    "    next_frame_index = current_frame_index + 1\n",
    "\n",
    "    if next_frame_index >= len(frames):\n",
    "        raise ValueError(\"Next frame index out of bounds\")\n",
    "    \n",
    "    return next_frame_index\n",
    "\n",
    "def convert_frames_to_grayscale(*frames):\n",
    "    gray_frames = []\n",
    "    for frame in frames:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frames.append(gray)\n",
    "    \n",
    "    return tuple(gray_frames)\n",
    "\n",
    "def get_frame_indices(video_path:str, num_frames:int) -> list:\n",
    "    # Count total frames by iterating once\n",
    "    total_frames = sum(1 for _ in iio.imiter(video_path))\n",
    "\n",
    "    if num_frames > total_frames:\n",
    "        raise ValueError(f\"Requested {num_frames} frames, but video has only {total_frames} frames.\")\n",
    "\n",
    "    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "    return indices.tolist()\n",
    "\n",
    "def load_specific_frames(video_path: str, selected_indices: list, display_frames=True) -> list:\n",
    "    frames = []\n",
    "    selected_set = set(selected_indices)\n",
    "    collected = 0\n",
    "\n",
    "    for frame_count, frame in enumerate(iio.imiter(video_path)):\n",
    "        if frame_count in selected_set:\n",
    "            frames.append(frame)\n",
    "            collected += 1\n",
    "\n",
    "            if display_frames:\n",
    "                plt.imshow(frame)\n",
    "                plt.title(f'Frame {frame_count}')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "    print(f\"\\nFinished loading selected frames.\")\n",
    "    print(f\"Total frames processed: {frame_count + 1}\")\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d647f",
   "metadata": {},
   "source": [
    "## Feature Detection and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955719a",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "This section introduces various feature detection algorithms for identifying distinctive points in images.\n",
    "\n",
    "- ORB: Oriented FAST and Rotated BRIEF\n",
    "- SIFT: Scale-Invariant Feature Transform \n",
    "- FAST with SIFT descriptors\n",
    "- Shi-Tomasi corners with ORB descriptors\n",
    "- Harris corners with ORB descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd36032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_orb(gray):\n",
    "    orb = cv2.ORB_create(nfeatures=3000) \n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_sift(gray):\n",
    "    sift = cv2.SIFT_create(nfeatures=1500, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6)\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_fast(gray):\n",
    "    fast = cv2.FastFeatureDetector_create(threshold=37, nonmaxSuppression=True)\n",
    "    keypoints = fast.detect(gray, None)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.compute(gray, keypoints)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_shi_tomasi(gray):\n",
    "    corners = cv2.goodFeaturesToTrack(gray, maxCorners=3000, qualityLevel=0.01, minDistance=10, blockSize=3)\n",
    "    keypoints = []\n",
    "    if corners is not None:\n",
    "        keypoints = [cv2.KeyPoint(float(x), float(y), 1) for [[x, y]] in corners]\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_harris(gray, block_size=2, ksize=3, k=0.04, threshold_ratio=0.07): \n",
    "    gray_f32 = np.float32(gray)\n",
    "    dst = cv2.cornerHarris(gray_f32, block_size, ksize, k)\n",
    "    dst = cv2.dilate(dst, None)\n",
    "    threshold = threshold_ratio * dst.max()\n",
    "    corners = np.argwhere(dst > threshold)\n",
    "    keypoints = [cv2.KeyPoint(float(pt[1]), float(pt[0]), 1) for pt in corners]\n",
    "\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b71742",
   "metadata": {},
   "source": [
    "This method processes video frames through a selected feature detector, tracking the number of keypoints found and computational performance. Each frame is first converted to grayscale before detection and result collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c77e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_features(frames, detector_func):\n",
    "    detector_results = {}\n",
    "    keypoint_counts = []\n",
    "    computation_times = []\n",
    "\n",
    "    for idx, frame in enumerate(frames):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        start = time.time()\n",
    "        kp, des = detector_func(gray)\n",
    "        end = time.time()\n",
    "\n",
    "        detector_results[idx] = (kp, des)\n",
    "        keypoint_counts.append(len(kp))\n",
    "        computation_times.append(end - start)\n",
    "\n",
    "    return detector_results, keypoint_counts, computation_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70779144",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "`display_feature_detector_pairs()` helps analyze feature detection results by assigning a distinct color to each detector for consistent visual representation across different plots and frame comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd7155a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations \n",
    "# Colors for matplotlib plots \n",
    "DETECTOR_COLORS_MPL = {\n",
    "    \"ORB\": 'green',\n",
    "    \"SIFT\": 'blue',\n",
    "    \"FAST\": 'red',\n",
    "    \"SHI-TOMASI\": 'cyan',\n",
    "    \"Harris\": 'magenta'\n",
    "}\n",
    "\n",
    "DETECTOR_COLORS = {\n",
    "    \"ORB\": (0, 255, 0),          \n",
    "    \"SIFT\": (255, 0, 0),         \n",
    "    \"FAST\": (0, 0, 255),         \n",
    "    \"SHI-TOMASI\": (255, 255, 0), \n",
    "    \"Harris\": (255, 0, 255)      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b24720f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_detector_pairs(frames, matches: list[MatchResult], detector_name: str):\n",
    "    color = DETECTOR_COLORS.get(detector_name, (0, 255, 0))\n",
    "\n",
    "    for match_result in matches:\n",
    "        i, j = match_result.frame_pair\n",
    "\n",
    "        img1 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "        img2 = cv2.cvtColor(frames[j], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        img_kp1 = cv2.drawKeypoints(img1, match_result.kp1, None, color=color)\n",
    "        img_kp2 = cv2.drawKeypoints(img2, match_result.kp2, None, color=color)\n",
    "\n",
    "        # Show keypoints on individual images\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        axes[0].imshow(img_kp1, cmap='gray')\n",
    "        axes[0].set_title(f\"{detector_name} - Frame {i}, Keypoints: {len(match_result.kp1)}\")\n",
    "\n",
    "        axes[1].imshow(img_kp2, cmap='gray')\n",
    "        axes[1].set_title(f\"{detector_name} - Frame {j}, Keypoints: {len(match_result.kp2)}\")\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        plt.suptitle(f\"{detector_name} Keypoints: Frame {i} â†” Frame {j}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18681a9",
   "metadata": {},
   "source": [
    "Functions to compare feature detectors by displaying detected keypoints side-by-side and plotting their performance metrics - including keypoint counts and computation times across video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4973180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_detectors_per_frame_cached(frames, detector_results_dict):\n",
    "    for idx, frame in enumerate(frames):\n",
    "        frame_results = {}\n",
    "        for name, frame_data in detector_results_dict.items():            \n",
    "            kp = frame_data[idx][0]  # (kp, des)\n",
    "            color = DETECTOR_COLORS.get(name, (0, 255, 0))\n",
    "            img_with_kp = cv2.drawKeypoints(frame, kp, None, color)\n",
    "            frame_results[name] = (img_with_kp, len(kp))\n",
    "\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "        axes = axes.flatten()\n",
    "        for ax, (name, (img, count)) in zip(axes, frame_results.items()):\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"{name} ({count})\")\n",
    "            ax.axis('off')\n",
    "        for ax in axes[len(frame_results):]:\n",
    "            ax.axis('off')\n",
    "        plt.suptitle(f\"Detector Comparison - Frame {idx}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_keypoint_counts_and_comp_time(results):\n",
    "    # Plots keypoint counts, computation times.\n",
    "    detector_names = list(results.keys())\n",
    "    detector_names.remove('frame_indices')\n",
    "    frame_indices = results['frame_indices']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 18), sharex=True)\n",
    "    \n",
    "    # Plot Keypoint Counts\n",
    "    for name in detector_names:\n",
    "        ax1.plot(frame_indices, results[name]['keypoint_counts'], linestyle='-', label=name)\n",
    "    ax1.set_ylabel('Number of Keypoints')\n",
    "    ax1.set_title('Keypoint Detection Count per Frame')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot Computation Times\n",
    "    for name in detector_names:\n",
    "        ax2.plot(frame_indices, results[name]['computation_times'], linestyle='-', label=name)\n",
    "    ax2.set_ylabel('Computation Time (seconds)')\n",
    "    ax2.set_title('Computation Time per Frame')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print averages\n",
    "    for name, data in results.items():\n",
    "        if name != 'frame_indices':\n",
    "            print(f\"Average {name.upper()}: {np.mean(data['keypoint_counts']):.1f} keypoints, {np.mean(data['computation_times']):.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e8daf",
   "metadata": {},
   "source": [
    "Function to create video visualizations of feature detection results. Creates labeled video visualizations showing detected features for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d1da512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_detection_montage(frames, detected_features_all, algorithm_name_func_map, output_dir='feature_detection_montage_videos', fps=10):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    frame_height, frame_width = frames[0].shape[:2]\n",
    "    video_writers = {}\n",
    "\n",
    "    # Initialise a video writer for each algorithm\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    for name in algorithm_name_func_map:\n",
    "        output_path = os.path.join(output_dir, f\"{name}.mp4\")\n",
    "        video_writers[name] = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Set how many times to repeat each frame\n",
    "    repeat_count = 5  # Increase this to hold the frame longer (e.g., 5 = 0.5 seconds at 10 fps)\n",
    "\n",
    "    for idx, frame in enumerate(frames):\n",
    "        for name in algorithm_name_func_map:\n",
    "            if name not in detected_features_all:\n",
    "                continue\n",
    "\n",
    "            keypoints, _ = detected_features_all[name][idx]\n",
    "            color = DETECTOR_COLORS.get(name, (0, 255, 0))\n",
    "            img_with_kp = cv2.drawKeypoints(frame, keypoints, None, color)\n",
    "\n",
    "            # Add label to the frame\n",
    "            cv2.putText(img_with_kp, f\"{name} - Frame {idx + 1}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            # Repeat writing this frame\n",
    "            for _ in range(repeat_count):\n",
    "                video_writers[name].write(img_with_kp)\n",
    "\n",
    "        print(f\"Processed frame {idx + 1}/{len(frames)}\")\n",
    "\n",
    "    # Release writers\n",
    "    for writer in video_writers.values():\n",
    "        writer.release()\n",
    "\n",
    "    print(f\"Videos saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63df05",
   "metadata": {},
   "source": [
    "## Feature Matching and Outlier Rejection\n",
    "\n",
    "Implementation of feature matching strategies based on descriptor types. FLANN-based matching for float descriptors (SIFT, SURF) and Brute Force matching for binary descriptors (ORB), with Lowe's ratio test to filter unreliable matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "460af133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matcher(detector_type: str):\n",
    "    detector_type = detector_type.lower()\n",
    "    if detector_type in ['sift', 'surf', 'fast']:\n",
    "        # FLANN for float descriptors\n",
    "        index_params = dict(algorithm=1, trees=5)  # KDTree\n",
    "        search_params = dict(checks=50)\n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        descriptor_type = 'float'\n",
    "    elif detector_type in ['orb', 'brief', 'shi-tomasi', 'harris']:\n",
    "        # Brute-force for binary descriptors\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "        descriptor_type = 'binary'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported detector type: {detector_type}\")\n",
    "    \n",
    "    return matcher, descriptor_type\n",
    "\n",
    "def match_features(frames, detector_type:str, detector_results:DetectorResults, ratio_thresh=0.75) -> list[MatchResult]:\n",
    "    matcher, descriptor_type = get_matcher(detector_type)\n",
    "    match_results = []\n",
    "\n",
    "    for start_idx in range(len(frames)-1):\n",
    "        end_idx = start_idx + 1        \n",
    "\n",
    "        kp1, des1 = detector_results.get(start_idx, (None, None))\n",
    "        kp2, des2 = detector_results.get(end_idx, (None, None))\n",
    "\n",
    "\n",
    "        if des1 is None or des2 is None or len(kp1) == 0 or len(kp2) == 0:\n",
    "            continue\n",
    "        \n",
    "        if descriptor_type == 'float':\n",
    "            des1 = des1.astype(np.float32)\n",
    "            des2 = des2.astype(np.float32)\n",
    "\n",
    "        knn_matches = matcher.knnMatch(des1, des2, k=2)\n",
    "        \n",
    "        good_matches = []\n",
    "        for match_pair in knn_matches:\n",
    "            if len(match_pair) == 2:\n",
    "                m, n = match_pair\n",
    "                if m.distance < ratio_thresh * n.distance:\n",
    "                    good_matches.append(m)\n",
    "\n",
    "        if len(good_matches) < 10:\n",
    "            continue\n",
    "\n",
    "        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
    "\n",
    "        result = MatchResult(\n",
    "            frame_pair=(start_idx, end_idx),\n",
    "            kp1=kp1,\n",
    "            kp2=kp2,\n",
    "            pts1=pts1,\n",
    "            pts2=pts2,\n",
    "            matches=good_matches\n",
    "        )\n",
    "\n",
    "        match_results.append(result)\n",
    "        print(f\"[{detector_type.upper()}] Matched {start_idx} â†’ {end_idx}: Before Lowe:{len(knn_matches)} matches, After Lowe {len(good_matches)} matches\")\n",
    "\n",
    "    return match_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "860deb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANSAC removes outlier matches to improve geometric consistency\n",
    "# Higher threshold = more lenient (accepts points further from predicted position)\n",
    "def apply_ransac_filter(match_result:MatchResult, reproj_thresh=4.0)-> MatchResult:\n",
    "    H, mask = cv2.findHomography(match_result.pts1, match_result.pts2, cv2.RANSAC, reproj_thresh)\n",
    "    mask = mask.ravel()\n",
    "\n",
    "    inlier_matches = [m for m, keep in zip(match_result.matches, mask) if keep]\n",
    "    pts1_inliers = match_result.pts1[mask == 1]\n",
    "    pts2_inliers = match_result.pts2[mask == 1]\n",
    "\n",
    "    return MatchResult(\n",
    "        frame_pair=match_result.frame_pair,\n",
    "        kp1=match_result.kp1,\n",
    "        kp2=match_result.kp2,\n",
    "        pts1=pts1_inliers,\n",
    "        pts2=pts2_inliers,\n",
    "        matches=inlier_matches\n",
    "    )\n",
    "\n",
    "def plot_visual_image_before_after_ransac(img1:Frame, img2:Frame, match_before: MatchResult, match_after: MatchResult, detector_type:str) -> None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "    # Before RANSAC\n",
    "    img_before = cv2.drawMatches(\n",
    "        img1, match_before.kp1,\n",
    "        img2, match_before.kp2,\n",
    "        match_before.matches,\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_DEFAULT\n",
    "    )\n",
    "    axes[0].imshow(img_before)\n",
    "    axes[0].set_title(f\"Lowe's Ratio Before RANSAC - Matches: {len(match_before.matches)}\", fontsize=20)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # After RANSAC\n",
    "    img_after = cv2.drawMatches(\n",
    "        img1, match_after.kp1,\n",
    "        img2, match_after.kp2,\n",
    "        match_after.matches,\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_DEFAULT\n",
    "    )\n",
    "    axes[1].imshow(img_after)\n",
    "    axes[1].set_title(f\"After RANSAC - Inliers: {len(match_after.matches)}\", fontsize=20)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    fig.suptitle(f\"{detector_type} Frames:{match_before.frame_pair}\", fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_before_after_ransac_detected_features_all_algos(feature_matching_results_all_algos):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for detector_type, result_dict in feature_matching_results_all_algos.items():\n",
    "        matches_before = result_dict['matches']\n",
    "        matches_after = result_dict['ransac']\n",
    "\n",
    "        total_inliers = sum(len(result_after.matches) for result_after in matches_after)\n",
    "        total_matches = sum(len(result_before.matches) for result_before in matches_before)\n",
    "        if total_matches == 0:\n",
    "            print(f\"Skipping {detector_type}: no matches\")\n",
    "            continue\n",
    "\n",
    "        inlier_ratio_overall = total_inliers / total_matches\n",
    "\n",
    "        # Compute per-frame-pair inlier ratios\n",
    "        ratios = [\n",
    "            len(after.matches) / len(before.matches)\n",
    "            if len(before.matches) > 0 else 0\n",
    "            for before, after in zip(matches_before, matches_after)\n",
    "        ]\n",
    "\n",
    "        color = DETECTOR_COLORS_MPL.get(detector_type, 'green')  # Fallback to black if undefined\n",
    "        label = f\"{detector_type} (overall: {inlier_ratio_overall:.2f})\"\n",
    "        plt.plot(ratios, label=label, color=color)\n",
    "\n",
    "    plt.title(\"Inlier Ratio per Frame Pair (RANSAC Filtered)\")\n",
    "    plt.xlabel(\"Frame Pair Index\")\n",
    "    plt.ylabel(\"Inlier Ratio (After / Before RANSAC)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213ae0e",
   "metadata": {},
   "source": [
    "## Essential/Fundamental Matrix Computation\n",
    "\n",
    "Methods for computing and validating the geometric relationship between image pairs. The fundamental matrix captures this relationship, while epipolar line visualization and error metrics help verify the accuracy of the computed geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d853e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Compute the fundamental matrix from matched keypoints\n",
    "def compute_fundamental_matrix(matches, keypoints1, keypoints2, method = cv2.FM_RANSAC, ransac_threshold = 3.0, confidence = 0.99):\n",
    "  # Extract coordinates of matched keypoints\n",
    "  points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])\n",
    "  points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])\n",
    "  \n",
    "  # Compute the fundamental matrix\n",
    "  fundamental_matrix, inlier_mask = cv2.findFundamentalMat(points1, points2, method = method, ransacReprojThreshold = ransac_threshold, confidence = confidence)\n",
    "  \n",
    "  #Â Convert mask to binary array for easier filtering\n",
    "  if fundamental_matrix is None or fundamental_matrix.shape != (3,3):\n",
    "    raise ValueError(\"Failed to compute a valid fundamental matrix\")\n",
    "  \n",
    "  return fundamental_matrix, inlier_mask\n",
    "\n",
    "#Â Clip epipolar lines to image boundaries\n",
    "def draw_epipolar_line_clipped(img_shape, line):\n",
    "  h, w = img_shape[:2]\n",
    "    \n",
    "  # Find intersections with image borders\n",
    "  points = []\n",
    "    \n",
    "  # Left border (x = 0)\n",
    "  if abs(line[1]) > 1e-6:\n",
    "    y = -line[2] / line[1]\n",
    "    if 0 <= y <= h:\n",
    "      points.append((0, int(y)))\n",
    "    \n",
    "  # Right border (x = w-1)\n",
    "  if abs(line[1]) > 1e-6:\n",
    "    y = -(line[2] + line[0] * (w-1)) / line[1]\n",
    "    if 0 <= y <= h:\n",
    "      points.append((w-1, int(y)))\n",
    "    \n",
    "  # Top border (y = 0)\n",
    "  if abs(line[0]) > 1e-6:\n",
    "    x = -line[2] / line[0]\n",
    "    if 0 <= x <= w:\n",
    "      points.append((int(x), 0))\n",
    "    \n",
    "  # Bottom border (y = h-1)\n",
    "  if abs(line[0]) > 1e-6:\n",
    "    x = -(line[2] + line[1] * (h-1)) / line[0]\n",
    "    if 0 <= x <= w:\n",
    "      points.append((int(x), h-1))\n",
    "    \n",
    "  # Remove duplicate points and return first two\n",
    "  unique_points = []\n",
    "  for p in points:\n",
    "    if not any(abs(p[0]-up[0]) < 2 and abs(p[1]-up[1]) < 2 for up in unique_points):\n",
    "      unique_points.append(p)\n",
    "    \n",
    "  return unique_points[:2] if len(unique_points) >= 2 else []\n",
    "\n",
    "# Visualise epipolar lines to verify the fundamental matrix\n",
    "def visualise_epipolar_lines(img1, img2, points1, points2, fundamental_matrix, sample_size = 20):\n",
    "  # Sample points if there are too many\n",
    "  if len(points1) > sample_size:\n",
    "    indices = np.random.choice(len(points1), sample_size, replace = False)\n",
    "    pts1 = points1[indices]\n",
    "    pts2 = points2[indices]\n",
    "  else:\n",
    "    pts1 = points1\n",
    "    pts2 = points2\n",
    "\n",
    "  # Create a new figure for each call\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 8))\n",
    "\n",
    "  # Convert images to RGB for matplotlib\n",
    "  img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "  img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  # Display the first image\n",
    "  ax1.imshow(img1_rgb) # Use RGB image\n",
    "  ax1.set_title('Epipolar Lines on Image 1')\n",
    "  ax1.axis('off')\n",
    "\n",
    "  # Display the second image\n",
    "  ax2.imshow(img2_rgb) # Use RGB image\n",
    "  ax2.set_title('Epipolar Lines on Image 2')\n",
    "  ax2.axis('off')\n",
    "\n",
    "  # Draw epipolar lines on both images\n",
    "  for i in range(len(pts1)):\n",
    "    # Draw points\n",
    "    ax1.plot(pts1[i, 0], pts1[i, 1], 'ro', markersize = 6)\n",
    "    ax2.plot(pts2[i, 0], pts2[i, 1], 'ro', markersize = 6)\n",
    "\n",
    "    # Compute epipolar line in second image for point in first image\n",
    "    line2 = cv2.computeCorrespondEpilines(pts1[i].reshape(-1, 1, 2), 1, fundamental_matrix)\n",
    "    line2 = line2.reshape(-1)\n",
    "\n",
    "    # Get clipped line endpoints for image 2\n",
    "    endpoints2 = draw_epipolar_line_clipped(img2.shape, line2)\n",
    "    if len(endpoints2) == 2:\n",
    "        ax2.plot([endpoints2[0][0], endpoints2[1][0]], [endpoints2[0][1], endpoints2[1][1]], 'g-', linewidth=1)\n",
    "\n",
    "    # Compute epipolar line in first image for point in second image\n",
    "    line1 = cv2.computeCorrespondEpilines(pts2[i].reshape(-1, 1, 2), 2, fundamental_matrix)\n",
    "    line1 = line1.reshape(-1)\n",
    "\n",
    "    # Get clipped line endpoints for image 1\n",
    "    endpoints1 = draw_epipolar_line_clipped(img1.shape, line1)\n",
    "    if len(endpoints1) == 2:\n",
    "        ax1.plot([endpoints1[0][0], endpoints1[1][0]], [endpoints1[0][1], endpoints1[1][1]], 'g-', linewidth=1)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  plt.close(fig)\n",
    "\n",
    "# Calculate the epipolar geometry error to evaluate the quality of the fundamental matrix\n",
    "def epipolar_error(points1, points2, fundamental_matrix):\n",
    "  # Convert each point to homogeneous coordinates\n",
    "  homogeneous_points1 = np.hstack((points1, np.ones((points1.shape[0],1))))\n",
    "  homogeneous_points2 = np.hstack((points2, np.ones((points2.shape[0],1))))\n",
    "  \n",
    "  # Calculate epipolar lines for points in image 1\n",
    "  lines2 = np.dot(homogeneous_points1, fundamental_matrix.T)\n",
    "  # Normalise lines\n",
    "  norms2 = np.sqrt(lines2[:, 0]**2 + lines2[:, 1]**2)\n",
    "  lines2 = lines2 / norms2.reshape(-1,1)\n",
    "  # Calculate the distance from points in image 2 to their corresponding epipolar lines\n",
    "  dist2 = np.abs(np.sum(lines2 * homogeneous_points2, axis = 1))\n",
    "  \n",
    "  # Calculate epipolar lines for points in image 2\n",
    "  lines1 = np.dot(homogeneous_points2, fundamental_matrix)\n",
    "  # Normalise lines\n",
    "  norms1 = np.sqrt(lines1[:, 0]**2 + lines1[:, 1]**2)\n",
    "  lines1 = lines1 / norms1.reshape(-1,1)\n",
    "  # Calculate the distance from points in image 1 to their corresponding epipolar lines\n",
    "  dist1 = np.abs(np.sum(lines1 * homogeneous_points1, axis = 1))\n",
    "  \n",
    "  metrics = {\n",
    "    \"mean_error\": (np.mean(dist1) + np.mean(dist2)) / 2,\n",
    "    \"max_error\": max(np.max(dist1), np.max(dist2)),\n",
    "    \"std_error\": (np.std(dist1) + np.std(dist2)) / 2\n",
    "  }\n",
    "  \n",
    "  return metrics\n",
    "\n",
    "# Main function to compute the fundamental matrix\n",
    "def process_fundamental_matrix(imgs, matches, keypoints1, keypoints2, visualise = True):\n",
    "  # Compute fundamental matrix\n",
    "  print(\"Computing fundamental matrix...\")\n",
    "  F, inlier_mask = compute_fundamental_matrix(matches, keypoints1, keypoints2)\n",
    "    \n",
    "  # Filter matches based on inlier mask\n",
    "  inlier_matches = [m for i, m in enumerate(matches) if inlier_mask[i]]\n",
    "  print(f\"Inlier matches: {len(inlier_matches)} out of {len(matches)} ({len(inlier_matches) / len(matches) * 100:.2f}%)\")\n",
    "    \n",
    "  # Extract coordinates of inlier keypoints\n",
    "  inlier_points1 = np.float32([keypoints1[m.queryIdx].pt for m in inlier_matches])\n",
    "  inlier_points2 = np.float32([keypoints2[m.trainIdx].pt for m in inlier_matches])\n",
    "    \n",
    "  # Calculate error metrics\n",
    "  error_metrics = epipolar_error(inlier_points1, inlier_points2, F)\n",
    "  print(f\"Mean epipolar error: {error_metrics['mean_error']:.4f} pixels\")\n",
    "  print(f\"Max epipolar error: {error_metrics['max_error']:.4f} pixels\")\n",
    "  print(f\"Std. epipolar error: {error_metrics['std_error']:.4f} pixels\")\n",
    "        \n",
    "  # Visualise epipolar lines if requested\n",
    "  if visualise and len(imgs) >= 2:\n",
    "      visualise_epipolar_lines(imgs[0], imgs[1], inlier_points1, inlier_points2, F)\n",
    "    \n",
    "  # Prepare results\n",
    "  results = {\n",
    "      \"fundamental_matrix\": F,\n",
    "      \"inlier_mask\": inlier_mask,\n",
    "      \"inlier_matches\": inlier_matches,\n",
    "      \"inlier_points1\": inlier_points1,\n",
    "      \"inlier_points2\": inlier_points2,\n",
    "      \"error_metrics\": error_metrics,\n",
    "  }\n",
    "    \n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1123511",
   "metadata": {},
   "source": [
    "## Camera Pose Estimation\n",
    "\n",
    "Implementation of camera pose recovery from matched features. Starting with approximate camera intrinsics, the pipeline converts fundamental to essential matrix, recovers relative camera positions, and validates the estimated poses through geometric constraints and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0843083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CameraIntrinsics:\n",
    "  fx: float\n",
    "  fy: float\n",
    "  cx: float\n",
    "  cy: float\n",
    "  \n",
    "  def matrix(self):\n",
    "    return np.array([\n",
    "      [self.fx, 0, self.cx],\n",
    "      [0, self.fy, self.cy],\n",
    "      [0, 0, 1]\n",
    "    ],dtype = np.float64)\n",
    "    \n",
    "# Estimate approximate intrinsics for a phone camera\n",
    "def estimate_camera_intrinsics(image_shape):\n",
    "  height, width = image_shape\n",
    "  \n",
    "  fov_degrees = 75\n",
    "  fov_radians = np.radians(fov_degrees)\n",
    "  f = (width / 2.0) / np.tan(fov_radians / 2.0)\n",
    "  \n",
    "  # Principal point at image center\n",
    "  cx = width / 2\n",
    "  cy = height / 2\n",
    "  \n",
    "  print(f\"Intrinsics fx = fy = {f:.1f}, cx = {cx:.1f}, cy = {cy:.1f}\")\n",
    "  return CameraIntrinsics(f, f, cx, cy)\n",
    "\n",
    "# Compute the essential matrix from the fundamental matrix using the formula:\n",
    "# E = K^T * F * K\n",
    "def convert_fundamental_to_essential_matrix(F, K):\n",
    "  E = K.T @ F @ K\n",
    "  return E\n",
    "\n",
    "# Recover relative camera pose (R, t) from the essential matrix\n",
    "def recover_camera_pose(E, pts1, pts2, K):\n",
    "  pts1 = np.asarray(pts1, dtype = np.float32)\n",
    "  pts2 = np.asarray(pts2, dtype = np.float32)\n",
    "  \n",
    "  # Make sure points have correct shape\n",
    "  if pts1.ndim == 1:\n",
    "    pts1 = pts1.reshape(-1,2)\n",
    "  if pts2.ndim == 1:\n",
    "    pts2 = pts2.reshape(-1,2)\n",
    "  \n",
    "  try:\n",
    "    # Chirality problem - Funtion automatically tests all 4 possible combinations of R and t\n",
    "    _, R, t, mask = cv2.recoverPose(E, pts1, pts2, K)\n",
    "    \n",
    "    if mask is not None:\n",
    "      actual_inliers = np.sum(mask > 0)\n",
    "    else:\n",
    "      actual_inliers = 0\n",
    "      \n",
    "    print(f\"Pose Inliers: {actual_inliers}/{len(pts1)}\")\n",
    "    \n",
    "    if actual_inliers < 5:\n",
    "      print(\"Very few inliers for pose recovery\")\n",
    "      \n",
    "    return R, t, mask, actual_inliers\n",
    "  except cv2.error as e:\n",
    "    print(f\"OpenCV error in pose recovery: {e}\")\n",
    "     # Return identity transformation as fallback\n",
    "    return np.eye(3), np.zeros((3, 1)), np.zeros(len(pts1), dtype = np.uint8), 0\n",
    "\n",
    "# Validity checks on the recovered pose\n",
    "def validate_pose(R, t):\n",
    "  metrics = {}\n",
    "  det_R = np.linalg.det(R)\n",
    "  metrics['det_R'] = det_R\n",
    "  metrics['is_rotation_valid'] = np.abs(det_R - 1.0) < 1e-6\n",
    "\n",
    "  orthogonality_error = np.linalg.norm(R @ R.T - np.eye(3))\n",
    "  metrics['orthogonality_error'] = orthogonality_error\n",
    "  metrics['is_orthogonal'] = orthogonality_error < 1e-6\n",
    "\n",
    "  t_mag = np.linalg.norm(t)\n",
    "  metrics['translation_magnitude'] = t_mag\n",
    "\n",
    "  trace = np.trace(R)\n",
    "  trace = np.clip(trace, -1.0, 3.0)\n",
    "  angle_rad = np.arccos((trace - 1) / 2)\n",
    "  metrics['rotation_angle_deg'] = np.degrees(angle_rad)\n",
    "\n",
    "  return metrics\n",
    "\n",
    "# Simple 3D plot of the initial and recovered camera pose\n",
    "def visualise_camera_poses(R, t):\n",
    "  try:\n",
    "    fig = plt.figure(figsize = (10,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "    cam1 = np.array([0, 0, 0])\n",
    "    cam2 = -R.T @ t.flatten()  # Camera center in world coordinates\n",
    "    \n",
    "    if cam2.ndim > 1:\n",
    "      cam2 = cam2.flatten()\n",
    "    \n",
    "    ax.scatter(cam1[0], cam1[1], cam1[2], c = 'blue', s = 50, label = 'Camera 1 (origin)')\n",
    "    ax.scatter(cam2[0], cam2[1], cam2[2], c = 'red', s = 50, label = 'Camera 2')\n",
    "    ax.plot([cam1[0], cam2[0]], [cam1[1], cam2[1]], [cam1[2], cam2[2]], 'k--', linewidth = 2, label = 'Baseline')\n",
    "\n",
    "    # Add coordinate axes for camera orientations\n",
    "    axes_length = 0.3\n",
    "        \n",
    "    # Camera 1 axes (identity rotation)\n",
    "    ax.plot([0, axes_length], [0, 0], [0, 0], 'r-', alpha = 0.7, linewidth = 2)  # X-axis\n",
    "    ax.plot([0, 0], [0, axes_length], [0, 0], 'g-', alpha = 0.7, linewidth = 2)  # Y-axis\n",
    "    ax.plot([0, 0], [0, 0], [0, axes_length], 'b-', alpha = 0.7, linewidth = 2)  # Z-axis\n",
    "        \n",
    "    # Camera 2 axes (rotated)\n",
    "    axes = axes_length * R.T  # Camera coordinate axes in world frame\n",
    "    origin = cam2\n",
    "        \n",
    "    x_end = origin + axes[:, 0]\n",
    "    y_end = origin + axes[:, 1] \n",
    "    z_end = origin + axes[:, 2]\n",
    "        \n",
    "    ax.plot([origin[0], x_end[0]], [origin[1], x_end[1]], [origin[2], x_end[2]], 'r-', alpha = 0.7, linewidth = 2)\n",
    "    ax.plot([origin[0], y_end[0]], [origin[1], y_end[1]], [origin[2], y_end[2]], 'g-', alpha = 0.7, linewidth = 2)\n",
    "    ax.plot([origin[0], z_end[0]], [origin[1], z_end[1]], [origin[2], z_end[2]], 'b-', alpha = 0.7, linewidth = 2)\n",
    "\n",
    "    ax.set_xlabel('X') # red line\n",
    "    ax.set_ylabel('Y') # green line\n",
    "    ax.set_zlabel('Z') # blue line\n",
    "    ax.set_title('Camera Poses')\n",
    "    ax.legend()\n",
    "        \n",
    "    # Set equal aspect ratio\n",
    "    max_range = max(np.abs(cam2).max(), 0.5)\n",
    "    ax.set_xlim([-max_range, max_range])\n",
    "    ax.set_ylim([-max_range, max_range])\n",
    "    ax.set_zlim([-max_range, max_range])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  except Exception as e:\n",
    "    print(f\"Visualisation error: {e}\")\n",
    "    print(\"Skipping visualisation process\")\n",
    "\n",
    "# Takes in fundamental matrix and inlier points computed from stage 5\n",
    "def run_pose_estimation(F, pts1, pts2, frame_shape, visualise = True):\n",
    "  print(\"Camera Pose Estimation: \")\n",
    "  \n",
    "  if F is None or F.shape != (3,3):\n",
    "    raise ValueError(\"Invalid fundamental matrix\")\n",
    "  \n",
    "  if len(pts1) < 5 or len(pts2) < 5:\n",
    "    raise ValueError(\"Needs at least 5 point correspondences\")\n",
    "  \n",
    "  try:\n",
    "    # Estimate the intrinsics\n",
    "    intrinsics = estimate_camera_intrinsics(frame_shape)\n",
    "    K = intrinsics.matrix()\n",
    "    \n",
    "    # Deduce the essential matrix\n",
    "    E = convert_fundamental_to_essential_matrix(F, K)\n",
    "    print(\"Essential Matrix\\n\", E)\n",
    "    \n",
    "    # Check if essential matrix is valid using SVD\n",
    "    U, S, Vt = np.linalg.svd(E)\n",
    "    print(f\"Essential matrix singular values: {S}\")\n",
    "    \n",
    "    # Recover pose\n",
    "    R, t, mask, actual_inliers = recover_camera_pose(E, pts1, pts2, K)\n",
    "    \n",
    "    # Validation step\n",
    "    pose_metrics = validate_pose(R, t)\n",
    "    print(\"\\nPose Validation: \")\n",
    "    for k, v in pose_metrics.items():\n",
    "      print(f\"{k}: {v}\")\n",
    "      \n",
    "    if not pose_metrics['is_rotation_valid']:\n",
    "      print(\"Invalid rotation matrix\")\n",
    "      \n",
    "    if pose_metrics['rotation_angle_deg'] > 90:\n",
    "      print(f\"Large rotation angle {pose_metrics['rotation_angle_deg']:.1f}\")\n",
    "    \n",
    "    # Check for degenerate \n",
    "    if pose_metrics['rotation_angle_deg'] < 0.1:\n",
    "      print(\"Very small rotation - possible degenerate case\")\n",
    "    \n",
    "    # Visualise the initial and recovered camera pose\n",
    "    if visualise:\n",
    "      visualise_camera_poses(R, t)\n",
    "      \n",
    "    return {\n",
    "      \"R\": R,\n",
    "      \"t\": t,\n",
    "      \"K\": K,\n",
    "      \"E\": E,\n",
    "      \"mask\": mask,\n",
    "      \"metrics\": pose_metrics,\n",
    "      \"actual_inliers\": actual_inliers\n",
    "    }\n",
    "  except Exception as e:\n",
    "    print(f\"Error in pose estimation: {e}\")\n",
    "    return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef9c9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to integrate stages 4-6 together\n",
    "def run_pose_estimation_from_matches(match_result, frame_shape, fundamental_matrix, visualise):\n",
    "  return run_pose_estimation(F = fundamental_matrix, pts1 = match_result.pts1, pts2 = match_result.pts2, frame_shape = frame_shape, visualise=visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a4d20",
   "metadata": {},
   "source": [
    "## 3D Point Triangulation and Scene Visualisation\n",
    "\n",
    "Methods for reconstructing 3D scenes from matched features and camera poses. The pipeline includes point triangulation, color mapping, bundle adjustment for pose refinement, quality analysis, and tools for visualizing and saving the reconstructed point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "954360ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_points(K, camera1_pose, camera2_pose, points1, points2):\n",
    "    # Triangulate points in 3D space from 2D correspondences and camera parameters\n",
    "\n",
    "    # Input validation\n",
    "    if points1.shape[0] != points2.shape[0]:\n",
    "        print(\"Error: Input point arrays must have the same number of points.\")\n",
    "        return np.array([])\n",
    "\n",
    "    # Deconstruct poses\n",
    "    R1, t1 = camera1_pose\n",
    "    R2, t2 = camera2_pose\n",
    "\n",
    "    # Compute projection matrices P = K[R|t]\n",
    "    P1 = K @ np.hstack((R1, t1))\n",
    "    P2 = K @ np.hstack((R2, t2))\n",
    "        \n",
    "    # Triangulate points using OpenCV\n",
    "    # cv2.triangulatePoints requires points in (2, N) format, so we transpose our (N, 2) inputs\n",
    "    points_4d_hom = cv2.triangulatePoints(P1, P2, points1.T, points2.T)\n",
    "    \n",
    "    # Convert homogeneous coordinates (x,y,z,w) to Euclidean coordinates (x,y,z)\n",
    "    points_3d = (points_4d_hom[:3, :] / points_4d_hom[3, :]).T\n",
    "    \n",
    "    return points_3d\n",
    "\n",
    "def get_pixel_colors(img, coordinate_list):\n",
    "    # Initialize empty array for color values\n",
    "    pixel_values = []\n",
    "    image_height, image_width = img.shape[:2]\n",
    "    \n",
    "    # Process each coordinate\n",
    "    for coord in coordinate_list:\n",
    "        col, row = map(lambda x: int(x + 0.5), coord)\n",
    "        \n",
    "        # Validate coordinates\n",
    "        is_valid = (0 <= col < image_width) and (0 <= row < image_height)\n",
    "        \n",
    "        if is_valid:\n",
    "            # Normalize RGB values to [0,1] range\n",
    "            rgb = img[row, col].astype(float) / 255\n",
    "        else:\n",
    "            rgb = np.array([0.5, 0.5, 0.5])  # Gray color for invalid points\n",
    "            \n",
    "        pixel_values.append(rgb)\n",
    "    \n",
    "    return np.array(pixel_values)\n",
    "\n",
    "def visualize_reconstruction(points_3d, colors, window_name):\n",
    "    # Create a point cloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    # Flip Y and Z coordinates for correct orientation\n",
    "    points_3d = points_3d.copy()\n",
    "    points_3d[:, 1] = -points_3d[:, 1] # Y coordinate\n",
    "    points_3d[:, 2] = -points_3d[:, 2] # Z coordinate\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(points_3d)\n",
    "    # Add RGB colors to the points\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    print(\"\\nVisualizing point cloud.\")\n",
    "    print(\"Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\")\n",
    "    print(\"Press 'q' to close the window.\")\n",
    "\n",
    "    # Display the 3D point cloud\n",
    "    o3d.visualization.draw_geometries([pcd], window_name=window_name)\n",
    "\n",
    "def bundle_adjust_poses(camera_poses, all_points_3d, all_matches, K, max_iterations=10):\n",
    "    # Iterative bundle adjustment to refine camera poses and 3D points.\n",
    "    print(f\"Starting bundle adjustment with {len(camera_poses)} poses...\")\n",
    "    \n",
    "    # Return early if no 3D points exist\n",
    "    if not all_points_3d:\n",
    "        print(\"No 3D points provided for bundle adjustment!\")\n",
    "        return camera_poses, []\n",
    "    \n",
    "    # Create copies to avoid modifying inputs\n",
    "    refined_poses = camera_poses.copy()\n",
    "    refined_points_3d = [points.copy() for points in all_points_3d]  # Keep as list of arrays\n",
    "    \n",
    "    # Iteratively refine poses up to max_iterations times\n",
    "    for iter_idx in range(max_iterations):\n",
    "        total_reproj_error = 0\n",
    "        pose_updates = 0\n",
    "        \n",
    "        # Iterate through each camera pose after first reference camera\n",
    "        for cam_idx in range(1, len(camera_poses)):\n",
    "            # Skip if we don't have matching data for this camera\n",
    "            if cam_idx - 1 >= len(all_matches):\n",
    "                continue\n",
    "                \n",
    "            # Get corresponding 2D-3D point pairs for this camera\n",
    "            match_data = all_matches[cam_idx - 1]\n",
    "            frame_points_3d = refined_points_3d[cam_idx - 1]  # Use refined points\n",
    "            image_points = match_data.pts2.astype(np.float32)\n",
    "            object_points = frame_points_3d.astype(np.float32)\n",
    "            \n",
    "            # Need at least 6 points for reliable pose estimation\n",
    "            if len(object_points) < 6:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Initialize with current pose estimate\n",
    "                R_current, t_current = refined_poses[cam_idx]\n",
    "                rvec_init, _ = cv2.Rodrigues(R_current)\n",
    "                tvec_init = t_current.flatten()\n",
    "                \n",
    "                # Use PnP RANSAC to optimize the camera pose\n",
    "                success, rvec_refined, tvec_refined, inliers = cv2.solvePnPRansac(\n",
    "                    object_points.reshape(-1, 1, 3),\n",
    "                    image_points.reshape(-1, 1, 2),\n",
    "                    K,\n",
    "                    None,\n",
    "                    rvec_init,\n",
    "                    tvec_init,\n",
    "                    useExtrinsicGuess=True,\n",
    "                    iterationsCount=100,\n",
    "                    reprojectionError=2.0,\n",
    "                    confidence=0.99,\n",
    "                    flags=cv2.SOLVEPNP_ITERATIVE\n",
    "                )\n",
    "                \n",
    "                # Only update pose if we have a good solution with sufficient inliers\n",
    "                if success and inliers is not None and len(inliers) > len(object_points) * 0.5:\n",
    "                    R_refined, _ = cv2.Rodrigues(rvec_refined)\n",
    "                    t_refined = tvec_refined.reshape(3, 1)\n",
    "                    \n",
    "                    refined_poses[cam_idx] = (R_refined, t_refined)\n",
    "                    pose_updates += 1\n",
    "                    \n",
    "            except cv2.error as e:\n",
    "                print(f\"Camera {cam_idx}: PnP failed - {e}\")\n",
    "                continue\n",
    "                \n",
    "        # Early stopping if very few poses were updated\n",
    "        if pose_updates < len(camera_poses) * 0.2:\n",
    "            print(\"Early stopping due to few pose updates\")\n",
    "            break\n",
    "    \n",
    "    return refined_poses, refined_points_3d\n",
    "\n",
    "def analyze_reconstruction_quality(points_3d, points1, points2, P1, P2, baseline_length):\n",
    "    # Function to analyze quality metrics for 3D reconstruction\n",
    "\n",
    "    # Convert 3D points to homogeneous coordinates by adding 1 as fourth dimension\n",
    "    # Project 3D points back into 2D using both camera matrices\n",
    "    points_3d_hom = np.hstack([points_3d, np.ones((len(points_3d), 1))])\n",
    "    reproj1 = (P1 @ points_3d_hom.T)\n",
    "    reproj2 = (P2 @ points_3d_hom.T)\n",
    "\n",
    "    # Convert back from homogeneous to euclidean coordinates\n",
    "    reproj1 = (reproj1[:2, :] / reproj1[2, :]).T\n",
    "    reproj2 = (reproj2[:2, :] / reproj2[2, :]).T\n",
    "    \n",
    "    # Calculate reprojection error as L2 distance between original and reprojected points\n",
    "    error1 = np.linalg.norm(reproj1 - points1, axis=1)\n",
    "    error2 = np.linalg.norm(reproj2 - points2, axis=1)\n",
    "    mean_reproj_error = (np.mean(error1) + np.mean(error2)) / 2\n",
    "    \n",
    "    # Calculate distances of reconstructed 3D points from origin\n",
    "    distances = np.linalg.norm(points_3d, axis=1)\n",
    "        \n",
    "    # Print statistics\n",
    "    print(f\"\\nReconstruction Quality Metrics:\")\n",
    "    print(f\"Points: {len(points_3d)}\")\n",
    "    print(f\"Reprojection error: {mean_reproj_error:.2f} pixels\")\n",
    "    print(f\"Baseline: {baseline_length:.3f}\")\n",
    "    print(f\"Scale ratio: {np.max(distances)/baseline_length:.1f}x\")\n",
    "    \n",
    "    # Return dictionary with key metrics\n",
    "    return {\n",
    "        'mean_reprojection_error': mean_reproj_error,\n",
    "        'num_points': len(points_3d),\n",
    "        'scale_ratio': np.max(distances)/baseline_length\n",
    "    }\n",
    "\n",
    "def save_point_cloud_ply(filename, points, colors):\n",
    "    # Save 3D points with colors to a .ply file\n",
    "\n",
    "    # Create reconstructions directory if it doesn't exist\n",
    "    reconstruction_dir = 'reconstructions'\n",
    "    os.makedirs(reconstruction_dir, exist_ok=True)\n",
    "\n",
    "    # Join path with filename\n",
    "    filepath = os.path.join(reconstruction_dir, filename)\n",
    "\n",
    "    # Flip coordinates for correct orientation\n",
    "    points = points.copy()\n",
    "    points[:, 1] = -points[:, 1]  # Y coordinate\n",
    "    points[:, 2] = -points[:, 2]  # Z coordinate\n",
    "\n",
    "    # Ensure points and colors are float32 / uint8\n",
    "    points = points.astype(np.float32)\n",
    "    colors = (colors * 255).astype(np.uint8)  # If colors were normalized [0,1]\n",
    "\n",
    "    # Create a structured array\n",
    "    vertex_data = np.array(\n",
    "        [tuple(p) + tuple(c) for p, c in zip(points, colors)],\n",
    "        dtype=[\n",
    "            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "            ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    el = plyfile.PlyElement.describe(vertex_data, 'vertex')\n",
    "    plyfile.PlyData([el]).write(filepath)\n",
    "    print(f\"Saved point cloud to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7c20a",
   "metadata": {},
   "source": [
    "Main reconstruction pipeline that combines all previous components. Starting with camera poses and matched features, it performs sequential reconstruction by triangulating 3D points, optionally refines results through bundle adjustment, analyzes reconstruction quality, and generates the final point cloud with color information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fc37fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_scene(pose_estimations, ransac_results, Frames, detector_name, use_bundle_adjustment=True, visualize=True):\n",
    "    # Initialize first camera pose\n",
    "    R_global = np.eye(3)\n",
    "    t_global = np.zeros((3, 1))\n",
    "    camera_poses = [(R_global, t_global)] # Store camera poses as list of (R,t) tuples, starting with initial pose\n",
    "\n",
    "    # Lists to store 3D points and their RGB colors\n",
    "    all_points_3d = []\n",
    "    all_colors = []\n",
    "\n",
    "    # List to store reconstruction quality metrics\n",
    "    all_quality_metrics = []\n",
    "\n",
    "    # Get camera intrinsic matrix K from first pose estimation result\n",
    "    if not pose_estimations:\n",
    "        print(\"No pose estimations available to proceed with triangulation.\")\n",
    "        exit()\n",
    "        \n",
    "    K = pose_estimations[0]['K']\n",
    "\n",
    "    # Process each pose estimation sequentially to build up 3D reconstruction\n",
    "    for i, pose_result in enumerate(pose_estimations):\n",
    "        # Extract relative pose (R,t) between current frame pair\n",
    "        R_rel = pose_result.get(\"R\")\n",
    "        t_rel = pose_result.get(\"t\")\n",
    "\n",
    "        # Skip if invalid pose\n",
    "        if R_rel is None or t_rel is None:\n",
    "            print(f\"Skipping pair {ransac_results[i].frame_pair} due to invalid pose.\")\n",
    "            continue\n",
    "\n",
    "        # Get current RANSAC result\n",
    "        ransac_res = ransac_results[i]\n",
    "        \n",
    "        # Calculate global pose\n",
    "        R_new_global = R_global @ R_rel\n",
    "        t_new_global = t_global + R_global @ t_rel\n",
    "        \n",
    "        # Initial 3D point triangulation\n",
    "        points_3d = triangulate_points(\n",
    "            K,\n",
    "            (R_global, t_global),\n",
    "            (R_new_global, t_new_global),\n",
    "            ransac_res.pts1,\n",
    "            ransac_res.pts2\n",
    "        )\n",
    "        \n",
    "        # Get RGB color values for each 3D point from first frame\n",
    "        frame_idx_1, _ = ransac_res.frame_pair\n",
    "        colors = get_pixel_colors(Frames[frame_idx_1], ransac_res.pts1)\n",
    "\n",
    "        # Store results for this frame pair\n",
    "        all_points_3d.append(points_3d)\n",
    "        all_colors.append(colors)\n",
    "        \n",
    "        print(f\"Triangulated {len(points_3d)} points.\")\n",
    "\n",
    "        # Update global pose for next iteration\n",
    "        R_global = R_new_global\n",
    "        t_global = t_new_global\n",
    "        camera_poses.append((R_global, t_global))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # Apply bundle adjustment\n",
    "    if use_bundle_adjustment and len(camera_poses) > 1 and all_points_3d:\n",
    "        print(\"\\nApplying bundle adjustment...\")\n",
    "        refined_poses, refined_points_3d = bundle_adjust_poses(\n",
    "            camera_poses, all_points_3d, ransac_results, K\n",
    "        )\n",
    "        \n",
    "        # Use refined results\n",
    "        camera_poses = refined_poses\n",
    "        all_points_3d = refined_points_3d\n",
    "        \n",
    "        print(\"Bundle adjustment completed.\")\n",
    "\n",
    "    # Calculate reconstruction quality metrics \n",
    "    for i in range(len(ransac_results)):\n",
    "        if i + 1 >= len(camera_poses):\n",
    "            continue\n",
    "            \n",
    "        ransac_res = ransac_results[i]\n",
    "        R1, t1 = camera_poses[i]\n",
    "        R2, t2 = camera_poses[i + 1]\n",
    "        \n",
    "        # Use existing points if available, otherwise triangulate\n",
    "        if i < len(all_points_3d):\n",
    "            points_3d = all_points_3d[i]\n",
    "        else:\n",
    "            points_3d = triangulate_points(\n",
    "                K, (R1, t1), (R2, t2),\n",
    "                ransac_res.pts1, ransac_res.pts2\n",
    "            )\n",
    "            all_points_3d.append(points_3d)\n",
    "        \n",
    "        # Get colors if not already stored\n",
    "        if i >= len(all_colors):\n",
    "            colors = get_pixel_colors(Frames[ransac_res.frame_pair[0]], ransac_res.pts1)\n",
    "            all_colors.append(colors)\n",
    "        \n",
    "        # Construct projection matrices for quality analysis\n",
    "        P1 = K @ np.hstack((R1, t1))\n",
    "        P2 = K @ np.hstack((R2, t2))\n",
    "        \n",
    "        # Analyze quality\n",
    "        quality = analyze_reconstruction_quality(\n",
    "            points_3d,\n",
    "            ransac_res.pts1,\n",
    "            ransac_res.pts2,\n",
    "            P1, P2,\n",
    "            np.linalg.norm(t2 - t1)\n",
    "        )\n",
    "        \n",
    "        all_quality_metrics.append(quality)\n",
    "        \n",
    "    # Compute overall reconstruction quality metrics across all frame pairs\n",
    "    if all_quality_metrics:\n",
    "        avg_error = np.mean([m['mean_reprojection_error'] for m in all_quality_metrics])\n",
    "        avg_scale = np.mean([m['scale_ratio'] for m in all_quality_metrics])\n",
    "        total_points = sum([m['num_points'] for m in all_quality_metrics])\n",
    "\n",
    "        print(f\"\\n{detector_name} Final Reconstruction Quality Summary:\")\n",
    "        print(f\"Average Reprojection Error: {avg_error:.2f} pixels\")\n",
    "        print(f\"Average Scale Ratio: {avg_scale:.1f}x\")\n",
    "        print(f\"Total Reconstructed Points: {total_points}\")\n",
    "\n",
    "    # Combine all frame pairs results into final point cloud\n",
    "    if all_points_3d:\n",
    "        final_points_3d = np.concatenate(all_points_3d, axis=0)\n",
    "        final_colors = np.concatenate(all_colors, axis=0)\n",
    "        \n",
    "        # Display final combined point cloud\n",
    "        print(f\"\\nTotal reconstructed points: {len(final_points_3d)}\")\n",
    "        \n",
    "        if visualize:\n",
    "            visualize_reconstruction(final_points_3d, final_colors, window_name=f'{detector_name} 3D Scene Reconstruction')\n",
    "        save_point_cloud_ply(f\"{detector_name}_reconstruction.ply\", final_points_3d, final_colors)\n",
    "\n",
    "    else:\n",
    "        print(\"Could not reconstruct any 3D points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310a735",
   "metadata": {},
   "source": [
    "## Main Driver Code\n",
    "\n",
    "Setup of input parameters, output control flags, and algorithm mappings for the reconstruction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de3b324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variables\n",
    "VIDEO_PATH = \"../videos/Video_Selection_CV.mp4\"\n",
    "NUMBER_OF_FRAME_PAIRS = 11\n",
    "\n",
    "# Output Toggles\n",
    "PLOT_DETECTOR_BENCHMARK_COMPARISONS = False\n",
    "PLOT_FEATURE_DETECTOR_PAIRS = False\n",
    "PLOT_RANSAC_FILTERING = False\n",
    "PLOT_FUNDAMENTAL_MATRIX_COMPUTATION = False\n",
    "PLOT_CAMERA_POSE_ESTIMATIONS = False\n",
    "VISUALIZE_3D_RECONTRUCTION = True\n",
    "CREATE_FEATURE_MONTAGE_VIDEOS = False\n",
    "\n",
    "# 3D Scene Reconstruction Toggles\n",
    "USE_BUNDLE_ADJUSTMENT = True\n",
    "\n",
    "ALGORITHM_NAME_FUNC_MAP = {\"ORB\" : detect_orb,\n",
    "                            \"SIFT\": detect_sift,\n",
    "                            \"FAST\": detect_fast,\n",
    "                            \"SHI-TOMASI\": detect_shi_tomasi,\n",
    "                            \"HARRIS\": detect_harris}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b74e0e",
   "metadata": {},
   "source": [
    "Initial frame extraction from video using specified indices. This step prepares the base frames that will be processed through the reconstruction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a33a3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlapping indices: [0, 24, 48, 72, 96, 120, 144, 168, 192, 216, 240]\n",
      "\n",
      "Finished loading selected frames.\n",
      "Total frames processed: 241\n"
     ]
    }
   ],
   "source": [
    "# Frame preparation\n",
    "indices = get_frame_indices(VIDEO_PATH, NUMBER_OF_FRAME_PAIRS)\n",
    "print(f\"Overlapping indices: {indices}\")\n",
    "Frames = load_specific_frames(VIDEO_PATH, selected_indices=indices, display_frames=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ba310",
   "metadata": {},
   "source": [
    "Main processing loop that runs each feature detector through the reconstruction pipeline:\n",
    "1. Feature detection and timing\n",
    "2. Feature matching between frames\n",
    "3. RANSAC-based outlier filtering\n",
    "4. Fundamental matrix computation\n",
    "5. Camera pose estimation\n",
    "6. 3D reconstruction with optional bundle adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19988e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running:ORB, func:detect_orb\n",
      "[ORB] Matched 0 â†’ 1: Before Lowe:3000 matches, After Lowe 1216 matches\n",
      "[ORB] Matched 1 â†’ 2: Before Lowe:3000 matches, After Lowe 1251 matches\n",
      "[ORB] Matched 2 â†’ 3: Before Lowe:3000 matches, After Lowe 1310 matches\n",
      "[ORB] Matched 3 â†’ 4: Before Lowe:3000 matches, After Lowe 1315 matches\n",
      "[ORB] Matched 4 â†’ 5: Before Lowe:3000 matches, After Lowe 1181 matches\n",
      "[ORB] Matched 5 â†’ 6: Before Lowe:3000 matches, After Lowe 942 matches\n",
      "[ORB] Matched 6 â†’ 7: Before Lowe:3000 matches, After Lowe 870 matches\n",
      "[ORB] Matched 7 â†’ 8: Before Lowe:3000 matches, After Lowe 762 matches\n",
      "[ORB] Matched 8 â†’ 9: Before Lowe:3000 matches, After Lowe 861 matches\n",
      "[ORB] Matched 9 â†’ 10: Before Lowe:3000 matches, After Lowe 1097 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 692 out of 732 (94.54%)\n",
      "Mean epipolar error: 1.0791 pixels\n",
      "Max epipolar error: 2.9453 pixels\n",
      "Std. epipolar error: 0.7681 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 735 out of 753 (97.61%)\n",
      "Mean epipolar error: 0.8605 pixels\n",
      "Max epipolar error: 2.9680 pixels\n",
      "Std. epipolar error: 0.6492 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 879 out of 931 (94.41%)\n",
      "Mean epipolar error: 0.9391 pixels\n",
      "Max epipolar error: 2.9830 pixels\n",
      "Std. epipolar error: 0.7406 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 686 out of 781 (87.84%)\n",
      "Mean epipolar error: 0.8888 pixels\n",
      "Max epipolar error: 2.9852 pixels\n",
      "Std. epipolar error: 0.7245 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 727 out of 756 (96.16%)\n",
      "Mean epipolar error: 1.0753 pixels\n",
      "Max epipolar error: 2.9970 pixels\n",
      "Std. epipolar error: 0.7426 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 454 out of 463 (98.06%)\n",
      "Mean epipolar error: 0.8717 pixels\n",
      "Max epipolar error: 2.9702 pixels\n",
      "Std. epipolar error: 0.6491 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 363 out of 391 (92.84%)\n",
      "Mean epipolar error: 1.0117 pixels\n",
      "Max epipolar error: 2.9655 pixels\n",
      "Std. epipolar error: 0.7077 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 352 out of 401 (87.78%)\n",
      "Mean epipolar error: 1.1816 pixels\n",
      "Max epipolar error: 2.9923 pixels\n",
      "Std. epipolar error: 0.7781 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 438 out of 446 (98.21%)\n",
      "Mean epipolar error: 0.8512 pixels\n",
      "Max epipolar error: 2.9687 pixels\n",
      "Std. epipolar error: 0.6552 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 500 out of 547 (91.41%)\n",
      "Mean epipolar error: 1.0425 pixels\n",
      "Max epipolar error: 2.9967 pixels\n",
      "Std. epipolar error: 0.7837 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -1.83945213 -56.96127185 -44.10206662]\n",
      " [ 55.38876096   0.76662989 -29.04664729]\n",
      " [ 44.57862844  22.75275723  -5.85643598]]\n",
      "Essential matrix singular values: [7.79321917e+01 7.46390384e+01 3.67609516e-15]\n",
      "Pose Inliers: 278/732\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999997\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.894866287095815e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 6.3779145211983925\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.93602914 -15.50570831  -8.87431007]\n",
      " [ 16.48449858   0.28120461 -11.14108553]\n",
      " [  8.62840218  12.69417342   0.83066697]]\n",
      "Essential matrix singular values: [2.18802772e+01 2.17608115e+01 1.05656103e-15]\n",
      "Pose Inliers: 67/753\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.530078123843497e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.95816555182869\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[-0.17640965 -7.08662964 -0.47273518]\n",
      " [ 7.59713079  0.06829126 -8.11102487]\n",
      " [ 0.37920088  8.09206038  0.34986922]]\n",
      "Essential matrix singular values: [1.11194560e+01 1.07745090e+01 1.62256266e-15]\n",
      "Pose Inliers: 931/931\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.37993427481533e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 3.406291254107182\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[-2.12725544e-01 -8.17731536e+00 -1.43182174e-01]\n",
      " [ 8.90392001e+00 -3.70528553e-02 -1.32611637e+01]\n",
      " [ 4.74031002e-01  1.27598579e+01  1.18237620e-02]]\n",
      "Essential matrix singular values: [1.59836943e+01 1.51536861e+01 1.12357236e-15]\n",
      "Pose Inliers: 781/781\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.93644619222047e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 1.8439269063059995\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[-5.07918463e-01  1.03382711e+01  5.74410722e+00]\n",
      " [-8.93884943e+00 -8.46756412e-03 -6.90029716e+00]\n",
      " [-5.27507472e+00  6.64767324e+00 -1.23614200e-01]]\n",
      "Essential matrix singular values: [1.36040775e+01 1.24343126e+01 2.82603059e-15]\n",
      "Pose Inliers: 756/756\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999999\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.228222359857072e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.648971406704319\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.52691632 -25.05057045  -6.17596368]\n",
      " [ 26.08774598   0.25933156 -25.77381875]\n",
      " [  5.71271336  26.99676623   0.43669763]]\n",
      "Essential matrix singular values: [3.73541294e+01 3.71106309e+01 4.24267020e-15]\n",
      "Pose Inliers: 463/463\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.741516792284726e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 2.5177502755073564\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -0.18042552  -3.09471962   0.94391822]\n",
      " [  6.81632246  -0.05388493 -25.11548702]\n",
      " [ -0.50061633  26.43419139  -0.53813592]]\n",
      "Essential matrix singular values: [2.66983137e+01 2.59665245e+01 1.92396487e-15]\n",
      "Pose Inliers: 391/391\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.1800592364255425e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 8.562689981642439\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  1.17115616  23.43330182  -0.57865563]\n",
      " [-24.10155274   1.53776877  27.72282359]\n",
      " [ -0.51462125 -25.95105876  -0.25745904]]\n",
      "Essential matrix singular values: [3.68175737e+01 3.49411713e+01 1.45412104e-15]\n",
      "Pose Inliers: 401/401\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.991169117332785e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 3.3358390919991447\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 1.33149767e-01 -8.80437491e-01 -2.48210217e+00]\n",
      " [-8.21992674e-01  8.85573960e-03  2.37947805e+01]\n",
      " [ 1.32785754e+00 -2.48587946e+01  3.48118923e-01]]\n",
      "Essential matrix singular values: [2.49214106e+01 2.39288197e+01 1.96594509e-16]\n",
      "Pose Inliers: 446/446\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999999\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.5485896100556755e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.965314909775391\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 0.41418749  7.02848965 -3.1920418 ]\n",
      " [-5.90991442  0.21802052 -8.92604668]\n",
      " [ 3.067841    9.262139   -0.44775265]]\n",
      "Essential matrix singular values: [1.22271142e+01 1.09684233e+01 2.83150838e-16]\n",
      "Pose Inliers: 511/547\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999999\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.37044039398452e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 3.8609769095273987\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 732 points.\n",
      "Triangulated 753 points.\n",
      "Triangulated 931 points.\n",
      "Triangulated 781 points.\n",
      "Triangulated 756 points.\n",
      "Triangulated 463 points.\n",
      "Triangulated 391 points.\n",
      "Triangulated 401 points.\n",
      "Triangulated 446 points.\n",
      "Triangulated 547 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 732\n",
      "Reprojection error: 1.29 pixels\n",
      "Baseline: 1.017\n",
      "Scale ratio: 51546.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 753\n",
      "Reprojection error: 0.74 pixels\n",
      "Baseline: 0.934\n",
      "Scale ratio: 18366.6x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 931\n",
      "Reprojection error: 2.03 pixels\n",
      "Baseline: 0.715\n",
      "Scale ratio: 65.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 781\n",
      "Reprojection error: 3.75 pixels\n",
      "Baseline: 0.839\n",
      "Scale ratio: 32.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 756\n",
      "Reprojection error: 5.49 pixels\n",
      "Baseline: 0.731\n",
      "Scale ratio: 36.3x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 463\n",
      "Reprojection error: 5.53 pixels\n",
      "Baseline: 0.961\n",
      "Scale ratio: 14.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 391\n",
      "Reprojection error: 2.87 pixels\n",
      "Baseline: 0.757\n",
      "Scale ratio: 10.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 401\n",
      "Reprojection error: 7.85 pixels\n",
      "Baseline: 1.381\n",
      "Scale ratio: 13.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 446\n",
      "Reprojection error: 4.99 pixels\n",
      "Baseline: 0.838\n",
      "Scale ratio: 11.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 547\n",
      "Reprojection error: 7.01 pixels\n",
      "Baseline: 0.497\n",
      "Scale ratio: 24979.6x\n",
      "\n",
      "ORB Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 4.15 pixels\n",
      "Average Scale Ratio: 9507.5x\n",
      "Total Reconstructed Points: 6201\n",
      "\n",
      "Total reconstructed points: 6201\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\ORB_reconstruction.ply\n",
      "\n",
      "Running:SIFT, func:detect_sift\n",
      "[SIFT] Matched 0 â†’ 1: Before Lowe:1500 matches, After Lowe 757 matches\n",
      "[SIFT] Matched 1 â†’ 2: Before Lowe:1501 matches, After Lowe 790 matches\n",
      "[SIFT] Matched 2 â†’ 3: Before Lowe:1500 matches, After Lowe 888 matches\n",
      "[SIFT] Matched 3 â†’ 4: Before Lowe:1500 matches, After Lowe 849 matches\n",
      "[SIFT] Matched 4 â†’ 5: Before Lowe:1500 matches, After Lowe 808 matches\n",
      "[SIFT] Matched 5 â†’ 6: Before Lowe:1500 matches, After Lowe 817 matches\n",
      "[SIFT] Matched 6 â†’ 7: Before Lowe:1500 matches, After Lowe 810 matches\n",
      "[SIFT] Matched 7 â†’ 8: Before Lowe:1501 matches, After Lowe 672 matches\n",
      "[SIFT] Matched 8 â†’ 9: Before Lowe:1500 matches, After Lowe 711 matches\n",
      "[SIFT] Matched 9 â†’ 10: Before Lowe:1500 matches, After Lowe 851 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 425 out of 450 (94.44%)\n",
      "Mean epipolar error: 0.6538 pixels\n",
      "Max epipolar error: 2.9061 pixels\n",
      "Std. epipolar error: 0.5494 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 469 out of 507 (92.50%)\n",
      "Mean epipolar error: 0.7464 pixels\n",
      "Max epipolar error: 2.9569 pixels\n",
      "Std. epipolar error: 0.6191 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 618 out of 623 (99.20%)\n",
      "Mean epipolar error: 0.7062 pixels\n",
      "Max epipolar error: 2.8488 pixels\n",
      "Std. epipolar error: 0.5757 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 528 out of 530 (99.62%)\n",
      "Mean epipolar error: 0.5495 pixels\n",
      "Max epipolar error: 2.4973 pixels\n",
      "Std. epipolar error: 0.4914 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 440 out of 440 (100.00%)\n",
      "Mean epipolar error: 0.5635 pixels\n",
      "Max epipolar error: 2.2856 pixels\n",
      "Std. epipolar error: 0.4388 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 418 out of 420 (99.52%)\n",
      "Mean epipolar error: 0.4665 pixels\n",
      "Max epipolar error: 2.2669 pixels\n",
      "Std. epipolar error: 0.3766 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 313 out of 313 (100.00%)\n",
      "Mean epipolar error: 0.4698 pixels\n",
      "Max epipolar error: 2.4281 pixels\n",
      "Std. epipolar error: 0.4778 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 295 out of 299 (98.66%)\n",
      "Mean epipolar error: 0.3934 pixels\n",
      "Max epipolar error: 2.8205 pixels\n",
      "Std. epipolar error: 0.4403 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 294 out of 306 (96.08%)\n",
      "Mean epipolar error: 0.9800 pixels\n",
      "Max epipolar error: 2.9703 pixels\n",
      "Std. epipolar error: 0.7392 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 444 out of 444 (100.00%)\n",
      "Mean epipolar error: 0.4529 pixels\n",
      "Max epipolar error: 2.5209 pixels\n",
      "Std. epipolar error: 0.3781 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[-5.22675950e-02 -2.91356841e+00 -5.63646793e-01]\n",
      " [ 3.55027797e+00 -6.36389668e-03 -1.20534945e+01]\n",
      " [ 9.75165919e-01  1.11995476e+01 -4.61683890e-01]]\n",
      "Essential matrix singular values: [1.26610965e+01 1.15321792e+01 4.40636669e-17]\n",
      "Pose Inliers: 450/450\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.942859033474006e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 2.4093980959805545\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.43861713  -8.71215717  -4.49882926]\n",
      " [  9.7151675    0.20435871 -11.73889537]\n",
      " [  4.37675384  12.60778738   0.40707971]]\n",
      "Essential matrix singular values: [1.59771085e+01 1.58610035e+01 1.64646491e-15]\n",
      "Pose Inliers: 14/507\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.829289854825745e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.743029062764031\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 7.79438310e-03  4.60021634e+00  2.20311907e+00]\n",
      " [-4.10620511e+00  3.04868432e-01 -8.27189386e+00]\n",
      " [-1.76034753e+00  8.34397095e+00  3.58749852e-01]]\n",
      "Essential matrix singular values: [9.79198170e+00 9.40001853e+00 3.48566922e-15]\n",
      "Pose Inliers: 623/623\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000009\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 9.333733745871804e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 3.7640398287634067\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 0.22537842  5.46964645 -0.74222371]\n",
      " [-4.78488478  0.13834956 -9.45693252]\n",
      " [ 1.00691675  9.34811566 -0.04363731]]\n",
      "Essential matrix singular values: [1.08868977e+01 1.06181481e+01 1.90985290e-15]\n",
      "Pose Inliers: 530/530\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999996\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.9343673627820955e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 3.9362838355796064\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 0.12893352 -0.31639082 -1.38917686]\n",
      " [ 1.19371196  0.01588392 -9.28653988]\n",
      " [ 1.44271867  9.30090147  0.04286161]]\n",
      "Essential matrix singular values: [9.50198271e+00 9.38157397e+00 1.98582882e-16]\n",
      "Pose Inliers: 440/440\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.457300619856263e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.324446586286176\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 5.93868680e-01 -1.14301183e+01 -7.34467309e+00]\n",
      " [ 1.54573635e+01 -8.23651514e-03 -4.68687148e+01]\n",
      " [ 7.73458580e+00  4.97492804e+01  6.80460595e-01]]\n",
      "Essential matrix singular values: [5.16333525e+01 4.98981538e+01 6.16615905e-15]\n",
      "Pose Inliers: 420/420\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.024824439000497e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 5.306904790155038\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[   3.38526105   54.32893496  -28.28732843]\n",
      " [ -70.14974171    0.95693114  161.64567542]\n",
      " [  26.74847915 -169.37810477    2.04204279]]\n",
      "Essential matrix singular values: [1.80013707e+02 1.78376431e+02 2.74431011e-14]\n",
      "Pose Inliers: 313/313\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.52173809509484e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.65296306058844\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.20877336   5.59137398   1.22718949]\n",
      " [ -1.54485151   0.28120424 -44.37456065]\n",
      " [  1.3748269   45.97852999   0.32208814]]\n",
      "Essential matrix singular values: [4.63465134e+01 4.44117116e+01 2.50046757e-16]\n",
      "Pose Inliers: 299/299\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 2.237318591382128e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.91679461075841\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 0.33963747  6.53228412 -1.67335144]\n",
      " [-6.28752732  0.28095096  7.29341008]\n",
      " [ 1.15568965 -7.00807749  0.01871519]]\n",
      "Essential matrix singular values: [9.82834662e+00 9.60440940e+00 7.26614391e-16]\n",
      "Pose Inliers: 305/306\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.829836616349133e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 3.921580157238643\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.44626467  -1.75944505  -5.41261353]\n",
      " [  0.25580381  -0.07526851  15.87267234]\n",
      " [  5.06581034 -16.91304341   0.76319608]]\n",
      "Essential matrix singular values: [1.77782085e+01 1.67581399e+01 5.53537111e-16]\n",
      "Pose Inliers: 444/444\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.651092229098438e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.784973391315095\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 450 points.\n",
      "Triangulated 507 points.\n",
      "Triangulated 623 points.\n",
      "Triangulated 530 points.\n",
      "Triangulated 440 points.\n",
      "Triangulated 420 points.\n",
      "Triangulated 313 points.\n",
      "Triangulated 299 points.\n",
      "Triangulated 306 points.\n",
      "Triangulated 444 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 450\n",
      "Reprojection error: 5.78 pixels\n",
      "Baseline: 0.623\n",
      "Scale ratio: 12.4x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 507\n",
      "Reprojection error: 16.70 pixels\n",
      "Baseline: 0.568\n",
      "Scale ratio: 391489.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 623\n",
      "Reprojection error: 3.00 pixels\n",
      "Baseline: 0.884\n",
      "Scale ratio: 63.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 530\n",
      "Reprojection error: 4.71 pixels\n",
      "Baseline: 1.071\n",
      "Scale ratio: 14.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 440\n",
      "Reprojection error: 1.56 pixels\n",
      "Baseline: 1.082\n",
      "Scale ratio: 14.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 420\n",
      "Reprojection error: 3.18 pixels\n",
      "Baseline: 0.880\n",
      "Scale ratio: 15.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 313\n",
      "Reprojection error: 4.82 pixels\n",
      "Baseline: 1.162\n",
      "Scale ratio: 12.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 299\n",
      "Reprojection error: 5.25 pixels\n",
      "Baseline: 0.904\n",
      "Scale ratio: 11.8x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 306\n",
      "Reprojection error: 6.76 pixels\n",
      "Baseline: 1.130\n",
      "Scale ratio: 42.8x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 444\n",
      "Reprojection error: 7.34 pixels\n",
      "Baseline: 1.098\n",
      "Scale ratio: 13.7x\n",
      "\n",
      "SIFT Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 5.91 pixels\n",
      "Average Scale Ratio: 39169.1x\n",
      "Total Reconstructed Points: 4332\n",
      "\n",
      "Total reconstructed points: 4332\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\SIFT_reconstruction.ply\n",
      "\n",
      "Running:FAST, func:detect_fast\n",
      "[FAST] Matched 0 â†’ 1: Before Lowe:702 matches, After Lowe 487 matches\n",
      "[FAST] Matched 1 â†’ 2: Before Lowe:976 matches, After Lowe 621 matches\n",
      "[FAST] Matched 2 â†’ 3: Before Lowe:901 matches, After Lowe 622 matches\n",
      "[FAST] Matched 3 â†’ 4: Before Lowe:996 matches, After Lowe 682 matches\n",
      "[FAST] Matched 4 â†’ 5: Before Lowe:974 matches, After Lowe 615 matches\n",
      "[FAST] Matched 5 â†’ 6: Before Lowe:1096 matches, After Lowe 627 matches\n",
      "[FAST] Matched 6 â†’ 7: Before Lowe:1217 matches, After Lowe 673 matches\n",
      "[FAST] Matched 7 â†’ 8: Before Lowe:1344 matches, After Lowe 603 matches\n",
      "[FAST] Matched 8 â†’ 9: Before Lowe:1425 matches, After Lowe 723 matches\n",
      "[FAST] Matched 9 â†’ 10: Before Lowe:1353 matches, After Lowe 814 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 239 out of 252 (94.84%)\n",
      "Mean epipolar error: 0.9993 pixels\n",
      "Max epipolar error: 2.9876 pixels\n",
      "Std. epipolar error: 0.7611 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 321 out of 333 (96.40%)\n",
      "Mean epipolar error: 0.9488 pixels\n",
      "Max epipolar error: 2.9236 pixels\n",
      "Std. epipolar error: 0.6228 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 392 out of 422 (92.89%)\n",
      "Mean epipolar error: 0.9992 pixels\n",
      "Max epipolar error: 2.9594 pixels\n",
      "Std. epipolar error: 0.6999 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 361 out of 371 (97.30%)\n",
      "Mean epipolar error: 0.6301 pixels\n",
      "Max epipolar error: 2.8399 pixels\n",
      "Std. epipolar error: 0.5506 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 305 out of 325 (93.85%)\n",
      "Mean epipolar error: 0.8571 pixels\n",
      "Max epipolar error: 2.9605 pixels\n",
      "Std. epipolar error: 0.6899 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 285 out of 296 (96.28%)\n",
      "Mean epipolar error: 0.9582 pixels\n",
      "Max epipolar error: 2.9224 pixels\n",
      "Std. epipolar error: 0.6389 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 296 out of 334 (88.62%)\n",
      "Mean epipolar error: 1.0405 pixels\n",
      "Max epipolar error: 2.9984 pixels\n",
      "Std. epipolar error: 0.7780 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 251 out of 262 (95.80%)\n",
      "Mean epipolar error: 0.9479 pixels\n",
      "Max epipolar error: 2.9451 pixels\n",
      "Std. epipolar error: 0.7206 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 286 out of 311 (91.96%)\n",
      "Mean epipolar error: 1.0506 pixels\n",
      "Max epipolar error: 2.9816 pixels\n",
      "Std. epipolar error: 0.7673 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 385 out of 401 (96.01%)\n",
      "Mean epipolar error: 0.8816 pixels\n",
      "Max epipolar error: 2.9200 pixels\n",
      "Std. epipolar error: 0.6576 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -0.09192918 -10.72262275  -4.58360871]\n",
      " [ 10.51872078   0.12745706  -4.14871219]\n",
      " [  4.65034498   2.90078682  -0.60849305]]\n",
      "Essential matrix singular values: [1.23313634e+01 1.19252722e+01 6.96443228e-16]\n",
      "Pose Inliers: 96/252\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.792194358422187e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 6.655594581866415\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 8.88001778e-02  1.04836298e-01 -7.88708809e-01]\n",
      " [ 7.65785609e-01  7.44458790e-03 -7.91743364e+00]\n",
      " [ 9.39021129e-01  7.82336682e+00  1.62369595e-02]]\n",
      "Essential matrix singular values: [8.01019002e+00 7.86365848e+00 5.61036717e-16]\n",
      "Pose Inliers: 276/333\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 7.491173926079356e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 6.358102639465544\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[-0.19582377 -3.2516356   0.53357225]\n",
      " [ 3.99622219  0.03280831 -8.37319784]\n",
      " [-0.30518336  8.27706164  0.32541698]]\n",
      "Essential matrix singular values: [9.31804450e+00 8.88033331e+00 1.35459684e-15]\n",
      "Pose Inliers: 422/422\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999993\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.724835264054487e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.2534099382416946\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 5.50648084e-03  1.65594972e+00  8.88210127e-01]\n",
      " [-9.02494752e-01  1.46525876e-01 -1.26747065e+01]\n",
      " [-4.33114395e-01  1.24358143e+01 -3.08893136e-02]]\n",
      "Essential matrix singular values: [1.27485146e+01 1.25430724e+01 1.07272333e-16]\n",
      "Pose Inliers: 371/371\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.23013316093231e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 4.112805872841154\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -0.27342467   1.79573259   3.09514395]\n",
      " [ -0.64642143   0.06294369 -11.79761483]\n",
      " [ -2.87951652  11.77626131  -0.01368077]]\n",
      "Essential matrix singular values: [1.23567125e+01 1.21148116e+01 5.98670580e-16]\n",
      "Pose Inliers: 325/325\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999997\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.9765180717251e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.601077065506155\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 1.21617952e-02 -9.65395411e+00 -1.05190182e+00]\n",
      " [ 1.17200699e+01 -1.04450018e-01 -2.22880792e+01]\n",
      " [ 1.07117881e+00  2.33198284e+01  4.49060182e-01]]\n",
      "Essential matrix singular values: [2.54222473e+01 2.50461117e+01 8.35161342e-15]\n",
      "Pose Inliers: 296/296\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 9.958888435579866e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999998\n",
      "rotation_angle_deg: 5.337847291998807\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.79296454  25.58297377  -4.13242486]\n",
      " [-26.01905575  -0.03481833  16.9286966 ]\n",
      " [  3.24810284 -16.98990637   0.28916517]]\n",
      "Essential matrix singular values: [3.16506619e+01 3.05498581e+01 1.00965276e-14]\n",
      "Pose Inliers: 334/334\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999999\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.571716845429123e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 1.1188728649485349\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.14745899   6.81452517   3.46968258]\n",
      " [ -3.89422693   0.80590295 -35.29205207]\n",
      " [ -1.21755104  36.98144903   0.41278601]]\n",
      "Essential matrix singular values: [3.76369246e+01 3.56732942e+01 3.22255651e-16]\n",
      "Pose Inliers: 262/262\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.5428116233018457e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.613979000684898\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.37518606 -10.99961086 -10.96518659]\n",
      " [  8.04816655  -0.48641743  36.40697621]\n",
      " [  9.85705321 -39.44021617  -0.21913676]]\n",
      "Essential matrix singular values: [4.21908592e+01 3.87880987e+01 2.77241929e-15]\n",
      "Pose Inliers: 311/311\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 2.451374295322466e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 4.088405808908633\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  1.16232967 -12.43947519 -13.19342315]\n",
      " [  6.42119852  -0.58629467  51.67106673]\n",
      " [ 12.56789915 -55.05259991   2.58922156]]\n",
      "Essential matrix singular values: [5.79782377e+01 5.36246353e+01 6.00887190e-15]\n",
      "Pose Inliers: 401/401\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.0563410579286374e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 6.370610274205486\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 252 points.\n",
      "Triangulated 333 points.\n",
      "Triangulated 422 points.\n",
      "Triangulated 371 points.\n",
      "Triangulated 325 points.\n",
      "Triangulated 296 points.\n",
      "Triangulated 334 points.\n",
      "Triangulated 262 points.\n",
      "Triangulated 311 points.\n",
      "Triangulated 401 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 252\n",
      "Reprojection error: 0.86 pixels\n",
      "Baseline: 0.971\n",
      "Scale ratio: 13106.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 333\n",
      "Reprojection error: 2.22 pixels\n",
      "Baseline: 0.931\n",
      "Scale ratio: 412.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 422\n",
      "Reprojection error: 5.12 pixels\n",
      "Baseline: 0.861\n",
      "Scale ratio: 20.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 371\n",
      "Reprojection error: 4.03 pixels\n",
      "Baseline: 1.340\n",
      "Scale ratio: 10.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 325\n",
      "Reprojection error: 1.63 pixels\n",
      "Baseline: 0.936\n",
      "Scale ratio: 14.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 296\n",
      "Reprojection error: 2.90 pixels\n",
      "Baseline: 0.883\n",
      "Scale ratio: 13.4x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 334\n",
      "Reprojection error: 2.93 pixels\n",
      "Baseline: 1.087\n",
      "Scale ratio: 31.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 262\n",
      "Reprojection error: 7.31 pixels\n",
      "Baseline: 1.146\n",
      "Scale ratio: 10.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 311\n",
      "Reprojection error: 13.78 pixels\n",
      "Baseline: 0.928\n",
      "Scale ratio: 14.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 401\n",
      "Reprojection error: 1.93 pixels\n",
      "Baseline: 0.992\n",
      "Scale ratio: 11.6x\n",
      "\n",
      "FAST Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 4.27 pixels\n",
      "Average Scale Ratio: 1364.5x\n",
      "Total Reconstructed Points: 3307\n",
      "\n",
      "Total reconstructed points: 3307\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\FAST_reconstruction.ply\n",
      "\n",
      "Running:SHI-TOMASI, func:detect_shi_tomasi\n",
      "[SHI-TOMASI] Matched 0 â†’ 1: Before Lowe:1473 matches, After Lowe 538 matches\n",
      "[SHI-TOMASI] Matched 1 â†’ 2: Before Lowe:1370 matches, After Lowe 548 matches\n",
      "[SHI-TOMASI] Matched 2 â†’ 3: Before Lowe:1631 matches, After Lowe 658 matches\n",
      "[SHI-TOMASI] Matched 3 â†’ 4: Before Lowe:1665 matches, After Lowe 672 matches\n",
      "[SHI-TOMASI] Matched 4 â†’ 5: Before Lowe:1832 matches, After Lowe 618 matches\n",
      "[SHI-TOMASI] Matched 5 â†’ 6: Before Lowe:1866 matches, After Lowe 509 matches\n",
      "[SHI-TOMASI] Matched 6 â†’ 7: Before Lowe:2032 matches, After Lowe 475 matches\n",
      "[SHI-TOMASI] Matched 7 â†’ 8: Before Lowe:2176 matches, After Lowe 417 matches\n",
      "[SHI-TOMASI] Matched 8 â†’ 9: Before Lowe:2300 matches, After Lowe 502 matches\n",
      "[SHI-TOMASI] Matched 9 â†’ 10: Before Lowe:2281 matches, After Lowe 713 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 248 out of 277 (89.53%)\n",
      "Mean epipolar error: 0.9333 pixels\n",
      "Max epipolar error: 2.9520 pixels\n",
      "Std. epipolar error: 0.7357 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 316 out of 326 (96.93%)\n",
      "Mean epipolar error: 0.9288 pixels\n",
      "Max epipolar error: 2.9891 pixels\n",
      "Std. epipolar error: 0.6956 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 327 out of 348 (93.97%)\n",
      "Mean epipolar error: 0.9606 pixels\n",
      "Max epipolar error: 2.9903 pixels\n",
      "Std. epipolar error: 0.7439 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 291 out of 334 (87.13%)\n",
      "Mean epipolar error: 1.2348 pixels\n",
      "Max epipolar error: 2.9951 pixels\n",
      "Std. epipolar error: 0.7873 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 294 out of 297 (98.99%)\n",
      "Mean epipolar error: 0.6228 pixels\n",
      "Max epipolar error: 2.9930 pixels\n",
      "Std. epipolar error: 0.5273 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 287 out of 292 (98.29%)\n",
      "Mean epipolar error: 0.7213 pixels\n",
      "Max epipolar error: 2.9373 pixels\n",
      "Std. epipolar error: 0.5642 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 187 out of 190 (98.42%)\n",
      "Mean epipolar error: 0.9202 pixels\n",
      "Max epipolar error: 2.9820 pixels\n",
      "Std. epipolar error: 0.6867 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 166 out of 185 (89.73%)\n",
      "Mean epipolar error: 0.8381 pixels\n",
      "Max epipolar error: 2.8745 pixels\n",
      "Std. epipolar error: 0.7014 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 195 out of 225 (86.67%)\n",
      "Mean epipolar error: 0.9222 pixels\n",
      "Max epipolar error: 2.9875 pixels\n",
      "Std. epipolar error: 0.7064 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 283 out of 304 (93.09%)\n",
      "Mean epipolar error: 0.9315 pixels\n",
      "Max epipolar error: 2.9464 pixels\n",
      "Std. epipolar error: 0.7825 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 0.07539528 -5.60948691 -0.98784015]\n",
      " [ 5.75238948  0.03149232  3.91285407]\n",
      " [ 1.14078272 -4.61041048 -0.07916989]]\n",
      "Essential matrix singular values: [7.36543871e+00 7.01164103e+00 3.01030809e-16]\n",
      "Pose Inliers: 225/277\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.170629249199898e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 5.233266159447145\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -0.10832013  -6.29517051   1.32407928]\n",
      " [  8.0330555    0.08617054 -21.38254449]\n",
      " [ -1.26279363  21.84015806  -0.23494657]]\n",
      "Essential matrix singular values: [2.29478286e+01 2.26976699e+01 1.27313187e-15]\n",
      "Pose Inliers: 9/326\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.330184475713208e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.541002777625204\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 0.12938524 -5.91050145 -4.97383543]\n",
      " [ 6.34972016  0.03244042 -5.64771939]\n",
      " [ 4.86466974  5.59407144  0.25083855]]\n",
      "Essential matrix singular values: [9.79290844e+00 9.54080975e+00 1.37924305e-15]\n",
      "Pose Inliers: 348/348\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 7.060082646582946e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 2.500259426639725\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -0.2654225   23.24004576  21.19988458]\n",
      " [-21.32185938  -0.16127363 -28.346233  ]\n",
      " [-19.33464799  27.94605628   0.34611868]]\n",
      "Essential matrix singular values: [4.23714265e+01 4.00918270e+01 5.61334685e-15]\n",
      "Pose Inliers: 334/334\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 2.273101218682743e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999998\n",
      "rotation_angle_deg: 2.2198954111944222\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.05392628  -0.9572387   -0.68961702]\n",
      " [  1.88452629   0.07956453 -10.13377292]\n",
      " [  0.77894766  10.23205491   0.05925074]]\n",
      "Essential matrix singular values: [1.03695057e+01 1.02676423e+01 4.34829934e-16]\n",
      "Pose Inliers: 297/297\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.122368965600469e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.165397879275032\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[-2.52307259e-02 -5.42246302e+01 -6.17318205e+00]\n",
      " [ 6.34962858e+01  4.92081295e-01 -1.36217391e+02]\n",
      " [ 6.58850040e+00  1.44856147e+02  2.49569720e+00]]\n",
      "Essential matrix singular values: [1.54910513e+02 1.50337256e+02 2.58376067e-14]\n",
      "Pose Inliers: 292/292\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.4263881881132516e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.5410181469286295\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -0.62095381   4.2596904    6.19135094]\n",
      " [ -8.60291188   0.45603531  42.632758  ]\n",
      " [ -6.97834817 -45.12501706   1.06295186]]\n",
      "Essential matrix singular values: [4.60563628e+01 4.37440202e+01 1.02754349e-15]\n",
      "Pose Inliers: 190/190\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999999\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.458199010916417e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999998\n",
      "rotation_angle_deg: 6.150241131484582\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  2.06217628  10.38670604 -19.01290183]\n",
      " [-16.50762097   1.45545799  84.74440194]\n",
      " [ 13.63922171 -86.0286628   -2.38622918]]\n",
      "Essential matrix singular values: [9.01381378e+01 8.60094741e+01 3.76898570e-15]\n",
      "Pose Inliers: 185/185\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999989\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 2.364492451711603e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.604320236381291\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -0.16449318 -12.97551396  -5.44714789]\n",
      " [  5.97644314  -1.51817893  78.10626329]\n",
      " [  1.70325571 -83.28618183   1.27244871]]\n",
      "Essential matrix singular values: [8.44859269e+01 7.83575777e+01 1.75967750e-15]\n",
      "Pose Inliers: 225/225\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999996\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.1407660380799559e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.459049521777076\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.2858853    1.20503865  -3.47170619]\n",
      " [ -1.94938795  -0.02430407  10.87055988]\n",
      " [  3.13391594 -11.49398307   0.54629144]]\n",
      "Essential matrix singular values: [1.20617091e+01 1.15023064e+01 1.07420638e-16]\n",
      "Pose Inliers: 304/304\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.694357227976396e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.905718034180765\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 277 points.\n",
      "Triangulated 326 points.\n",
      "Triangulated 348 points.\n",
      "Triangulated 334 points.\n",
      "Triangulated 297 points.\n",
      "Triangulated 292 points.\n",
      "Triangulated 190 points.\n",
      "Triangulated 185 points.\n",
      "Triangulated 225 points.\n",
      "Triangulated 304 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 277\n",
      "Reprojection error: 2.99 pixels\n",
      "Baseline: 0.775\n",
      "Scale ratio: 849.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 326\n",
      "Reprojection error: 2.46 pixels\n",
      "Baseline: 1.139\n",
      "Scale ratio: 10575.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 348\n",
      "Reprojection error: 1.69 pixels\n",
      "Baseline: 0.918\n",
      "Scale ratio: 36.8x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 334\n",
      "Reprojection error: 2.93 pixels\n",
      "Baseline: 0.907\n",
      "Scale ratio: 32.3x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 297\n",
      "Reprojection error: 2.63 pixels\n",
      "Baseline: 0.878\n",
      "Scale ratio: 18.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 292\n",
      "Reprojection error: 1.78 pixels\n",
      "Baseline: 1.078\n",
      "Scale ratio: 12.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 190\n",
      "Reprojection error: 3.61 pixels\n",
      "Baseline: 0.924\n",
      "Scale ratio: 12.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 185\n",
      "Reprojection error: 4.43 pixels\n",
      "Baseline: 1.169\n",
      "Scale ratio: 7.8x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 225\n",
      "Reprojection error: 7.30 pixels\n",
      "Baseline: 0.851\n",
      "Scale ratio: 10.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 304\n",
      "Reprojection error: 8.60 pixels\n",
      "Baseline: 1.207\n",
      "Scale ratio: 16.1x\n",
      "\n",
      "SHI-TOMASI Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 3.84 pixels\n",
      "Average Scale Ratio: 1157.0x\n",
      "Total Reconstructed Points: 2778\n",
      "\n",
      "Total reconstructed points: 2778\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\SHI-TOMASI_reconstruction.ply\n",
      "\n",
      "Running:HARRIS, func:detect_harris\n",
      "[HARRIS] Matched 0 â†’ 1: Before Lowe:7287 matches, After Lowe 689 matches\n",
      "[HARRIS] Matched 1 â†’ 2: Before Lowe:2230 matches, After Lowe 404 matches\n",
      "[HARRIS] Matched 2 â†’ 3: Before Lowe:5309 matches, After Lowe 863 matches\n",
      "[HARRIS] Matched 3 â†’ 4: Before Lowe:5716 matches, After Lowe 980 matches\n",
      "[HARRIS] Matched 4 â†’ 5: Before Lowe:7733 matches, After Lowe 866 matches\n",
      "[HARRIS] Matched 5 â†’ 6: Before Lowe:8480 matches, After Lowe 394 matches\n",
      "[HARRIS] Matched 6 â†’ 7: Before Lowe:9751 matches, After Lowe 337 matches\n",
      "[HARRIS] Matched 7 â†’ 8: Before Lowe:9886 matches, After Lowe 372 matches\n",
      "[HARRIS] Matched 8 â†’ 9: Before Lowe:11399 matches, After Lowe 501 matches\n",
      "[HARRIS] Matched 9 â†’ 10: Before Lowe:7403 matches, After Lowe 665 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 463 out of 489 (94.68%)\n",
      "Mean epipolar error: 0.7132 pixels\n",
      "Max epipolar error: 2.9242 pixels\n",
      "Std. epipolar error: 0.6637 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 341 out of 342 (99.71%)\n",
      "Mean epipolar error: 0.5614 pixels\n",
      "Max epipolar error: 2.6252 pixels\n",
      "Std. epipolar error: 0.4733 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 751 out of 751 (100.00%)\n",
      "Mean epipolar error: 0.3455 pixels\n",
      "Max epipolar error: 1.6882 pixels\n",
      "Std. epipolar error: 0.2839 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 842 out of 842 (100.00%)\n",
      "Mean epipolar error: 0.4386 pixels\n",
      "Max epipolar error: 2.9787 pixels\n",
      "Std. epipolar error: 0.3807 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 657 out of 679 (96.76%)\n",
      "Mean epipolar error: 0.6787 pixels\n",
      "Max epipolar error: 2.9229 pixels\n",
      "Std. epipolar error: 0.5408 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 268 out of 270 (99.26%)\n",
      "Mean epipolar error: 0.7355 pixels\n",
      "Max epipolar error: 2.8124 pixels\n",
      "Std. epipolar error: 0.5790 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 207 out of 207 (100.00%)\n",
      "Mean epipolar error: 0.4278 pixels\n",
      "Max epipolar error: 1.6429 pixels\n",
      "Std. epipolar error: 0.3553 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 240 out of 240 (100.00%)\n",
      "Mean epipolar error: 0.4450 pixels\n",
      "Max epipolar error: 2.9798 pixels\n",
      "Std. epipolar error: 0.5720 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 361 out of 377 (95.76%)\n",
      "Mean epipolar error: 0.6844 pixels\n",
      "Max epipolar error: 2.9556 pixels\n",
      "Std. epipolar error: 0.6468 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 459 out of 497 (92.35%)\n",
      "Mean epipolar error: 0.7251 pixels\n",
      "Max epipolar error: 2.9656 pixels\n",
      "Std. epipolar error: 0.6816 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[-0.11597698 -6.1957449  -0.40348841]\n",
      " [ 6.28023358  0.10951484 -5.12803237]\n",
      " [ 0.66107003  4.261453   -0.19794726]]\n",
      "Essential matrix singular values: [8.17519212e+00 7.49105892e+00 8.90961821e-16]\n",
      "Pose Inliers: 489/489\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000007\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.0280470519717667e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 4.936628084711782\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[  0.14310773  14.46816909   1.33914987]\n",
      " [-15.18522388  -0.17520167   8.44683705]\n",
      " [ -1.63430892  -9.77844849  -0.04804812]]\n",
      "Essential matrix singular values: [1.77485042e+01 1.72161094e+01 2.52929389e-15]\n",
      "Pose Inliers: 94/342\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.795490337587987e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.8555462146768065\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 0.02057277  3.8938561   1.68604961]\n",
      " [-3.32371949  0.2009288  -8.53776787]\n",
      " [-1.2391438   8.53445835  0.36402599]]\n",
      "Essential matrix singular values: [9.59858597e+00 9.18468207e+00 4.70666363e-16]\n",
      "Pose Inliers: 751/751\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999996\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.922437343532012e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.0628835865161435\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[-8.91344697e-02 -1.25497203e+00  6.46171660e-01]\n",
      " [ 2.09301553e+00  5.13742611e-03 -1.16500677e+01]\n",
      " [-2.41543518e-01  1.12905019e+01 -5.63898825e-03]]\n",
      "Essential matrix singular values: [1.18553526e+01 1.13617639e+01 3.53315779e-16]\n",
      "Pose Inliers: 842/842\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999997\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.7071440641591486e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.309478430664953\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 0.0875007   6.43792436 -0.70097986]\n",
      " [-5.64203822  0.08331538 -6.80955779]\n",
      " [ 0.74018039  6.83753248  0.03549742]]\n",
      "Essential matrix singular values: [9.42276872e+00 8.86949381e+00 7.78127459e-16]\n",
      "Pose Inliers: 679/679\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.629903712137499e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 3.59777696605\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 1.44654956e-02 -1.21189641e+01 -1.00862865e+00]\n",
      " [ 1.38943099e+01 -1.81785639e-01 -1.75340861e+01]\n",
      " [ 9.29386105e-01  1.81958681e+01  3.15151684e-01]]\n",
      "Essential matrix singular values: [2.24431914e+01 2.18351038e+01 8.67753588e-15]\n",
      "Pose Inliers: 270/270\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000007\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.665679227241643e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.826339793901674\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ 4.53303566e+00  2.86382083e+01 -4.04199389e+01]\n",
      " [-7.00703323e+01  1.57163157e-01  3.93329153e+02]\n",
      " [ 3.75734867e+01 -4.16925140e+02  6.96860237e+00]]\n",
      "Essential matrix singular values: [4.19777507e+02 4.01454743e+02 2.60038499e-15]\n",
      "Pose Inliers: 207/207\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.059296771082122e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 6.217133953010074\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[   1.0306029    22.95908455   -3.44174108]\n",
      " [ -29.30882987    2.45843119  129.16125356]\n",
      " [  -1.2404033  -128.84890634   -0.68895114]]\n",
      "Essential matrix singular values: [1.33067136e+02 1.30326127e+02 1.28505059e-14]\n",
      "Pose Inliers: 240/240\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.500087405919671e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 3.4840833864328298\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[   1.00107771   51.77194665    4.45888656]\n",
      " [ -36.51223574    2.89625636 -144.72333929]\n",
      " [  -1.23981453  155.69534152   -3.44107748]]\n",
      "Essential matrix singular values: [1.64476590e+02 1.48961282e+02 2.58240178e-14]\n",
      "Pose Inliers: 377/377\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999994\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.776703733088792e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 4.70585090030906\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 469.2, cx = 360.0, cy = 636.0\n",
      "Essential Matrix\n",
      " [[ -0.13755214  12.48183883  -6.19143243]\n",
      " [-12.34991608   0.28086055  -4.75353342]\n",
      " [  5.60471573   4.63380921  -0.18487138]]\n",
      "Essential matrix singular values: [1.48696951e+01 1.41829411e+01 2.33665170e-15]\n",
      "Pose Inliers: 471/497\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999997\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.7396829095415635e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 1.612383871994091\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 489 points.\n",
      "Triangulated 342 points.\n",
      "Triangulated 751 points.\n",
      "Triangulated 842 points.\n",
      "Triangulated 679 points.\n",
      "Triangulated 270 points.\n",
      "Triangulated 207 points.\n",
      "Triangulated 240 points.\n",
      "Triangulated 377 points.\n",
      "Triangulated 497 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 489\n",
      "Reprojection error: 2.11 pixels\n",
      "Baseline: 0.674\n",
      "Scale ratio: 49.4x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 342\n",
      "Reprojection error: 3.47 pixels\n",
      "Baseline: 0.346\n",
      "Scale ratio: 11858.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 751\n",
      "Reprojection error: 3.45 pixels\n",
      "Baseline: 0.677\n",
      "Scale ratio: 49.4x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 842\n",
      "Reprojection error: 7.16 pixels\n",
      "Baseline: 1.046\n",
      "Scale ratio: 11.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 679\n",
      "Reprojection error: 5.06 pixels\n",
      "Baseline: 0.926\n",
      "Scale ratio: 45.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 270\n",
      "Reprojection error: 5.86 pixels\n",
      "Baseline: 0.970\n",
      "Scale ratio: 10.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 207\n",
      "Reprojection error: 1.91 pixels\n",
      "Baseline: 0.965\n",
      "Scale ratio: 10.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 240\n",
      "Reprojection error: 2.45 pixels\n",
      "Baseline: 1.070\n",
      "Scale ratio: 10.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 377\n",
      "Reprojection error: 7.27 pixels\n",
      "Baseline: 0.657\n",
      "Scale ratio: 16.3x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 497\n",
      "Reprojection error: 6.34 pixels\n",
      "Baseline: 0.643\n",
      "Scale ratio: 2284.1x\n",
      "\n",
      "HARRIS Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 4.51 pixels\n",
      "Average Scale Ratio: 1434.7x\n",
      "Total Reconstructed Points: 4694\n",
      "\n",
      "Total reconstructed points: 4694\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\HARRIS_reconstruction.ply\n"
     ]
    }
   ],
   "source": [
    "feature_matching_results_all_algos = {}\n",
    "\n",
    "results_benchmark_data = {\n",
    "    'frame_indices': list(range(len(Frames)))\n",
    "}\n",
    "\n",
    "detected_features_all = {}\n",
    "\n",
    "# Loop through each detector type and function pair to process feature matching\n",
    "for detector_type, detector_func in ALGORITHM_NAME_FUNC_MAP.items():\n",
    "    print(f\"\\nRunning:{detector_type}, func:{detector_func.__name__}\")\n",
    "\n",
    "    # 1. Detect Features\n",
    "    detected_features, keypoint_counts, computation_times = detect_features(Frames, detector_func)\n",
    "    results_benchmark_data[detector_type] = {\n",
    "        'keypoint_counts': keypoint_counts,\n",
    "        'computation_times': computation_times\n",
    "    }\n",
    "    detected_features_all[detector_type] = detected_features\n",
    "\n",
    "    # 2. Match Features\n",
    "    match_results = match_features(Frames, detector_type, detected_features)\n",
    "    if not match_results: raise RuntimeError(\"No valid matches found!\")\n",
    "    if PLOT_FEATURE_DETECTOR_PAIRS: display_feature_detector_pairs(Frames, match_results, detector_type)  \n",
    "    \n",
    "    # 3. RANSAC Filtering\n",
    "    ransac_results = [apply_ransac_filter(res, reproj_thresh=16) for res in match_results]\n",
    "    print(f\"Number of match results: {len(match_results)}\")\n",
    "    feature_matching_results_all_algos[detector_type] = {\n",
    "        \"matches\": match_results,\n",
    "        \"ransac\": ransac_results\n",
    "    }\n",
    "\n",
    "    # Visualize matches before and after RANSAC filtering\n",
    "    if PLOT_RANSAC_FILTERING:\n",
    "        for before, after in zip(match_results, ransac_results):\n",
    "                    i, j = before.frame_pair\n",
    "                    print(f\"\\nVisualising Matches: Frame Pair ({i}, {j})\")\n",
    "                    plot_visual_image_before_after_ransac(Frames[i], Frames[j], before, after, detector_type)\n",
    "    \n",
    "\n",
    "    # 4. Fundamental Matrix and Epipolar Geometry\n",
    "    print(\"\\nFundamental Matrix and Epipolar Geometry:\")\n",
    "    fundamental_results_list = []\n",
    "\n",
    "    # Process each RANSAC result to compute fundamental matrix\n",
    "    for res in ransac_results:\n",
    "        i, j = res.frame_pair\n",
    "        print(f\"\\nFundamental Matrix Computation for Frame Pair ({i}, {j})\")\n",
    "        imgs = [Frames[i], Frames[j]]\n",
    "        fundamental_result = process_fundamental_matrix(imgs, res.matches, res.kp1, res.kp2, visualise=PLOT_FUNDAMENTAL_MATRIX_COMPUTATION)\n",
    "        fundamental_results_list.append(fundamental_result)\n",
    "\n",
    "    # 5. Pose Estimation\n",
    "    print(\"\\nCamera Pose Estimation:\")\n",
    "    pose_estimations = []\n",
    "    frame_shape = (Frames[0].shape[0], Frames[0].shape[1])  # Get frame dimensions\n",
    "\n",
    "    # Process each fundamental matrix result and RANSAC result\n",
    "    for idx, (fundamental_data, res) in enumerate(zip(fundamental_results_list, ransac_results)):\n",
    "        print(f\"\\nPose Estimation for Frame Pair {res.frame_pair}\")\n",
    "        pose_result = run_pose_estimation_from_matches(\n",
    "            match_result = res, \n",
    "            frame_shape = frame_shape, \n",
    "            fundamental_matrix = fundamental_data['fundamental_matrix'],\n",
    "            visualise=PLOT_CAMERA_POSE_ESTIMATIONS\n",
    "        )\n",
    "        pose_estimations.append(pose_result)\n",
    "\n",
    "    # 6. 3D Point Triangulation and Scene Visualisation\n",
    "    print(\"\\n3D Point Triangulation and Scene Visualisation:\")\n",
    "    reconstruct_scene(pose_estimations, ransac_results, Frames, detector_name=detector_type, use_bundle_adjustment=USE_BUNDLE_ADJUSTMENT, visualize=VISUALIZE_3D_RECONTRUCTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f5f69",
   "metadata": {},
   "source": [
    "Generation of final analysis visualizations based on configured output flags. Creates side-by-side detector comparisons, performance metric plots, RANSAC filtering effectiveness and video montages of the feature detection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64f38d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_DETECTOR_BENCHMARK_COMPARISONS:\n",
    "    compare_detectors_per_frame_cached(Frames, detected_features_all)\n",
    "    plot_keypoint_counts_and_comp_time(results_benchmark_data)\n",
    "\n",
    "if PLOT_RANSAC_FILTERING:\n",
    "    plot_before_after_ransac_detected_features_all_algos(feature_matching_results_all_algos)\n",
    "\n",
    "if CREATE_FEATURE_MONTAGE_VIDEOS: \n",
    "    create_feature_detection_montage(Frames, detected_features_all, ALGORITHM_NAME_FUNC_MAP)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
