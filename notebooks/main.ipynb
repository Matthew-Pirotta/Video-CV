{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb680e4d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "27bcbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "import open3d as o3d # 3D scene visualization\n",
    "import plyfile # Export point cloud\n",
    "import os # File operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20093f9a",
   "metadata": {},
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7a273637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for matplotlib plots \n",
    "DETECTOR_COLORS_MPL = {\n",
    "    \"orb\": 'green',\n",
    "    \"sift\": 'blue',\n",
    "    \"fast\": 'red',\n",
    "    \"shi_tomasi\": 'cyan',\n",
    "    \"harris\": 'magenta'\n",
    "}\n",
    "\n",
    "DETECTOR_COLORS = {\n",
    "    \"ORB\": (0, 255, 0),          \n",
    "    \"SIFT\": (255, 0, 0),         \n",
    "    \"FAST\": (0, 0, 255),         \n",
    "    \"Shi-Tomasi\": (255, 255, 0), \n",
    "    \"Harris\": (255, 0, 255)      \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228a99b",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d57fa357",
   "metadata": {},
   "outputs": [],
   "source": [
    "MatchResult = namedtuple(\"MatchResult\", [\"frame_pair\", \"kp1\", \"kp2\", \"pts1\", \"pts2\", \"matches\"])\n",
    "Frame = np.ndarray\n",
    "\n",
    "def get_next_frame_index(frames, current_frame_index):\n",
    "    next_frame_index = current_frame_index + 1\n",
    "\n",
    "    if next_frame_index >= len(frames):\n",
    "        raise ValueError(\"Next frame index out of bounds\")\n",
    "    \n",
    "    return next_frame_index\n",
    "\n",
    "def convert_frames_to_grayscale(*frames):\n",
    "    gray_frames = []\n",
    "    for frame in frames:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frames.append(gray)\n",
    "    \n",
    "    return tuple(gray_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3903994",
   "metadata": {},
   "source": [
    "# 1. Video Acquisition  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716b0f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a61a469",
   "metadata": {},
   "source": [
    "# 2. Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "850110fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_indices(video_path:str, num_frames:int) -> list:\n",
    "    # Count total frames by iterating once\n",
    "    total_frames = sum(1 for _ in iio.imiter(video_path))\n",
    "\n",
    "    if num_frames > total_frames:\n",
    "        raise ValueError(f\"Requested {num_frames} frames, but video has only {total_frames} frames.\")\n",
    "\n",
    "    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "    return indices.tolist()\n",
    "\n",
    "def load_specific_frames(video_path: str, selected_indices: list, display_frames=True) -> list:\n",
    "    frames = []\n",
    "    selected_set = set(selected_indices)\n",
    "    collected = 0\n",
    "\n",
    "    for frame_count, frame in enumerate(iio.imiter(video_path)):\n",
    "        if frame_count in selected_set:\n",
    "            frames.append(frame)\n",
    "            collected += 1\n",
    "\n",
    "            if display_frames:\n",
    "                plt.imshow(frame)\n",
    "                plt.title(f'Frame {frame_count}')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "    print(f\"\\nFinished loading selected frames.\")\n",
    "    print(f\"Total frames processed: {frame_count + 1}\")\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d647f",
   "metadata": {},
   "source": [
    "# 3. Feature Detection and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955719a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dd36032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_orb(gray):\n",
    "    orb = cv2.ORB_create(nfeatures=1500) \n",
    "    keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_sift(gray):\n",
    "    sift = cv2.SIFT_create(nfeatures=1500, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6)\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_fast(gray):\n",
    "    fast = cv2.FastFeatureDetector_create(threshold=37, nonmaxSuppression=True)\n",
    "    keypoints = fast.detect(gray, None)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.compute(gray, keypoints)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_shi_tomasi(gray):\n",
    "    corners = cv2.goodFeaturesToTrack(gray, maxCorners=3000, qualityLevel=0.01, minDistance=10, blockSize=3)\n",
    "    keypoints = []\n",
    "    if corners is not None:\n",
    "        keypoints = [cv2.KeyPoint(float(x), float(y), 1) for [[x, y]] in corners]\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_harris(gray, block_size=2, ksize=3, k=0.04, threshold_ratio=0.01): \n",
    "    gray_f32 = np.float32(gray)\n",
    "    dst = cv2.cornerHarris(gray_f32, block_size, ksize, k)\n",
    "    dst = cv2.dilate(dst, None)\n",
    "    threshold = threshold_ratio * dst.max()\n",
    "    corners = np.argwhere(dst > threshold)\n",
    "    keypoints = [cv2.KeyPoint(float(pt[1]), float(pt[0]), 1) for pt in corners]\n",
    "\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70779144",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b24720f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_detector_pairs(frames, matches: list, detector_name: str):\n",
    "\n",
    "    color = DETECTOR_COLORS.get(detector_name, (0, 255, 0))\n",
    "    keypoint_draw_flags = (\n",
    "        cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS\n",
    "        if detector_name in [\"SIFT\"]\n",
    "        else cv2.DrawMatchesFlags_DEFAULT\n",
    "    )\n",
    "\n",
    "    for match_result in matches:\n",
    "        i, j = match_result.frame_pair\n",
    "\n",
    "        img1 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "        img2 = cv2.cvtColor(frames[j], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        img_kp1 = cv2.drawKeypoints(img1, match_result.kp1, None, color=color, flags=keypoint_draw_flags)\n",
    "        img_kp2 = cv2.drawKeypoints(img2, match_result.kp2, None, color=color, flags=keypoint_draw_flags)\n",
    "\n",
    "        # Show keypoints on individual images\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        axes[0].imshow(img_kp1, cmap='gray')\n",
    "        axes[0].set_title(f\"{detector_name} - Frame {i}, Keypoints: {len(match_result.kp1)}\")\n",
    "\n",
    "        axes[1].imshow(img_kp2, cmap='gray')\n",
    "        axes[1].set_title(f\"{detector_name} - Frame {j}, Keypoints: {len(match_result.kp2)}\")\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        plt.suptitle(f\"{detector_name} Keypoints: Frame {i} â†” Frame {j}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18681a9",
   "metadata": {},
   "source": [
    "## Combined Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e33e4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keypoints_over_frames(results):\n",
    "    indices = results['frame_indices']\n",
    "\n",
    "    # Keypoints\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, data in results.items():\n",
    "        if name != 'frame_indices':\n",
    "            color = DETECTOR_COLORS_MPL.get(name, 'green')\n",
    "            plt.plot(indices, data['keypoint_counts'], label=f'{name.upper()} Keypoints', color=color)\n",
    "    plt.title(\"Keypoints per Frame\")\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Number of Keypoints\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_computationTime_over_frames(results):\n",
    "    # Computation time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, data in results.items():\n",
    "        if name != 'frame_indices':\n",
    "            color = DETECTOR_COLORS_MPL.get(name, 'green')\n",
    "            plt.plot(indices, data['computation_times'], label=f'{name.upper()} Time', color=color)\n",
    "    plt.title(\"Computation Time per Frame\")\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_keypoints_and_computTimes(results):\n",
    "\n",
    "    plot_keypoints_over_frames(results)\n",
    "    plot_computationTime_over_frames(results)\n",
    "\n",
    "    # Print averages\n",
    "    for name, data in results.items():\n",
    "        if name != 'frame_indices':\n",
    "            print(f\"Average {name.upper()}: {np.mean(data['keypoint_counts']):.1f} keypoints, {np.mean(data['computation_times']):.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4973180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_detectors_per_frame(frames):\n",
    "    detectors = {\n",
    "        \"ORB\": detect_orb,\n",
    "        \"SIFT\": detect_sift,\n",
    "        \"FAST\": detect_fast,\n",
    "        \"Shi-Tomasi\": detect_shi_tomasi,\n",
    "        \"Harris\": detect_harris\n",
    "    }\n",
    "\n",
    "    for idx in range(len(frames)):\n",
    "        if idx < 0 or idx >= len(frames):\n",
    "            print(f\"Skipping invalid frame index {idx}\")\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(frames[idx], cv2.COLOR_BGR2GRAY)\n",
    "        frame_results = {}\n",
    "\n",
    "        for name, fn in detectors.items():\n",
    "            keypoints, _ = fn(gray)\n",
    "                \n",
    "            color = DETECTOR_COLORS.get(name, (0, 255, 0))  # default green if missing\n",
    "            img_with_kp = cv2.drawKeypoints(frames[idx], keypoints, None, color)\n",
    "            frame_results[name] = (img_with_kp, len(keypoints))\n",
    "\n",
    "        # Plot all detector results in 3x3 grid\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "\n",
    "        # Flatten axes for easy iteration\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Plot the 5 detector images\n",
    "        for ax, (name, (img, count)) in zip(axes, frame_results.items()):\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            ax.set_title(f\"{name} ({count})\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        # Hide the remaining axes (empty slots)\n",
    "        for ax in axes[len(frame_results):]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Detector Comparison - Frame {idx}\", fontsize=16)\n",
    "        plt.subplots_adjust(left=0.05, right=0.95, top=0.90, bottom=0.05, wspace=0.05, hspace=0.1)\n",
    "        plt.show()\n",
    "\n",
    "def calculate_repeatability(keypoints1, keypoints2, homography, threshold=3):\n",
    "    \"\"\"\n",
    "    Calculates the repeatability score between two sets of keypoints given a homography.\n",
    "\n",
    "    Args:\n",
    "        keypoints1 (list): List of cv2.KeyPoint objects from the first image.\n",
    "        keypoints2 (list): List of cv2.KeyPoint objects from the second image.\n",
    "        homography (np.ndarray): The 3x3 homography matrix transforming points from image 1 to image 2.\n",
    "        threshold (int): The maximum pixel distance to consider a keypoint as a match.\n",
    "\n",
    "    Returns:\n",
    "        float: The repeatability score.\n",
    "        int: The number of corresponding keypoints found.\n",
    "    \"\"\"\n",
    "    if not keypoints1 or not keypoints2 or homography is None:\n",
    "        return 0.0, 0\n",
    "\n",
    "    # Get coordinates from keypoint objects\n",
    "    points1 = np.float32([kp.pt for kp in keypoints1]).reshape(-1, 1, 2)\n",
    "    points2 = np.float32([kp.pt for kp in keypoints2])\n",
    "\n",
    "    # Project keypoints from image 1 to image 2\n",
    "    projected_points1 = cv2.perspectiveTransform(points1, homography)\n",
    "    projected_points1 = projected_points1.reshape(-1, 2)\n",
    "\n",
    "    correspondences = 0\n",
    "    for pt1 in projected_points1:\n",
    "        # Calculate the distance from the projected point to all keypoints in the second image\n",
    "        distances = np.linalg.norm(points2 - pt1, axis=1)\n",
    "        # Check if the minimum distance is within the threshold\n",
    "        if np.min(distances) < threshold:\n",
    "            correspondences += 1\n",
    "\n",
    "    # Repeatability is the ratio of correspondences to the minimum number of keypoints\n",
    "    repeatability_score = correspondences / min(len(keypoints1), len(keypoints2))\n",
    "    return repeatability_score, correspondences\n",
    "\n",
    "def benchmark_detectors_with_repeatability(frames):\n",
    "    \"\"\"\n",
    "    Benchmarks detectors for keypoint count, computation time, and repeatability.\n",
    "    \"\"\"\n",
    "    detectors = {\n",
    "        \"ORB\": detect_orb,\n",
    "        \"SIFT\": detect_sift,\n",
    "        \"FAST\": detect_fast,\n",
    "        \"Shi-Tomasi\": detect_shi_tomasi,\n",
    "        \"Harris\": detect_harris\n",
    "    }\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    results = {name: {'keypoint_counts': [], 'computation_times': [], 'repeatability': []} for name in detectors}\n",
    "    results['frame_indices'] = list(range(len(frames)))\n",
    "\n",
    "    # --- Part 1: Gather per-frame stats (as in your original code) ---\n",
    "    for i, frame in enumerate(frames):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        for name, detector_fn in detectors.items():\n",
    "            start = time.time()\n",
    "            keypoints, _ = detector_fn(gray)\n",
    "            elapsed = time.time() - start\n",
    "            results[name]['keypoint_counts'].append(len(keypoints))\n",
    "            results[name]['computation_times'].append(elapsed)\n",
    "\n",
    "    # --- Part 2: Calculate repeatability between consecutive frames ---\n",
    "    # We need a robust detector like SIFT or ORB to find the homography itself\n",
    "    sift = cv2.SIFT_create()\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2)\n",
    "\n",
    "    for i in range(len(frames) - 1):\n",
    "        frame1 = frames[i]\n",
    "        frame2 = frames[i+1]\n",
    "        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find a \"ground-truth\" homography between the frames using a reliable method\n",
    "        kp1_h, des1_h = sift.detectAndCompute(gray1, None)\n",
    "        kp2_h, des2_h = sift.detectAndCompute(gray2, None)\n",
    "        \n",
    "        homography = None\n",
    "        if des1_h is not None and des2_h is not None and len(des1_h) > 10 and len(des2_h) > 10:\n",
    "            matches = bf.knnMatch(des1_h, des2_h, k=2)\n",
    "            good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "            \n",
    "            if len(good_matches) > 10: # Minimum matches to find homography\n",
    "                src_pts = np.float32([kp1_h[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "                dst_pts = np.float32([kp2_h[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "                homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Now, calculate repeatability for each of our target detectors\n",
    "        for name, detector_fn in detectors.items():\n",
    "            # Detect keypoints for each frame with the current detector\n",
    "            keypoints1, _ = detector_fn(gray1)\n",
    "            keypoints2, _ = detector_fn(gray2)\n",
    "\n",
    "            score, _ = calculate_repeatability(keypoints1, keypoints2, homography)\n",
    "            # We append the score for the first frame of the pair\n",
    "            results[name]['repeatability'].append(score)\n",
    "\n",
    "    # Since repeatability is calculated between pairs, the last frame has no score.\n",
    "    # We add a placeholder (e.g., 0 or NaN) for consistency in list lengths.\n",
    "    for name in detectors:\n",
    "        results[name]['repeatability'].append(0.0) \n",
    "\n",
    "    # --- Part 3: Plot the results (you can create a new plot function for this) ---\n",
    "    # plot_keypoints_and_computTimes(results) # Your existing function\n",
    "    plot_all_benchmarks(results) # A new comprehensive plot function\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_all_benchmarks(results):\n",
    "    \"\"\"Plots keypoint counts, computation times, and repeatability.\"\"\"\n",
    "    detector_names = list(results.keys())\n",
    "    detector_names.remove('frame_indices')\n",
    "    frame_indices = results['frame_indices']\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 18), sharex=True)\n",
    "    \n",
    "    # Plot Keypoint Counts\n",
    "    for name in detector_names:\n",
    "        ax1.plot(frame_indices, results[name]['keypoint_counts'], linestyle='-', label=name)\n",
    "    ax1.set_ylabel('Number of Keypoints')\n",
    "    ax1.set_title('Keypoint Detection Count per Frame')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot Computation Times\n",
    "    for name in detector_names:\n",
    "        ax2.plot(frame_indices, results[name]['computation_times'], linestyle='-', label=name)\n",
    "    ax2.set_ylabel('Computation Time (seconds)')\n",
    "    ax2.set_title('Computation Time per Frame')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot Repeatability\n",
    "    for name in detector_names:\n",
    "        # We plot up to the second to last frame, as the last has no subsequent pair\n",
    "        ax3.plot(frame_indices[:-1], results[name]['repeatability'][:-1], linestyle='-', label=name)\n",
    "    ax3.set_xlabel('Frame Index')\n",
    "    ax3.set_ylabel('Repeatability Score')\n",
    "    ax3.set_title('Repeatability Score between Consecutive Frames')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63df05",
   "metadata": {},
   "source": [
    "# 4. Feature Matching and Outlier Rejection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "460af133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matcher(detector_type: str):\n",
    "    detector_type = detector_type.lower()\n",
    "    if detector_type in ['sift', 'surf', 'fast']:\n",
    "        # FLANN for float descriptors\n",
    "        index_params = dict(algorithm=1, trees=5)  # KDTree\n",
    "        search_params = dict(checks=50)\n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        descriptor_type = 'float'\n",
    "    elif detector_type in ['orb', 'brief', 'shi-tomasi', 'harris']:\n",
    "        # Brute-force for binary descriptors\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "        descriptor_type = 'binary'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported detector type: {detector_type}\")\n",
    "    \n",
    "    return matcher, descriptor_type\n",
    "\n",
    "def match_features(frames, detector_type, detect_func, ratio_thresh=0.75):\n",
    "    matcher, descriptor_type = get_matcher(detector_type)\n",
    "    match_results = []\n",
    "\n",
    "    for start_idx in range(len(frames)-1):\n",
    "        end_idx = start_idx + 1\n",
    "\n",
    "        if start_idx >= len(frames) or end_idx >= len(frames):\n",
    "            continue\n",
    "\n",
    "        gray1, gray2 = convert_frames_to_grayscale(frames[start_idx], frames[end_idx])\n",
    "        \n",
    "\n",
    "        kp1, des1 = detect_func(gray1)\n",
    "        kp2, des2 = detect_func(gray2)\n",
    "\n",
    "\n",
    "        if des1 is None or des2 is None:\n",
    "            continue\n",
    "        \n",
    "        if descriptor_type == 'float':\n",
    "            des1 = des1.astype(np.float32)\n",
    "            des2 = des2.astype(np.float32)\n",
    "\n",
    "        knn_matches = matcher.knnMatch(des1, des2, k=2)\n",
    "        \n",
    "        good_matches = []\n",
    "        for match_pair in knn_matches:\n",
    "            if len(match_pair) == 2:\n",
    "                m, n = match_pair\n",
    "                if m.distance < ratio_thresh * n.distance:\n",
    "                    good_matches.append(m)\n",
    "\n",
    "        if len(good_matches) < 10:\n",
    "            continue\n",
    "\n",
    "        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
    "\n",
    "        result = MatchResult(\n",
    "            frame_pair=(start_idx, end_idx),\n",
    "            kp1=kp1,\n",
    "            kp2=kp2,\n",
    "            pts1=pts1,\n",
    "            pts2=pts2,\n",
    "            matches=good_matches\n",
    "        )\n",
    "\n",
    "        match_results.append(result)\n",
    "        print(f\"[{detector_type.upper()}] Matched {start_idx} â†’ {end_idx} : {len(good_matches)} matches\")\n",
    "\n",
    "    return match_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "860deb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANSAC removes outlier matches to improve geometric consistency\n",
    "# Higher threshold = more lenient (accepts points further from predicted position)\n",
    "def apply_ransac_filter(match_result:MatchResult, reproj_thresh=4.0)-> MatchResult:\n",
    "    H, mask = cv2.findHomography(match_result.pts1, match_result.pts2, cv2.RANSAC, reproj_thresh)\n",
    "    mask = mask.ravel()\n",
    "\n",
    "    inlier_matches = [m for m, keep in zip(match_result.matches, mask) if keep]\n",
    "    pts1_inliers = match_result.pts1[mask == 1]\n",
    "    pts2_inliers = match_result.pts2[mask == 1]\n",
    "\n",
    "    return MatchResult(\n",
    "        frame_pair=match_result.frame_pair,\n",
    "        kp1=match_result.kp1,\n",
    "        kp2=match_result.kp2,\n",
    "        pts1=pts1_inliers,\n",
    "        pts2=pts2_inliers,\n",
    "        matches=inlier_matches\n",
    "    )\n",
    "\n",
    "def plot_before_after_ransac(img1:Frame, img2:Frame, match_before: MatchResult, match_after: MatchResult) -> None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "    # Before RANSAC\n",
    "    img_before = cv2.drawMatches(\n",
    "        img1, match_before.kp1,\n",
    "        img2, match_before.kp2,\n",
    "        match_before.matches,\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_DEFAULT\n",
    "    )\n",
    "    axes[0].imshow(img_before)\n",
    "    axes[0].set_title(f\"Before RANSAC - Matches: {len(match_before.matches)}\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # After RANSAC\n",
    "    img_after = cv2.drawMatches(\n",
    "        img1, match_after.kp1,\n",
    "        img2, match_after.kp2,\n",
    "        match_after.matches,\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_DEFAULT\n",
    "    )\n",
    "    axes[1].imshow(img_after)\n",
    "    axes[1].set_title(f\"After RANSAC - Inliers: {len(match_after.matches)}\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    fig.suptitle(f\"Frames:{match_before.frame_pair}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213ae0e",
   "metadata": {},
   "source": [
    "# 5. Essential/Fundamental Matrix Computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d853e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Compute the fundamental matrix from matches keypoints\n",
    "def compute_fundamental_matrix(matches, keypoints1, keypoints2, method = cv2.FM_RANSAC, ransac_threshold = 3.0, confidence = 0.99):\n",
    "  # Extract coordinates of matches keypoints\n",
    "  points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])\n",
    "  points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])\n",
    "  \n",
    "  # Compute the fundamental matrix\n",
    "  fundamental_matrix, inlier_mask = cv2.findFundamentalMat(points1, points2, method = method, ransacReprojThreshold = ransac_threshold, confidence = confidence)\n",
    "  \n",
    "  #Â Convert mask to binary array for easier filtering\n",
    "  if fundamental_matrix is None or fundamental_matrix.shape != (3,3):\n",
    "    raise ValueError(\"Failed to compute a valid fundamental matrix\")\n",
    "  \n",
    "  return fundamental_matrix, inlier_mask\n",
    "\n",
    "#Â Clip epipolar line to image boundaries\n",
    "def draw_epipolar_line_clipped(img_shape, line):\n",
    "  h, w = img_shape[:2]\n",
    "    \n",
    "  # Find intersections with image borders\n",
    "  points = []\n",
    "    \n",
    "  # Left border (x = 0)\n",
    "  if abs(line[1]) > 1e-6:\n",
    "    y = -line[2] / line[1]\n",
    "    if 0 <= y <= h:\n",
    "      points.append((0, int(y)))\n",
    "    \n",
    "  # Right border (x = w-1)\n",
    "  if abs(line[1]) > 1e-6:\n",
    "    y = -(line[2] + line[0] * (w-1)) / line[1]\n",
    "    if 0 <= y <= h:\n",
    "      points.append((w-1, int(y)))\n",
    "    \n",
    "  # Top border (y = 0)\n",
    "  if abs(line[0]) > 1e-6:\n",
    "    x = -line[2] / line[0]\n",
    "    if 0 <= x <= w:\n",
    "      points.append((int(x), 0))\n",
    "    \n",
    "  # Bottom border (y = h-1)\n",
    "  if abs(line[0]) > 1e-6:\n",
    "    x = -(line[2] + line[1] * (h-1)) / line[0]\n",
    "    if 0 <= x <= w:\n",
    "      points.append((int(x), h-1))\n",
    "    \n",
    "  # Remove duplicate points and return first two\n",
    "  unique_points = []\n",
    "  for p in points:\n",
    "    if not any(abs(p[0]-up[0]) < 2 and abs(p[1]-up[1]) < 2 for up in unique_points):\n",
    "      unique_points.append(p)\n",
    "    \n",
    "  return unique_points[:2] if len(unique_points) >= 2 else []\n",
    "\n",
    "# Visualise epipolar lines to verify the fundamental matrix\n",
    "def visualise_epipolar_lines(img1, img2, points1, points2, fundamental_matrix, sample_size = 20):\n",
    "  # Sample points if there are too many\n",
    "  if len(points1) > sample_size:\n",
    "    indices = np.random.choice(len(points1), sample_size, replace = False)\n",
    "    pts1 = points1[indices]\n",
    "    pts2 = points2[indices]\n",
    "  else:\n",
    "    pts1 = points1\n",
    "    pts2 = points2\n",
    "\n",
    "  # Create a new figure for each call\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 8))\n",
    "\n",
    "  # Convert images to RGB for matplotlib\n",
    "  img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "  img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  # Display the first image\n",
    "  ax1.imshow(img1_rgb) # Use RGB image\n",
    "  ax1.set_title('Epipolar Lines on Image 1')\n",
    "  ax1.axis('off')\n",
    "\n",
    "  # Display the second image\n",
    "  ax2.imshow(img2_rgb) # Use RGB image\n",
    "  ax2.set_title('Epipolar Lines on Image 2')\n",
    "  ax2.axis('off')\n",
    "\n",
    "  # Draw epipolar lines on both images\n",
    "  for i in range(len(pts1)):\n",
    "    # Draw points\n",
    "    ax1.plot(pts1[i, 0], pts1[i, 1], 'ro', markersize = 6)\n",
    "    ax2.plot(pts2[i, 0], pts2[i, 1], 'ro', markersize = 6)\n",
    "\n",
    "    # Compute epipolar line in second image for point in first image\n",
    "    line2 = cv2.computeCorrespondEpilines(pts1[i].reshape(-1, 1, 2), 1, fundamental_matrix)\n",
    "    line2 = line2.reshape(-1)\n",
    "\n",
    "    # Get clipped line endpoints for image 2\n",
    "    endpoints2 = draw_epipolar_line_clipped(img2.shape, line2)\n",
    "    if len(endpoints2) == 2:\n",
    "        ax2.plot([endpoints2[0][0], endpoints2[1][0]], [endpoints2[0][1], endpoints2[1][1]], 'g-', linewidth=1)\n",
    "\n",
    "    # Compute epipolar line in first image for point in second image\n",
    "    line1 = cv2.computeCorrespondEpilines(pts2[i].reshape(-1, 1, 2), 2, fundamental_matrix)\n",
    "    line1 = line1.reshape(-1)\n",
    "\n",
    "    # Get clipped line endpoints for image 1\n",
    "    endpoints1 = draw_epipolar_line_clipped(img1.shape, line1)\n",
    "    if len(endpoints1) == 2:\n",
    "        ax1.plot([endpoints1[0][0], endpoints1[1][0]], [endpoints1[0][1], endpoints1[1][1]], 'g-', linewidth=1)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  plt.close(fig)\n",
    "\n",
    "# Calculate the epipolar geometry error to evaluate the quality of the fundamental matrix\n",
    "'''\n",
    "Tips on Evaluating Epipolar Error:\n",
    "\n",
    "- Ideal: As close to 0 as possible (perfect geometry).\n",
    "- Practical: For real-world data, a mean epipolar error of less than 1 pixel is considered very good. Values up to 2-3\n",
    "pixels are often acceptable, depending on image resolution, noise, and feature localization accuracy.\n",
    "- If error > 5 pixels: This usually indicates problems with matching, calibration, or outliers.\n",
    "'''\n",
    "def epipolar_error(points1, points2, fundamental_matrix):\n",
    "  # Convert each point to homogeneous coordinates\n",
    "  homogeneous_points1 = np.hstack((points1, np.ones((points1.shape[0],1))))\n",
    "  homogeneous_points2 = np.hstack((points2, np.ones((points2.shape[0],1))))\n",
    "  \n",
    "  # Calculate epipolar lines for points in image 1\n",
    "  lines2 = np.dot(homogeneous_points1, fundamental_matrix.T)\n",
    "  # Normalise lines\n",
    "  norms2 = np.sqrt(lines2[:, 0]**2 + lines2[:, 1]**2)\n",
    "  lines2 = lines2 / norms2.reshape(-1,1)\n",
    "  # Calculate the distance from points in image 2 to their corresponding epipolar lines\n",
    "  dist2 = np.abs(np.sum(lines2 * homogeneous_points2, axis = 1))\n",
    "  \n",
    "  # Calculate epipolar lines for points in image 2\n",
    "  lines1 = np.dot(homogeneous_points2, fundamental_matrix)\n",
    "  # Normalise lines\n",
    "  norms1 = np.sqrt(lines1[:, 0]**2 + lines1[:, 1]**2)\n",
    "  lines1 = lines1 / norms1.reshape(-1,1)\n",
    "  # Calculate the distance from points in image 1 to their corresponding epipolar lines\n",
    "  dist1 = np.abs(np.sum(lines1 * homogeneous_points1, axis = 1))\n",
    "  \n",
    "  metrics = {\n",
    "    \"mean_error\": (np.mean(dist1) + np.mean(dist2)) / 2,\n",
    "    \"max_error\": max(np.max(dist1), np.max(dist2)),\n",
    "    \"std_error\": (np.std(dist1) + np.std(dist2)) / 2\n",
    "  }\n",
    "  \n",
    "  return metrics\n",
    "\n",
    "# Main function to process the fundamental computation step\n",
    "def process_fundamental_matrix(imgs, matches, keypoints1, keypoints2, visualise = True):\n",
    "  # Compute fundamental matrix\n",
    "  print(\"Computing fundamental matrix...\")\n",
    "  F, inlier_mask = compute_fundamental_matrix(matches, keypoints1, keypoints2)\n",
    "    \n",
    "  # Filter matches based on inlier mask\n",
    "  inlier_matches = [m for i, m in enumerate(matches) if inlier_mask[i]]\n",
    "  print(f\"Inlier matches: {len(inlier_matches)} out of {len(matches)} ({len(inlier_matches) / len(matches) * 100:.2f}%)\")\n",
    "    \n",
    "  # Extract coordinates of inlier keypoints\n",
    "  inlier_points1 = np.float32([keypoints1[m.queryIdx].pt for m in inlier_matches])\n",
    "  inlier_points2 = np.float32([keypoints2[m.trainIdx].pt for m in inlier_matches])\n",
    "    \n",
    "  # Calculate error metrics\n",
    "  error_metrics = epipolar_error(inlier_points1, inlier_points2, F)\n",
    "  print(f\"Mean epipolar error: {error_metrics['mean_error']:.4f} pixels\")\n",
    "        \n",
    "  # Visualise epipolar lines if requested\n",
    "  if visualise and len(imgs) >= 2:\n",
    "      visualise_epipolar_lines(imgs[0], imgs[1], inlier_points1, inlier_points2, F)\n",
    "    \n",
    "  # Prepare results\n",
    "  results = {\n",
    "      \"fundamental_matrix\": F,\n",
    "      \"inlier_mask\": inlier_mask,\n",
    "      \"inlier_matches\": inlier_matches,\n",
    "      \"inlier_points1\": inlier_points1,\n",
    "      \"inlier_points2\": inlier_points2,\n",
    "      \"error_metrics\": error_metrics,\n",
    "  }\n",
    "    \n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1123511",
   "metadata": {},
   "source": [
    "# 6. Camera Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0843083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CameraIntrinsics:\n",
    "  fx: float\n",
    "  fy: float\n",
    "  cx: float\n",
    "  cy: float\n",
    "  \n",
    "  def matrix(self):\n",
    "    return np.array([\n",
    "      [self.fx, 0, self.cx],\n",
    "      [0, self.fy, self.cy],\n",
    "      [0, 0, 1]\n",
    "    ],dtype = np.float64)\n",
    "    \n",
    "# Estimate approximate intrinsics for a phone camera\n",
    "def estimate_camera_intrinsics(image_shape):\n",
    "  height, width = image_shape\n",
    "  \n",
    "  fov_degrees = 75\n",
    "  fov_radians = np.radians(fov_degrees)\n",
    "  f = (width / 2.0) / np.tan(fov_radians / 2.0)\n",
    "  \n",
    "  # Principal point at image center\n",
    "  cx = width / 2\n",
    "  cy = height / 2\n",
    "  \n",
    "  print(f\"Intrinsics fx = fy = {f:.1f}, cx = {cx:.1f}, cy = {cy:.1f}\")\n",
    "  return CameraIntrinsics(f, f, cx, cy)\n",
    "\n",
    "# Compute the essential matrix from the fundamental matrix using the formula:\n",
    "# E = K^T * F * K\n",
    "def convert_fundamental_to_essential_matrix(F, K):\n",
    "  E = K.T @ F @ K\n",
    "  return E\n",
    "\n",
    "# Recover relative camera pose (R, t) from the essential matrix\n",
    "def recover_camera_pose(E, pts1, pts2, K):\n",
    "  pts1 = np.asarray(pts1, dtype = np.float32)\n",
    "  pts2 = np.asarray(pts2, dtype = np.float32)\n",
    "  \n",
    "  # Make sure points have correct shape\n",
    "  if pts1.ndim == 1:\n",
    "    pts1 = pts1.reshape(-1,2)\n",
    "  if pts2.ndim == 1:\n",
    "    pts2 = pts2.reshape(-1,2)\n",
    "  \n",
    "  try:\n",
    "    # Chirality problem - Funtion automatically tests all 4 possible combinations of R and t\n",
    "    inliers, R, t, mask = cv2.recoverPose(E, pts1, pts2, K)\n",
    "    print(f\"Pose Inliers: {inliers}/{len(pts1)}\")\n",
    "    \n",
    "    if inliers < 5:\n",
    "      print(\"Very few inliers for pose recovery\")\n",
    "    \n",
    "    return R, t, mask\n",
    "  except cv2.error as e:\n",
    "    print(f\"OpenCV error in pose recovery: {e}\")\n",
    "     # Return identity transformation as fallback\n",
    "    return np.eye(3), np.zeros((3, 1)), np.zeros(len(pts1), dtype = np.uint8)\n",
    "\n",
    "# Validity checks on the recovered pose\n",
    "def validate_pose(R, t):\n",
    "  metrics = {}\n",
    "  det_R = np.linalg.det(R)\n",
    "  metrics['det_R'] = det_R\n",
    "  metrics['is_rotation_valid'] = np.abs(det_R - 1.0) < 1e-6\n",
    "\n",
    "  orthogonality_error = np.linalg.norm(R @ R.T - np.eye(3))\n",
    "  metrics['orthogonality_error'] = orthogonality_error\n",
    "  metrics['is_orthogonal'] = orthogonality_error < 1e-6\n",
    "\n",
    "  t_mag = np.linalg.norm(t)\n",
    "  metrics['translation_magnitude'] = t_mag\n",
    "\n",
    "  trace = np.trace(R)\n",
    "  trace = np.clip(trace, -1.0, 3.0)\n",
    "  angle_rad = np.arccos((trace - 1) / 2)\n",
    "  metrics['rotation_angle_deg'] = np.degrees(angle_rad)\n",
    "\n",
    "  return metrics\n",
    "\n",
    "# Simple 3D plot of the initial and recovered camera pose\n",
    "def visualise_camera_poses(R, t):\n",
    "  try:\n",
    "    fig = plt.figure(figsize = (10,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "    cam1 = np.array([0, 0, 0])\n",
    "    cam2 = -R.T @ t.flatten()  # Camera center in world coordinates\n",
    "    \n",
    "    if cam2.ndim > 1:\n",
    "      cam2 = cam2.flatten()\n",
    "    \n",
    "    ax.scatter(cam1[0], cam1[1], cam1[2], c = 'blue', s = 50, label = 'Camera 1 (origin)')\n",
    "    ax.scatter(cam2[0], cam2[1], cam2[2], c = 'red', s = 50, label = 'Camera 2')\n",
    "    ax.plot([cam1[0], cam2[0]], [cam1[1], cam2[1]], [cam1[2], cam2[2]], 'k--', linewidth = 2, label = 'Baseline')\n",
    "\n",
    "    # Add coordinate axes for camera orientations\n",
    "    axes_length = 0.3\n",
    "        \n",
    "    # Camera 1 axes (identity rotation)\n",
    "    ax.plot([0, axes_length], [0, 0], [0, 0], 'r-', alpha = 0.7, linewidth = 2)  # X-axis\n",
    "    ax.plot([0, 0], [0, axes_length], [0, 0], 'g-', alpha = 0.7, linewidth = 2)  # Y-axis\n",
    "    ax.plot([0, 0], [0, 0], [0, axes_length], 'b-', alpha = 0.7, linewidth = 2)  # Z-axis\n",
    "        \n",
    "    # Camera 2 axes (rotated)\n",
    "    axes = axes_length * R.T  # Camera coordinate axes in world frame\n",
    "    origin = cam2\n",
    "        \n",
    "    x_end = origin + axes[:, 0]\n",
    "    y_end = origin + axes[:, 1] \n",
    "    z_end = origin + axes[:, 2]\n",
    "        \n",
    "    ax.plot([origin[0], x_end[0]], [origin[1], x_end[1]], [origin[2], x_end[2]], 'r-', alpha = 0.7, linewidth = 2)\n",
    "    ax.plot([origin[0], y_end[0]], [origin[1], y_end[1]], [origin[2], y_end[2]], 'g-', alpha = 0.7, linewidth = 2)\n",
    "    ax.plot([origin[0], z_end[0]], [origin[1], z_end[1]], [origin[2], z_end[2]], 'b-', alpha = 0.7, linewidth = 2)\n",
    "\n",
    "    ax.set_xlabel('X') # red line\n",
    "    ax.set_ylabel('Y') # green line\n",
    "    ax.set_zlabel('Z') # blue line\n",
    "    ax.set_title('Camera Poses')\n",
    "    ax.legend()\n",
    "        \n",
    "    # Set equal aspect ratio\n",
    "    max_range = max(np.abs(cam2).max(), 0.5)\n",
    "    ax.set_xlim([-max_range, max_range])\n",
    "    ax.set_ylim([-max_range, max_range])\n",
    "    ax.set_zlim([-max_range, max_range])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  except Exception as e:\n",
    "    print(f\"Visualisation error: {e}\")\n",
    "    print(\"Skipping visualisation process\")\n",
    "\n",
    "# Takes in fundamental matrix and inlier points computed from stage 5\n",
    "def run_pose_estimation(F, pts1, pts2, frame_shape, visualise = True):\n",
    "  print(\"Camera Pose Estimation: \")\n",
    "  \n",
    "  if F is None or F.shape != (3,3):\n",
    "    raise ValueError(\"Invalid fundamental matrix\")\n",
    "  \n",
    "  if len(pts1) < 5 or len(pts2) < 5:\n",
    "    raise ValueError(\"Needs at least 5 point correspondences\")\n",
    "  \n",
    "  try:\n",
    "    # Estimate the intrinsics\n",
    "    intrinsics = estimate_camera_intrinsics(frame_shape)\n",
    "    K = intrinsics.matrix()\n",
    "    \n",
    "    # Deduce the essential matrix\n",
    "    E = convert_fundamental_to_essential_matrix(F, K)\n",
    "    print(\"Essential Matrix\\n\", E)\n",
    "    \n",
    "    # Check if essential matrix is valid using SVD\n",
    "    U, S, Vt = np.linalg.svd(E)\n",
    "    print(f\"Essential matrix singular values: {S}\")\n",
    "    \n",
    "    # Recover pose\n",
    "    R, t, mask = recover_camera_pose(E, pts1, pts2, K)\n",
    "    \n",
    "    # Validation step\n",
    "    pose_metrics = validate_pose(R, t)\n",
    "    print(\"\\nPose Validation: \")\n",
    "    for k, v in pose_metrics.items():\n",
    "      print(f\"{k}: {v}\")\n",
    "      \n",
    "    if not pose_metrics['is_rotation_valid']:\n",
    "      print(\"Invalid rotation matrix\")\n",
    "      \n",
    "    if pose_metrics['rotation_angle_deg'] > 90:\n",
    "      print(f\"Large rotation angle {pose_metrics['rotation_angle_deg']:.1f}\")\n",
    "    \n",
    "    # Check for degenerate \n",
    "    if pose_metrics['rotation_angle_deg'] < 0.1:\n",
    "      print(\"Very small rotation - possible degenerate case\")\n",
    "    \n",
    "    # Visualise the initial and recovered camera pose\n",
    "    if visualise:\n",
    "      visualise_camera_poses(R, t)\n",
    "      \n",
    "    return {\n",
    "      \"R\": R,\n",
    "      \"t\": t,\n",
    "      \"K\": K,\n",
    "      \"E\": E,\n",
    "      \"mask\": mask,\n",
    "      \"metrics\": pose_metrics\n",
    "    }\n",
    "  except Exception as e:\n",
    "    print(f\"Error in pose estimation: {e}\")\n",
    "    return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ef9c9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to integrate stages 4-6 together\n",
    "def run_pose_estimation_from_matches(match_result, frame_shape, fundamental_matrix, visualise):\n",
    "  return run_pose_estimation(F = fundamental_matrix, pts1 = match_result.pts1, pts2 = match_result.pts2, frame_shape = frame_shape, visualise=visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a4d20",
   "metadata": {},
   "source": [
    "# 7. 3D Point Triangulation and Scene Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "954360ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_points(K, camera1_pose, camera2_pose, points1, points2):\n",
    "    # Triangulate points in 3D space from 2D correspondences and camera parameters\n",
    "\n",
    "    # Input validation\n",
    "    if points1.shape[0] != points2.shape[0]:\n",
    "        print(\"Error: Input point arrays must have the same number of points.\")\n",
    "        return np.array([])\n",
    "\n",
    "    # Deconstruct poses\n",
    "    R1, t1 = camera1_pose\n",
    "    R2, t2 = camera2_pose\n",
    "\n",
    "    # Compute projection matrices P = K[R|t]\n",
    "    P1 = K @ np.hstack((R1, t1))\n",
    "    P2 = K @ np.hstack((R2, t2))\n",
    "        \n",
    "    # Triangulate points using OpenCV\n",
    "    # cv2.triangulatePoints requires points in (2, N) format, so we transpose our (N, 2) inputs\n",
    "    points_4d_hom = cv2.triangulatePoints(P1, P2, points1.T, points2.T)\n",
    "    \n",
    "    # Convert homogeneous coordinates (x,y,z,w) to Euclidean coordinates (x,y,z)\n",
    "    points_3d = (points_4d_hom[:3, :] / points_4d_hom[3, :]).T\n",
    "    \n",
    "    return points_3d\n",
    "\n",
    "def get_pixel_colors(img, coordinate_list):\n",
    "    # Initialize empty array for color values\n",
    "    pixel_values = []\n",
    "    image_height, image_width = img.shape[:2]\n",
    "    \n",
    "    # Process each coordinate\n",
    "    for coord in coordinate_list:\n",
    "        col, row = map(lambda x: int(x + 0.5), coord)  # Alternative rounding approach\n",
    "        \n",
    "        # Validate coordinates\n",
    "        is_valid = (0 <= col < image_width) and (0 <= row < image_height)\n",
    "        \n",
    "        if is_valid:\n",
    "            # Normalize RGB values to [0,1] range\n",
    "            rgb = img[row, col].astype(float) / 255\n",
    "        else:\n",
    "            rgb = np.array([0.5, 0.5, 0.5])  # Gray color for invalid points\n",
    "            \n",
    "        pixel_values.append(rgb)\n",
    "    \n",
    "    return np.array(pixel_values)\n",
    "\n",
    "def visualize_reconstruction(points_3d, colors, window_name):\n",
    "    # Create a point cloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    # Flip Y and Z coordinates for correct orientation\n",
    "    points_3d = points_3d.copy()\n",
    "    points_3d[:, 1] = -points_3d[:, 1] # Flip Y coordinate\n",
    "    points_3d[:, 2] = -points_3d[:, 2] # Flip Z coordinate\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(points_3d)\n",
    "    # Add RGB colors to the points\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    print(\"\\nVisualizing point cloud.\")\n",
    "    print(\"Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\")\n",
    "    print(\"Press 'q' to close the window.\")\n",
    "\n",
    "    # Display the 3D point cloud\n",
    "    o3d.visualization.draw_geometries([pcd], window_name=window_name)\n",
    "\n",
    "def bundle_adjust_poses(camera_poses, all_points_3d, all_matches, K, max_iterations=10):\n",
    "    # Iterative bundle adjustment to refine camera poses and 3D points.\n",
    "    print(f\"Starting bundle adjustment with {len(camera_poses)} poses...\")\n",
    "    \n",
    "    # Return early if no 3D points exist\n",
    "    if not all_points_3d:\n",
    "        print(\"No 3D points provided for bundle adjustment!\")\n",
    "        return camera_poses, []\n",
    "    \n",
    "    # Create copies to avoid modifying inputs\n",
    "    refined_poses = camera_poses.copy()\n",
    "    refined_points_3d = [points.copy() for points in all_points_3d]  # Keep as list of arrays\n",
    "    \n",
    "    # Iteratively refine poses up to max_iterations times\n",
    "    for iter_idx in range(max_iterations):\n",
    "        total_reproj_error = 0\n",
    "        pose_updates = 0\n",
    "        \n",
    "        # Iterate through each camera pose after first reference camera\n",
    "        for cam_idx in range(1, len(camera_poses)):\n",
    "            # Skip if we don't have matching data for this camera\n",
    "            if cam_idx - 1 >= len(all_matches):\n",
    "                continue\n",
    "                \n",
    "            # Get corresponding 2D-3D point pairs for this camera\n",
    "            match_data = all_matches[cam_idx - 1]\n",
    "            frame_points_3d = refined_points_3d[cam_idx - 1]  # Use refined points\n",
    "            image_points = match_data.pts2.astype(np.float32)\n",
    "            object_points = frame_points_3d.astype(np.float32)\n",
    "            \n",
    "            # Need at least 6 points for reliable pose estimation\n",
    "            if len(object_points) < 6:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Initialize with current pose estimate\n",
    "                R_current, t_current = refined_poses[cam_idx]\n",
    "                rvec_init, _ = cv2.Rodrigues(R_current)\n",
    "                tvec_init = t_current.flatten()\n",
    "                \n",
    "                # Use PnP RANSAC to optimize the camera pose\n",
    "                success, rvec_refined, tvec_refined, inliers = cv2.solvePnPRansac(\n",
    "                    object_points.reshape(-1, 1, 3),\n",
    "                    image_points.reshape(-1, 1, 2),\n",
    "                    K,\n",
    "                    None,\n",
    "                    rvec_init,\n",
    "                    tvec_init,\n",
    "                    useExtrinsicGuess=True,\n",
    "                    iterationsCount=100,\n",
    "                    reprojectionError=2.0,\n",
    "                    confidence=0.99,\n",
    "                    flags=cv2.SOLVEPNP_ITERATIVE\n",
    "                )\n",
    "                \n",
    "                # Only update pose if we have a good solution with sufficient inliers\n",
    "                if success and inliers is not None and len(inliers) > len(object_points) * 0.5:\n",
    "                    R_refined, _ = cv2.Rodrigues(rvec_refined)\n",
    "                    t_refined = tvec_refined.reshape(3, 1)\n",
    "                    \n",
    "                    refined_poses[cam_idx] = (R_refined, t_refined)\n",
    "                    pose_updates += 1\n",
    "                    \n",
    "            except cv2.error as e:\n",
    "                print(f\"Camera {cam_idx}: PnP failed - {e}\")\n",
    "                continue\n",
    "                \n",
    "        # Early stopping if very few poses were updated\n",
    "        if pose_updates < len(camera_poses) * 0.2:\n",
    "            print(\"Early stopping due to few pose updates\")\n",
    "            break\n",
    "    \n",
    "    return refined_poses, refined_points_3d\n",
    "\n",
    "def analyze_reconstruction_quality(points_3d, points1, points2, P1, P2, baseline_length):\n",
    "    # Function to analyze quality metrics for 3D reconstruction\n",
    "\n",
    "    # Convert 3D points to homogeneous coordinates by adding 1 as fourth dimension\n",
    "    # Project 3D points back into 2D using both camera matrices\n",
    "    points_3d_hom = np.hstack([points_3d, np.ones((len(points_3d), 1))])\n",
    "    reproj1 = (P1 @ points_3d_hom.T)\n",
    "    reproj2 = (P2 @ points_3d_hom.T)\n",
    "\n",
    "    # Convert back from homogeneous to euclidean coordinates\n",
    "    reproj1 = (reproj1[:2, :] / reproj1[2, :]).T\n",
    "    reproj2 = (reproj2[:2, :] / reproj2[2, :]).T\n",
    "    \n",
    "    # Calculate reprojection error as L2 distance between original and reprojected points\n",
    "    error1 = np.linalg.norm(reproj1 - points1, axis=1)\n",
    "    error2 = np.linalg.norm(reproj2 - points2, axis=1)\n",
    "    mean_reproj_error = (np.mean(error1) + np.mean(error2)) / 2\n",
    "    \n",
    "    # Calculate distances of reconstructed 3D points from origin\n",
    "    distances = np.linalg.norm(points_3d, axis=1)\n",
    "        \n",
    "    # Print statistics\n",
    "    print(f\"\\nReconstruction Quality Metrics:\")\n",
    "    print(f\"Points: {len(points_3d)}\")\n",
    "    print(f\"Reprojection error: {mean_reproj_error:.2f} pixels\")\n",
    "    print(f\"Baseline: {baseline_length:.3f}\")\n",
    "    print(f\"Scale ratio: {np.max(distances)/baseline_length:.1f}x\")\n",
    "    \n",
    "    # Return dictionary with key metrics\n",
    "    return {\n",
    "        'mean_reprojection_error': mean_reproj_error,\n",
    "        'num_points': len(points_3d),\n",
    "        'scale_ratio': np.max(distances)/baseline_length\n",
    "    }\n",
    "\n",
    "def save_point_cloud_ply(filename, points, colors):\n",
    "    # Save 3D points with colors to a .ply file\n",
    "\n",
    "    # Create reconstructions directory if it doesn't exist\n",
    "    reconstruction_dir = 'reconstructions'\n",
    "    os.makedirs(reconstruction_dir, exist_ok=True)\n",
    "\n",
    "    # Join path with filename\n",
    "    filepath = os.path.join(reconstruction_dir, filename)\n",
    "\n",
    "    # Flip coordinates for correct orientation\n",
    "    points = points.copy()\n",
    "    points[:, 1] = -points[:, 1]  # Flip Y coordinate\n",
    "    points[:, 2] = -points[:, 2]  # Flip Z coordinate\n",
    "\n",
    "    # Ensure points and colors are float32 / uint8\n",
    "    points = points.astype(np.float32)\n",
    "    colors = (colors * 255).astype(np.uint8)  # If colors were normalized [0,1]\n",
    "\n",
    "    # Create a structured array\n",
    "    vertex_data = np.array(\n",
    "        [tuple(p) + tuple(c) for p, c in zip(points, colors)],\n",
    "        dtype=[\n",
    "            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "            ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    el = plyfile.PlyElement.describe(vertex_data, 'vertex')\n",
    "    plyfile.PlyData([el]).write(filepath)\n",
    "    print(f\"Saved point cloud to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fc37fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_scene(pose_estimations, ransac_results, Frames, detector_name, use_bundle_adjustment=True, visualize=True):\n",
    "    # Initialize first camera pose\n",
    "    R_global = np.eye(3)\n",
    "    t_global = np.zeros((3, 1))\n",
    "    camera_poses = [(R_global, t_global)] # Store camera poses as list of (R,t) tuples, starting with initial pose\n",
    "\n",
    "    # Lists to store 3D points and their RGB colors\n",
    "    all_points_3d = []\n",
    "    all_colors = []\n",
    "\n",
    "    # List to store reconstruction quality metrics\n",
    "    all_quality_metrics = []\n",
    "\n",
    "    # Get camera intrinsic matrix K from first pose estimation result\n",
    "    if not pose_estimations:\n",
    "        print(\"No pose estimations available to proceed with triangulation.\")\n",
    "        exit()\n",
    "        \n",
    "    K = pose_estimations[0]['K']\n",
    "\n",
    "    # Process each pose estimation sequentially to build up 3D reconstruction\n",
    "    for i, pose_result in enumerate(pose_estimations):\n",
    "        # Extract relative pose (R,t) between current frame pair\n",
    "        R_rel = pose_result.get(\"R\")\n",
    "        t_rel = pose_result.get(\"t\")\n",
    "\n",
    "        # Skip if invalid pose\n",
    "        if R_rel is None or t_rel is None:\n",
    "            print(f\"Skipping pair {ransac_results[i].frame_pair} due to invalid pose.\")\n",
    "            continue\n",
    "\n",
    "        # Get current RANSAC result\n",
    "        ransac_res = ransac_results[i]\n",
    "        \n",
    "        # Calculate global pose\n",
    "        R_new_global = R_global @ R_rel\n",
    "        t_new_global = t_global + R_global @ t_rel\n",
    "        \n",
    "        # Initial 3D point triangulation\n",
    "        points_3d = triangulate_points(\n",
    "            K,\n",
    "            (R_global, t_global),\n",
    "            (R_new_global, t_new_global),\n",
    "            ransac_res.pts1,\n",
    "            ransac_res.pts2\n",
    "        )\n",
    "        \n",
    "        # Get RGB color values for each 3D point from first frame\n",
    "        frame_idx_1, _ = ransac_res.frame_pair\n",
    "        colors = get_pixel_colors(Frames[frame_idx_1], ransac_res.pts1)\n",
    "\n",
    "        # Store results for this frame pair\n",
    "        all_points_3d.append(points_3d)\n",
    "        all_colors.append(colors)\n",
    "        \n",
    "        print(f\"Triangulated {len(points_3d)} points.\")\n",
    "\n",
    "        # Update global pose for next iteration\n",
    "        R_global = R_new_global\n",
    "        t_global = t_new_global\n",
    "        camera_poses.append((R_global, t_global))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # Apply bundle adjustment\n",
    "    if use_bundle_adjustment and len(camera_poses) > 1 and all_points_3d:\n",
    "        print(\"\\nApplying bundle adjustment...\")\n",
    "        refined_poses, refined_points_3d = bundle_adjust_poses(\n",
    "            camera_poses, all_points_3d, ransac_results, K\n",
    "        )\n",
    "        \n",
    "        # Use refined results\n",
    "        camera_poses = refined_poses\n",
    "        all_points_3d = refined_points_3d\n",
    "        \n",
    "        print(\"Bundle adjustment completed.\")\n",
    "\n",
    "    # Calculate reconstruction quality metrics \n",
    "    for i in range(len(ransac_results)):\n",
    "        if i + 1 >= len(camera_poses):\n",
    "            continue\n",
    "            \n",
    "        ransac_res = ransac_results[i]\n",
    "        R1, t1 = camera_poses[i]\n",
    "        R2, t2 = camera_poses[i + 1]\n",
    "        \n",
    "        # Use existing points if available, otherwise triangulate\n",
    "        if i < len(all_points_3d):\n",
    "            points_3d = all_points_3d[i]\n",
    "        else:\n",
    "            points_3d = triangulate_points(\n",
    "                K, (R1, t1), (R2, t2),\n",
    "                ransac_res.pts1, ransac_res.pts2\n",
    "            )\n",
    "            all_points_3d.append(points_3d)\n",
    "        \n",
    "        # Get colors if not already stored\n",
    "        if i >= len(all_colors):\n",
    "            colors = get_pixel_colors(Frames[ransac_res.frame_pair[0]], ransac_res.pts1)\n",
    "            all_colors.append(colors)\n",
    "        \n",
    "        # Construct projection matrices for quality analysis\n",
    "        P1 = K @ np.hstack((R1, t1))\n",
    "        P2 = K @ np.hstack((R2, t2))\n",
    "        \n",
    "        # Analyze quality\n",
    "        quality = analyze_reconstruction_quality(\n",
    "            points_3d,\n",
    "            ransac_res.pts1,\n",
    "            ransac_res.pts2,\n",
    "            P1, P2,\n",
    "            np.linalg.norm(t2 - t1)\n",
    "        )\n",
    "        \n",
    "        all_quality_metrics.append(quality)\n",
    "        \n",
    "    # Compute overall reconstruction quality metrics across all frame pairs\n",
    "    if all_quality_metrics:\n",
    "        avg_error = np.mean([m['mean_reprojection_error'] for m in all_quality_metrics])\n",
    "        avg_scale = np.mean([m['scale_ratio'] for m in all_quality_metrics])\n",
    "        total_points = sum([m['num_points'] for m in all_quality_metrics])\n",
    "\n",
    "        print(f\"\\n{detector_name} Final Reconstruction Quality Summary:\")\n",
    "        print(f\"Average Reprojection Error: {avg_error:.2f} pixels\")\n",
    "        print(f\"Average Scale Ratio: {avg_scale:.1f}x\")\n",
    "        print(f\"Total Reconstructed Points: {total_points}\")\n",
    "\n",
    "    # Combine all frame pairs results into final point cloud\n",
    "    if all_points_3d:\n",
    "        final_points_3d = np.concatenate(all_points_3d, axis=0)\n",
    "        final_colors = np.concatenate(all_colors, axis=0)\n",
    "        \n",
    "        # Display final combined point cloud\n",
    "        print(f\"\\nTotal reconstructed points: {len(final_points_3d)}\")\n",
    "        \n",
    "        if visualize:\n",
    "            visualize_reconstruction(final_points_3d, final_colors, window_name=f'{detector_name} 3D Scene Reconstruction')\n",
    "        save_point_cloud_ply(f\"{detector_name}_reconstruction.ply\", final_points_3d, final_colors)\n",
    "\n",
    "    else:\n",
    "        print(\"Could not reconstruct any 3D points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310a735",
   "metadata": {},
   "source": [
    "# Main Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "de3b324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"../videos/vid3.mp4\"\n",
    "NUMBER_OF_FRAME_PAIRS = 11\n",
    "\n",
    "PLOT_DETECTOR_BENCHMARK_COMPARISONS = False\n",
    "PLOT_FEATURE_DETECTOR_PAIRS = False\n",
    "PLOT_RANSAC_FILTERING = False\n",
    "PLOT_FUNDAMENTAL_MATRIX_COMPUTATION = False\n",
    "PLOT_CAMERA_POSE_ESTIMATIONS = False\n",
    "VISUALIZE_3D_RECONTRUCTION = True # Toggle Open3D interactive window display (point clouds export is always enabled) \n",
    "\n",
    "ALGORITHM_NAME_FUNC_MAP = {\"ORB\" : detect_orb,\n",
    "                            \"SIFT\": detect_sift,\n",
    "                            \"FAST\": detect_fast,\n",
    "                            \"SHI-TOMASI\": detect_shi_tomasi,\n",
    "                            \"HARRIS\": detect_harris}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a33a3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame size for reading (480, 848) is different from the source frame size (848, 480).\n",
      "The frame size for reading (480, 848) is different from the source frame size (848, 480).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlapping indices: [0, 24, 48, 72, 96, 121, 145, 169, 193, 217, 242]\n",
      "\n",
      "Finished loading selected frames.\n",
      "Total frames processed: 243\n"
     ]
    }
   ],
   "source": [
    "# Frame preparation\n",
    "indices = get_frame_indices(VIDEO_PATH, NUMBER_OF_FRAME_PAIRS)\n",
    "print(f\"Overlapping indices: {indices}\")\n",
    "Frames = load_specific_frames(VIDEO_PATH, selected_indices=indices, display_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7fc3d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_DETECTOR_BENCHMARK_COMPARISONS:\n",
    "    benchmark_results = benchmark_detectors_with_repeatability(Frames)\n",
    "    compare_detectors_per_frame(Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "19988e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running:ORB, func:detect_orb\n",
      "[ORB] Matched 0 â†’ 1 : 618 matches\n",
      "[ORB] Matched 1 â†’ 2 : 632 matches\n",
      "[ORB] Matched 2 â†’ 3 : 687 matches\n",
      "[ORB] Matched 3 â†’ 4 : 709 matches\n",
      "[ORB] Matched 4 â†’ 5 : 605 matches\n",
      "[ORB] Matched 5 â†’ 6 : 481 matches\n",
      "[ORB] Matched 6 â†’ 7 : 466 matches\n",
      "[ORB] Matched 7 â†’ 8 : 476 matches\n",
      "[ORB] Matched 8 â†’ 9 : 486 matches\n",
      "[ORB] Matched 9 â†’ 10 : 627 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 485 out of 525 (92.38%)\n",
      "Mean epipolar error: 1.0397 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 479 out of 507 (94.48%)\n",
      "Mean epipolar error: 0.9738 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 573 out of 618 (92.72%)\n",
      "Mean epipolar error: 0.8250 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 561 out of 632 (88.77%)\n",
      "Mean epipolar error: 0.9194 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 468 out of 539 (86.83%)\n",
      "Mean epipolar error: 1.1521 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 393 out of 431 (91.18%)\n",
      "Mean epipolar error: 1.1231 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 373 out of 392 (95.15%)\n",
      "Mean epipolar error: 0.9024 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 381 out of 406 (93.84%)\n",
      "Mean epipolar error: 1.0265 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 401 out of 415 (96.63%)\n",
      "Mean epipolar error: 0.8319 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 587 out of 609 (96.39%)\n",
      "Mean epipolar error: 1.0530 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[-0.05752937 -1.75908298  0.07081747]\n",
      " [ 2.31985069  0.1791269  -9.2082737 ]\n",
      " [ 0.27547432  8.69025452 -0.31512769]]\n",
      "Essential matrix singular values: [9.60367136e+00 8.76207781e+00 1.47745891e-15]\n",
      "Pose Inliers: 525/525\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.532171639667608e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 3.032920379844713\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  5.87407336 -13.95752436  32.4192328 ]\n",
      " [ 12.03418509  -0.69594625   5.66715989]\n",
      " [-38.10577923  -4.89263268  -2.4925038 ]]\n",
      "Essential matrix singular values: [4.25995734e+01 3.35443545e+01 2.15921280e-14]\n",
      "Pose Inliers: 507/507\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.883723044707993e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 8.523131662610187\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[-0.16741803 -3.9146239   0.18665821]\n",
      " [ 4.55725785  0.03912019 -7.36469725]\n",
      " [-0.07862621  7.24766335  0.28246131]]\n",
      "Essential matrix singular values: [8.67301212e+00 8.23343771e+00 2.98501683e-16]\n",
      "Pose Inliers: 618/618\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.816618486909652e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.5294686488256914\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[   4.59942146   53.08959718  -82.76379776]\n",
      " [ -55.19235615   -1.22726015  129.40107279]\n",
      " [  75.81946665 -128.33037468   -1.14262282]]\n",
      "Essential matrix singular values: [1.63932267e+02 1.57564717e+02 1.80726794e-14]\n",
      "Pose Inliers: 631/632\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.218146423288331e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 2.702318634575458\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.31811526 -20.05788335 -11.75134525]\n",
      " [ 21.70176397   0.310527   -35.44079087]\n",
      " [ 12.28449271  36.37086687   0.20200722]]\n",
      "Essential matrix singular values: [4.36655145e+01 4.28338535e+01 2.14904885e-15]\n",
      "Pose Inliers: 539/539\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999996\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 9.016251913944648e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 2.3400036024086397\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.70171053  -5.9707361   -6.21631264]\n",
      " [  8.36324689  -0.17796937 -22.15388531]\n",
      " [  6.43538447  23.22528594   0.04571019]]\n",
      "Essential matrix singular values: [2.49304462e+01 2.43897080e+01 4.04487651e-15]\n",
      "Pose Inliers: 431/431\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.0354786589271861e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 6.339611769463504\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.99835558  -1.05667224   8.74250644]\n",
      " [ 10.56508393   0.25807656 -84.02082564]\n",
      " [ -8.51686581  89.0944344   -1.72608041]]\n",
      "Essential matrix singular values: [8.95789233e+01 8.50804474e+01 5.48627757e-15]\n",
      "Pose Inliers: 392/392\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.1808252892620665e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 6.5501457630610735\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 3.43548724e-01  1.75169595e-01 -3.82810876e+00]\n",
      " [ 2.07826498e+01 -3.05899383e+00 -2.45667590e+02]\n",
      " [ 2.17416936e+01  2.50023723e+02  4.26971600e+00]]\n",
      "Essential matrix singular values: [2.52297466e+02 2.45270109e+02 2.42108950e-15]\n",
      "Pose Inliers: 406/406\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.247111566614018e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 6.449717666370496\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.15998619 -12.58623184  -3.80017885]\n",
      " [ 10.39624002  -0.36034448  24.386875  ]\n",
      " [  3.25396586 -26.77217822   0.38175211]]\n",
      "Essential matrix singular values: [2.97588505e+01 2.67900333e+01 1.97979657e-16]\n",
      "Pose Inliers: 415/415\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 7.862848373999009e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 3.2459029094243532\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.08154605   0.63247788  -0.86975187]\n",
      " [ -2.19906388  -0.10001617  21.16376674]\n",
      " [  0.20795018 -22.10163467   0.98613982]]\n",
      "Essential matrix singular values: [2.23872340e+01 2.10291174e+01 4.90712884e-16]\n",
      "Pose Inliers: 609/609\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999999\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.663945104019561e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.728380017851062\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 525 points.\n",
      "Triangulated 507 points.\n",
      "Triangulated 618 points.\n",
      "Triangulated 632 points.\n",
      "Triangulated 539 points.\n",
      "Triangulated 431 points.\n",
      "Triangulated 392 points.\n",
      "Triangulated 406 points.\n",
      "Triangulated 415 points.\n",
      "Triangulated 609 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 525\n",
      "Reprojection error: 3.08 pixels\n",
      "Baseline: 0.687\n",
      "Scale ratio: 9.8x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 507\n",
      "Reprojection error: 10.05 pixels\n",
      "Baseline: 0.465\n",
      "Scale ratio: 14.6x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 618\n",
      "Reprojection error: 14.16 pixels\n",
      "Baseline: 0.883\n",
      "Scale ratio: 27.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 632\n",
      "Reprojection error: 3.64 pixels\n",
      "Baseline: 1.370\n",
      "Scale ratio: 100.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 539\n",
      "Reprojection error: 1.91 pixels\n",
      "Baseline: 0.936\n",
      "Scale ratio: 39.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 431\n",
      "Reprojection error: 4.06 pixels\n",
      "Baseline: 0.806\n",
      "Scale ratio: 13.4x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 392\n",
      "Reprojection error: 3.84 pixels\n",
      "Baseline: 1.170\n",
      "Scale ratio: 8.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 406\n",
      "Reprojection error: 1.46 pixels\n",
      "Baseline: 1.044\n",
      "Scale ratio: 10.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 415\n",
      "Reprojection error: 7.11 pixels\n",
      "Baseline: 0.287\n",
      "Scale ratio: 40.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 609\n",
      "Reprojection error: 9.18 pixels\n",
      "Baseline: 1.633\n",
      "Scale ratio: 14.6x\n",
      "\n",
      "ORB Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 5.85 pixels\n",
      "Average Scale Ratio: 27.8x\n",
      "Total Reconstructed Points: 5074\n",
      "\n",
      "Total reconstructed points: 5074\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\ORB_reconstruction.ply\n",
      "\n",
      "Running:SIFT, func:detect_sift\n",
      "[SIFT] Matched 0 â†’ 1 : 595 matches\n",
      "[SIFT] Matched 1 â†’ 2 : 713 matches\n",
      "[SIFT] Matched 2 â†’ 3 : 764 matches\n",
      "[SIFT] Matched 3 â†’ 4 : 769 matches\n",
      "[SIFT] Matched 4 â†’ 5 : 714 matches\n",
      "[SIFT] Matched 5 â†’ 6 : 658 matches\n",
      "[SIFT] Matched 6 â†’ 7 : 671 matches\n",
      "[SIFT] Matched 7 â†’ 8 : 620 matches\n",
      "[SIFT] Matched 8 â†’ 9 : 663 matches\n",
      "[SIFT] Matched 9 â†’ 10 : 742 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 553 out of 557 (99.28%)\n",
      "Mean epipolar error: 0.3216 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 638 out of 641 (99.53%)\n",
      "Mean epipolar error: 0.3523 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 670 out of 672 (99.70%)\n",
      "Mean epipolar error: 0.2738 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 597 out of 597 (100.00%)\n",
      "Mean epipolar error: 0.4976 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 529 out of 529 (100.00%)\n",
      "Mean epipolar error: 0.2709 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 514 out of 515 (99.81%)\n",
      "Mean epipolar error: 0.5046 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 556 out of 557 (99.82%)\n",
      "Mean epipolar error: 0.4511 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 491 out of 496 (98.99%)\n",
      "Mean epipolar error: 0.4199 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 438 out of 442 (99.10%)\n",
      "Mean epipolar error: 0.4385 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 613 out of 615 (99.67%)\n",
      "Mean epipolar error: 0.5495 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.09498639   5.32719455   8.08080188]\n",
      " [ -4.5157322   -0.1027586  -48.25681923]\n",
      " [ -7.64545898  47.62424269  -0.12279335]]\n",
      "Essential matrix singular values: [4.91473547e+01 4.85168478e+01 1.47132690e-15]\n",
      "Pose Inliers: 557/557\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999999\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.18521277407288e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 1.0772371871308752\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[-0.07002094  0.99238662  0.78598422]\n",
      " [-0.06617559  0.06276101 -8.64720818]\n",
      " [-0.64922483  8.51119272 -0.11699378]]\n",
      "Essential matrix singular values: [8.70337584e+00 8.57419756e+00 9.67627278e-17]\n",
      "Pose Inliers: 484/641\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.508665967248044e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 6.336855121164938\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.08469076  1.6289963  -0.35476092]\n",
      " [-1.09212101  0.15046535 -7.81641352]\n",
      " [ 0.68426067  7.76294365  0.29552121]]\n",
      "Essential matrix singular values: [8.01869316e+00 7.84969711e+00 3.15007325e-16]\n",
      "Pose Inliers: 672/672\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.009404292249236e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.536009639452128\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 1.27753800e-02  4.26598542e+00  2.11764654e+00]\n",
      " [-3.42836927e+00  1.04513281e-01 -1.54043311e+01]\n",
      " [-1.62382720e+00  1.51873817e+01  1.39520593e-02]]\n",
      "Essential matrix singular values: [1.59617198e+01 1.58195561e+01 2.26165780e-16]\n",
      "Pose Inliers: 597/597\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.0426946353555502e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 3.5735947201037748\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.0652254    0.07244759   0.67643489]\n",
      " [  0.96363621   0.05468465 -10.60825114]\n",
      " [ -0.53774972  10.68233028   0.06701936]]\n",
      "Essential matrix singular values: [1.07195378e+01 1.06503990e+01 5.94392084e-16]\n",
      "Pose Inliers: 529/529\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.93329832515728e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.663278373680703\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[   3.20863393  -92.37780294  -40.19325437]\n",
      " [ 110.70914036   -0.31100405 -254.73129196]\n",
      " [  40.58078352  270.96485528    2.91787061]]\n",
      "Essential matrix singular values: [2.89137254e+02 2.80679547e+02 2.34810005e-14]\n",
      "Pose Inliers: 515/515\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.898032729187953e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.677808723407835\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.92726235  13.54101515  -8.09822495]\n",
      " [-17.62233786   0.08079469  46.42818465]\n",
      " [  7.38223996 -48.72978223   0.88147534]]\n",
      "Essential matrix singular values: [5.11560209e+01 5.02877468e+01 9.22784760e-16]\n",
      "Pose Inliers: 557/557\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.517493867873421e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 5.339464448937266\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.42107426  -3.9567921    1.60417465]\n",
      " [  9.53092065  -0.35668883 -63.98146393]\n",
      " [  2.83505904  65.1065131    1.03668652]]\n",
      "Essential matrix singular values: [6.55226987e+01 6.44805906e+01 2.55585907e-15]\n",
      "Pose Inliers: 496/496\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999997\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 7.806512181283707e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 6.460857737701345\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.21628707   2.09661243  -2.07468698]\n",
      " [ -3.25886634   0.19917283  18.30095547]\n",
      " [  1.29812152 -18.85764473   0.36507522]]\n",
      "Essential matrix singular values: [1.90435870e+01 1.86842901e+01 3.33972473e-15]\n",
      "Pose Inliers: 442/442\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.531382678085208e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 4.4212337756361215\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.29557285  2.46393356 -2.9253082 ]\n",
      " [-2.79725158  0.07692415  6.09174557]\n",
      " [ 2.58808256 -6.46567298  0.26566131]]\n",
      "Essential matrix singular values: [7.44378579e+00 7.26769463e+00 1.20004914e-15]\n",
      "Pose Inliers: 615/615\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999997\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 9.123182710786915e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.649884934558578\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 557 points.\n",
      "Triangulated 641 points.\n",
      "Triangulated 672 points.\n",
      "Triangulated 597 points.\n",
      "Triangulated 529 points.\n",
      "Triangulated 515 points.\n",
      "Triangulated 557 points.\n",
      "Triangulated 496 points.\n",
      "Triangulated 442 points.\n",
      "Triangulated 615 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 557\n",
      "Reprojection error: 0.68 pixels\n",
      "Baseline: 1.006\n",
      "Scale ratio: 12.4x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 641\n",
      "Reprojection error: 0.93 pixels\n",
      "Baseline: 0.957\n",
      "Scale ratio: 690.8x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 672\n",
      "Reprojection error: 1.43 pixels\n",
      "Baseline: 1.048\n",
      "Scale ratio: 44.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 597\n",
      "Reprojection error: 1.03 pixels\n",
      "Baseline: 1.020\n",
      "Scale ratio: 20.6x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 529\n",
      "Reprojection error: 0.69 pixels\n",
      "Baseline: 0.977\n",
      "Scale ratio: 19.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 515\n",
      "Reprojection error: 1.32 pixels\n",
      "Baseline: 1.027\n",
      "Scale ratio: 15.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 557\n",
      "Reprojection error: 1.56 pixels\n",
      "Baseline: 1.049\n",
      "Scale ratio: 15.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 496\n",
      "Reprojection error: 2.51 pixels\n",
      "Baseline: 1.023\n",
      "Scale ratio: 12.6x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 442\n",
      "Reprojection error: 2.85 pixels\n",
      "Baseline: 0.951\n",
      "Scale ratio: 11.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 615\n",
      "Reprojection error: 2.39 pixels\n",
      "Baseline: 0.978\n",
      "Scale ratio: 29.3x\n",
      "\n",
      "SIFT Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 1.54 pixels\n",
      "Average Scale Ratio: 87.2x\n",
      "Total Reconstructed Points: 5621\n",
      "\n",
      "Total reconstructed points: 5621\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\SIFT_reconstruction.ply\n",
      "\n",
      "Running:FAST, func:detect_fast\n",
      "[FAST] Matched 0 â†’ 1 : 696 matches\n",
      "[FAST] Matched 1 â†’ 2 : 766 matches\n",
      "[FAST] Matched 2 â†’ 3 : 871 matches\n",
      "[FAST] Matched 3 â†’ 4 : 879 matches\n",
      "[FAST] Matched 4 â†’ 5 : 798 matches\n",
      "[FAST] Matched 5 â†’ 6 : 705 matches\n",
      "[FAST] Matched 6 â†’ 7 : 863 matches\n",
      "[FAST] Matched 7 â†’ 8 : 761 matches\n",
      "[FAST] Matched 8 â†’ 9 : 920 matches\n",
      "[FAST] Matched 9 â†’ 10 : 1062 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 567 out of 607 (93.41%)\n",
      "Mean epipolar error: 0.9766 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 638 out of 653 (97.70%)\n",
      "Mean epipolar error: 0.8485 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 814 out of 837 (97.25%)\n",
      "Mean epipolar error: 0.7072 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 693 out of 793 (87.39%)\n",
      "Mean epipolar error: 0.9863 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 656 out of 673 (97.47%)\n",
      "Mean epipolar error: 0.7401 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 608 out of 643 (94.56%)\n",
      "Mean epipolar error: 0.7498 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 633 out of 737 (85.89%)\n",
      "Mean epipolar error: 1.0454 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 575 out of 603 (95.36%)\n",
      "Mean epipolar error: 0.6501 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 563 out of 599 (93.99%)\n",
      "Mean epipolar error: 0.7530 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 756 out of 927 (81.55%)\n",
      "Mean epipolar error: 1.2364 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.0763392    2.85231296   1.31935205]\n",
      " [ -2.17966264   0.01710756 -16.75587935]\n",
      " [ -1.45130146  16.49891277  -0.13577428]]\n",
      "Essential matrix singular values: [1.69692104e+01 1.67862340e+01 1.42716220e-16]\n",
      "Pose Inliers: 607/607\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000007\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.394918887682974e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 2.4553191390679503\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.16198322  2.38025513 -0.85842456]\n",
      " [-1.69049691 -0.00779307 -6.81987562]\n",
      " [ 1.04215134  6.58464453  0.02355925]]\n",
      "Essential matrix singular values: [7.08374817e+00 7.07544909e+00 1.29495575e-16]\n",
      "Pose Inliers: 372/653\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.136884389091736e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 6.244851102608886\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.06371999  2.39879393  0.374477  ]\n",
      " [-1.86009524  0.2031965  -6.75031025]\n",
      " [-0.04120986  6.72534543  0.25060144]]\n",
      "Essential matrix singular values: [7.17689267e+00 6.98236488e+00 2.99530738e-15]\n",
      "Pose Inliers: 837/837\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.083524032777643e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.988355324628011\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.36461722  23.17116731   6.80694296]\n",
      " [-21.64495261  -0.14976041 -38.13109162]\n",
      " [ -5.66021662  37.97681028   0.14373341]]\n",
      "Essential matrix singular values: [4.51926122e+01 4.40204679e+01 3.08624469e-15]\n",
      "Pose Inliers: 793/793\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.918635436232309e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 2.051350553290794\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[-0.03651477 -0.09948909  0.35325059]\n",
      " [ 1.10163279  0.03977452 -9.95382839]\n",
      " [-0.27351552 10.04001144  0.08399345]]\n",
      "Essential matrix singular values: [1.00697065e+01 9.99572906e+00 2.50206416e-17]\n",
      "Pose Inliers: 673/673\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999996\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.571200186324607e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999998\n",
      "rotation_angle_deg: 5.7895483190412795\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  -1.08763614   43.24723643   14.27796613]\n",
      " [ -52.09925121    1.07614318  128.25594294]\n",
      " [ -14.21077099 -136.01706267   -1.59887833]]\n",
      "Essential matrix singular values: [1.43585315e+02 1.39028216e+02 2.41023976e-14]\n",
      "Pose Inliers: 643/643\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999991\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.0061650884215284e-15\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 4.547461391903019\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.09379368   5.71429438   2.09757899]\n",
      " [ -7.04513758   0.33148907  17.05023452]\n",
      " [ -2.61416455 -17.76364357   0.46689741]]\n",
      "Essential matrix singular values: [1.89485405e+01 1.84680220e+01 1.13976197e-15]\n",
      "Pose Inliers: 737/737\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999997\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.932395191996297e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.9568742188931125\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 2.82169808e-02 -5.41275968e-01 -8.03308737e-01]\n",
      " [ 7.88356780e+00 -2.15046109e-01 -9.19633627e+01]\n",
      " [ 6.97732460e+00  9.39913574e+01  1.22780398e+00]]\n",
      "Essential matrix singular values: [9.43454339e+01 9.22165982e+01 3.64802846e-15]\n",
      "Pose Inliers: 603/603\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 1.230796720365337e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 5.955156692702915\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[   1.75280463    0.36934549  -20.35914026]\n",
      " [  -9.09987015   -0.7218196   103.57602829]\n",
      " [  16.94076683 -108.20801501    1.47372167]]\n",
      "Essential matrix singular values: [1.09576567e+02 1.05925151e+02 3.26773370e-16]\n",
      "Pose Inliers: 599/599\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.624604659037936e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.395879565394545\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.34405366   3.54865395  -3.08785764]\n",
      " [  0.81017243   0.54351779 -43.25063458]\n",
      " [  3.69819042  44.91457599  -2.01144369]]\n",
      "Essential matrix singular values: [4.59907023e+01 4.25877015e+01 6.03596890e-15]\n",
      "Pose Inliers: 927/927\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.993153736804006e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 5.576146702688194\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 607 points.\n",
      "Triangulated 653 points.\n",
      "Triangulated 837 points.\n",
      "Triangulated 793 points.\n",
      "Triangulated 673 points.\n",
      "Triangulated 643 points.\n",
      "Triangulated 737 points.\n",
      "Triangulated 603 points.\n",
      "Triangulated 599 points.\n",
      "Triangulated 927 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 607\n",
      "Reprojection error: 0.73 pixels\n",
      "Baseline: 0.957\n",
      "Scale ratio: 9.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 653\n",
      "Reprojection error: 1.66 pixels\n",
      "Baseline: 1.026\n",
      "Scale ratio: 25098.8x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 837\n",
      "Reprojection error: 1.37 pixels\n",
      "Baseline: 0.952\n",
      "Scale ratio: 21.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 793\n",
      "Reprojection error: 2.15 pixels\n",
      "Baseline: 1.038\n",
      "Scale ratio: 44.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 673\n",
      "Reprojection error: 1.47 pixels\n",
      "Baseline: 0.998\n",
      "Scale ratio: 16.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 643\n",
      "Reprojection error: 1.96 pixels\n",
      "Baseline: 0.981\n",
      "Scale ratio: 16.8x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 737\n",
      "Reprojection error: 2.53 pixels\n",
      "Baseline: 1.068\n",
      "Scale ratio: 15.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 603\n",
      "Reprojection error: 3.34 pixels\n",
      "Baseline: 1.055\n",
      "Scale ratio: 10.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 599\n",
      "Reprojection error: 3.64 pixels\n",
      "Baseline: 0.938\n",
      "Scale ratio: 12.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 927\n",
      "Reprojection error: 3.81 pixels\n",
      "Baseline: 0.915\n",
      "Scale ratio: 24.8x\n",
      "\n",
      "FAST Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 2.27 pixels\n",
      "Average Scale Ratio: 2527.0x\n",
      "Total Reconstructed Points: 7072\n",
      "\n",
      "Total reconstructed points: 7072\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\FAST_reconstruction.ply\n",
      "\n",
      "Running:SHI-TOMASI, func:detect_shi_tomasi\n",
      "[SHI-TOMASI] Matched 0 â†’ 1 : 324 matches\n",
      "[SHI-TOMASI] Matched 1 â†’ 2 : 350 matches\n",
      "[SHI-TOMASI] Matched 2 â†’ 3 : 391 matches\n",
      "[SHI-TOMASI] Matched 3 â†’ 4 : 396 matches\n",
      "[SHI-TOMASI] Matched 4 â†’ 5 : 351 matches\n",
      "[SHI-TOMASI] Matched 5 â†’ 6 : 315 matches\n",
      "[SHI-TOMASI] Matched 6 â†’ 7 : 324 matches\n",
      "[SHI-TOMASI] Matched 7 â†’ 8 : 284 matches\n",
      "[SHI-TOMASI] Matched 8 â†’ 9 : 320 matches\n",
      "[SHI-TOMASI] Matched 9 â†’ 10 : 366 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 238 out of 253 (94.07%)\n",
      "Mean epipolar error: 0.6831 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 253 out of 274 (92.34%)\n",
      "Mean epipolar error: 0.7303 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 337 out of 340 (99.12%)\n",
      "Mean epipolar error: 0.5747 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 308 out of 324 (95.06%)\n",
      "Mean epipolar error: 0.8774 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 238 out of 266 (89.47%)\n",
      "Mean epipolar error: 0.9453 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 214 out of 242 (88.43%)\n",
      "Mean epipolar error: 1.1364 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 238 out of 246 (96.75%)\n",
      "Mean epipolar error: 0.7913 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 246 out of 255 (96.47%)\n",
      "Mean epipolar error: 0.7871 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 277 out of 281 (98.58%)\n",
      "Mean epipolar error: 0.5306 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 295 out of 318 (92.77%)\n",
      "Mean epipolar error: 0.9399 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.16753649   9.57159325  10.29470281]\n",
      " [ -8.30588023   0.05901771 -39.18519436]\n",
      " [ -9.55348631  39.15939408   0.20737769]]\n",
      "Essential matrix singular values: [4.16993609e+01 4.10856241e+01 1.47157669e-14]\n",
      "Pose Inliers: 253/253\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.096496727838265e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 1.916763245741593\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[-0.01106065  2.06244943  0.40722072]\n",
      " [-1.32772154  0.04642048 -5.41006417]\n",
      " [-0.29391862  5.18376885 -0.06288398]]\n",
      "Essential matrix singular values: [5.59778984e+00 5.57494335e+00 6.48066032e-15]\n",
      "Pose Inliers: 274/274\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 7.314914440332114e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 8.051493173467247\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.03998311  0.94958623 -0.1144754 ]\n",
      " [-0.40196565  0.11677718 -7.41958956]\n",
      " [ 0.37498814  7.35377944  0.30291139]]\n",
      "Essential matrix singular values: [7.52548328e+00 7.33617643e+00 4.51142295e-16]\n",
      "Pose Inliers: 340/340\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999999\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 2.6658316608246557e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.828736762345463\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[-0.10146448  1.08661197  1.2896879 ]\n",
      " [-0.3620627   0.0764729  -7.74642851]\n",
      " [-1.11006102  7.56083173  0.05177013]]\n",
      "Essential matrix singular values: [7.87795138e+00 7.70307613e+00 1.21184576e-15]\n",
      "Pose Inliers: 324/324\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 7.260416097697698e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999998\n",
      "rotation_angle_deg: 5.691039100530019\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.02504075  0.97615274 -0.19385703]\n",
      " [-0.20065304  0.02508058 -5.65884053]\n",
      " [ 0.18626494  5.61661823  0.06943111]]\n",
      "Essential matrix singular values: [5.70573979e+00 5.66435324e+00 6.39942949e-17]\n",
      "Pose Inliers: 266/266\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999994\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.462718172550419e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 7.832172390163538\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -1.52592596  34.26268323  22.73476677]\n",
      " [-42.19150525   1.43399098  88.57931398]\n",
      " [-25.28469436 -93.07911396  -0.54583772]]\n",
      "Essential matrix singular values: [1.02388655e+02 1.00705026e+02 1.03956691e-14]\n",
      "Pose Inliers: 242/242\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.679485431885583e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 5.52607177875702\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  2.9254188   -4.85943833 -27.60251114]\n",
      " [ -4.15234553   0.35966265  80.60513942]\n",
      " [ 27.06063854 -85.41317294   1.05700842]]\n",
      "Essential matrix singular values: [9.00159154e+01 8.50562824e+01 1.44293210e-14]\n",
      "Pose Inliers: 246/246\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 7.627424089089449e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 6.057637167455706\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.46091504  -5.78032895   1.11814873]\n",
      " [  9.72314484  -0.60258786 -42.48322893]\n",
      " [  1.33551146  42.95614889   0.80129715]]\n",
      "Essential matrix singular values: [4.39595800e+01 4.30094643e+01 8.64108782e-16]\n",
      "Pose Inliers: 255/255\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.267642973500281e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 6.359773559196121\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.31329369   3.2732743   -3.1437782 ]\n",
      " [ -4.49377789   0.18028051  20.77978926]\n",
      " [  2.29090454 -21.32509541   0.36014554]]\n",
      "Essential matrix singular values: [2.17324517e+01 2.14606660e+01 4.90168558e-16]\n",
      "Pose Inliers: 281/281\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.846890414396782e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.123785146912165\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.47733878  1.74471985 -5.18127904]\n",
      " [-2.09981841 -0.01762824  5.82302005]\n",
      " [ 5.00902154 -6.34982506  0.27439436]]\n",
      "Essential matrix singular values: [8.26026686e+00 8.10484556e+00 1.81484449e-15]\n",
      "Pose Inliers: 318/318\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 9.968430286714865e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 5.481297667544832\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 253 points.\n",
      "Triangulated 274 points.\n",
      "Triangulated 340 points.\n",
      "Triangulated 324 points.\n",
      "Triangulated 266 points.\n",
      "Triangulated 242 points.\n",
      "Triangulated 246 points.\n",
      "Triangulated 255 points.\n",
      "Triangulated 281 points.\n",
      "Triangulated 318 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 253\n",
      "Reprojection error: 0.72 pixels\n",
      "Baseline: 0.975\n",
      "Scale ratio: 10.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 274\n",
      "Reprojection error: 0.83 pixels\n",
      "Baseline: 0.985\n",
      "Scale ratio: 41.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 340\n",
      "Reprojection error: 1.00 pixels\n",
      "Baseline: 1.050\n",
      "Scale ratio: 26.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 324\n",
      "Reprojection error: 2.07 pixels\n",
      "Baseline: 0.951\n",
      "Scale ratio: 12.5x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 266\n",
      "Reprojection error: 1.86 pixels\n",
      "Baseline: 1.014\n",
      "Scale ratio: 11.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 242\n",
      "Reprojection error: 0.97 pixels\n",
      "Baseline: 1.025\n",
      "Scale ratio: 13.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 246\n",
      "Reprojection error: 2.89 pixels\n",
      "Baseline: 0.867\n",
      "Scale ratio: 15.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 255\n",
      "Reprojection error: 2.22 pixels\n",
      "Baseline: 1.211\n",
      "Scale ratio: 9.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 281\n",
      "Reprojection error: 3.57 pixels\n",
      "Baseline: 0.851\n",
      "Scale ratio: 19.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 318\n",
      "Reprojection error: 2.49 pixels\n",
      "Baseline: 1.143\n",
      "Scale ratio: 23.8x\n",
      "\n",
      "SHI-TOMASI Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 1.86 pixels\n",
      "Average Scale Ratio: 18.3x\n",
      "Total Reconstructed Points: 2799\n",
      "\n",
      "Total reconstructed points: 2799\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\SHI-TOMASI_reconstruction.ply\n",
      "\n",
      "Running:HARRIS, func:detect_harris\n",
      "[HARRIS] Matched 0 â†’ 1 : 2864 matches\n",
      "[HARRIS] Matched 1 â†’ 2 : 1980 matches\n",
      "[HARRIS] Matched 2 â†’ 3 : 4484 matches\n",
      "[HARRIS] Matched 3 â†’ 4 : 3826 matches\n",
      "[HARRIS] Matched 4 â†’ 5 : 2726 matches\n",
      "[HARRIS] Matched 5 â†’ 6 : 911 matches\n",
      "[HARRIS] Matched 6 â†’ 7 : 1172 matches\n",
      "[HARRIS] Matched 7 â†’ 8 : 1300 matches\n",
      "[HARRIS] Matched 8 â†’ 9 : 1782 matches\n",
      "[HARRIS] Matched 9 â†’ 10 : 2394 matches\n",
      "Number of match results: 10\n",
      "\n",
      "Fundamental Matrix and Epipolar Geometry:\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (0, 1)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 2502 out of 2530 (98.89%)\n",
      "Mean epipolar error: 0.8743 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (1, 2)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 1934 out of 1939 (99.74%)\n",
      "Mean epipolar error: 0.4230 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (2, 3)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 4281 out of 4282 (99.98%)\n",
      "Mean epipolar error: 0.4051 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (3, 4)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 3149 out of 3324 (94.74%)\n",
      "Mean epipolar error: 0.7759 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (4, 5)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 2395 out of 2413 (99.25%)\n",
      "Mean epipolar error: 0.5678 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (5, 6)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 779 out of 827 (94.20%)\n",
      "Mean epipolar error: 0.7941 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (6, 7)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 1056 out of 1127 (93.70%)\n",
      "Mean epipolar error: 0.8846 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (7, 8)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 1142 out of 1213 (94.15%)\n",
      "Mean epipolar error: 0.8224 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (8, 9)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 1683 out of 1748 (96.28%)\n",
      "Mean epipolar error: 0.7293 pixels\n",
      "\n",
      "Fundamental Matrix Computation for Frame Pair (9, 10)\n",
      "Computing fundamental matrix...\n",
      "Inlier matches: 2167 out of 2310 (93.81%)\n",
      "Mean epipolar error: 0.7505 pixels\n",
      "\n",
      "Camera Pose Estimation:\n",
      "\n",
      "Pose Estimation for Frame Pair (0, 1)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.11914367  -3.73729844   1.72091928]\n",
      " [  3.51859369   0.01477917 -11.92579202]\n",
      " [ -1.89045428  11.21045232   0.03037475]]\n",
      "Essential matrix singular values: [1.25514229e+01 1.19690917e+01 1.38235856e-16]\n",
      "Pose Inliers: 2530/2530\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999992\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.79590175455962e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 2.382023518869049\n",
      "\n",
      "Pose Estimation for Frame Pair (1, 2)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[-0.1127235   0.76157237  1.12338246]\n",
      " [ 0.33844118  0.01197062 -9.60621135]\n",
      " [-0.89732338  9.42391623 -0.17351836]]\n",
      "Essential matrix singular values: [9.68413526e+00 9.49271768e+00 7.53826685e-16]\n",
      "Pose Inliers: 1676/1939\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.120265965489484e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 6.784026032718409\n",
      "\n",
      "Pose Estimation for Frame Pair (2, 3)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.0790157  -0.33540943 -1.26378905]\n",
      " [ 0.85469453  0.09218178 -6.90032492]\n",
      " [ 1.46436603  6.79476758  0.25473024]]\n",
      "Essential matrix singular values: [7.07203677e+00 6.95944020e+00 5.13408648e-16]\n",
      "Pose Inliers: 4282/4282\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000002\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.5914866126050514e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 4.704674849980401\n",
      "\n",
      "Pose Estimation for Frame Pair (3, 4)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[  0.05335939 -36.30857444 -18.76097449]\n",
      " [ 36.68777697  -0.1558282   10.67086511]\n",
      " [ 18.49916437 -11.57334883  -0.5638111 ]]\n",
      "Essential matrix singular values: [4.27372811e+01 4.21921602e+01 2.71178752e-15]\n",
      "Pose Inliers: 1146/3324\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 4.0319894609394277e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 1.406617122512543\n",
      "\n",
      "Pose Estimation for Frame Pair (4, 5)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 9.23450353e-03  5.35597169e+00  2.06780293e-01]\n",
      " [-4.22262704e+00 -5.17219144e-02 -1.21991684e+01]\n",
      " [-1.08293701e-01  1.22832279e+01  1.00233059e-01]]\n",
      "Essential matrix singular values: [1.34179533e+01 1.28934181e+01 3.46676665e-15]\n",
      "Pose Inliers: 2413/2413\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000007\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 8.260004796957307e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 4.463120103152463\n",
      "\n",
      "Pose Estimation for Frame Pair (5, 6)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -0.24086709 -12.07750796   2.47716315]\n",
      " [ 13.03394523  -0.06757149 -11.89233423]\n",
      " [ -2.85944896  12.15907057   0.33936455]]\n",
      "Essential matrix singular values: [1.79597987e+01 1.72324318e+01 9.54979207e-15]\n",
      "Pose Inliers: 827/827\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0000000000000004\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.148589212628986e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 3.187797322158749\n",
      "\n",
      "Pose Estimation for Frame Pair (6, 7)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 1.02270722e+00  3.05958904e+01 -1.12236986e+01]\n",
      " [-3.08982083e+01 -2.18625051e-02  2.00874740e+01]\n",
      " [ 1.05464191e+01 -2.01781194e+01  1.09911182e-01]]\n",
      "Essential matrix singular values: [3.88693401e+01 3.78008221e+01 8.53079942e-15]\n",
      "Pose Inliers: 1127/1127\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 1.0\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 2.8943789905202334e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 1.1657139313795692\n",
      "\n",
      "Pose Estimation for Frame Pair (7, 8)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ -10.80001564 -282.19133484  -97.27033781]\n",
      " [ 255.38360902   -8.1522542   348.46933629]\n",
      " [  71.9371273  -380.20638279  -12.2364389 ]]\n",
      "Essential matrix singular values: [4.82374106e+02 4.39459930e+02 1.86284199e-14]\n",
      "Pose Inliers: 1213/1213\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 5.082356375782598e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 2.6738661954195266\n",
      "\n",
      "Pose Estimation for Frame Pair (8, 9)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[-8.64285147e-02  2.46262676e+01  7.83999917e+00]\n",
      " [-1.41680647e+01  1.39885566e+00 -8.17748936e+01]\n",
      " [-5.48162005e+00  8.81341340e+01 -1.96804852e+00]]\n",
      "Essential matrix singular values: [9.17912451e+01 8.32686284e+01 2.68511790e-15]\n",
      "Pose Inliers: 1748/1748\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999998\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 3.9040006415817525e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 1.0\n",
      "rotation_angle_deg: 6.360647314195728\n",
      "\n",
      "Pose Estimation for Frame Pair (9, 10)\n",
      "Camera Pose Estimation: \n",
      "Intrinsics fx = fy = 312.8, cx = 240.0, cy = 424.0\n",
      "Essential Matrix\n",
      " [[ 0.17899531  2.45340167 -2.34188298]\n",
      " [-2.45553844  0.13936315  3.16666463]\n",
      " [ 2.07216204 -3.38960205  0.13154625]]\n",
      "Essential matrix singular values: [4.73084228e+00 4.58608595e+00 8.25806721e-15]\n",
      "Pose Inliers: 1517/2310\n",
      "\n",
      "Pose Validation: \n",
      "det_R: 0.9999999999999994\n",
      "is_rotation_valid: True\n",
      "orthogonality_error: 6.374136151159385e-16\n",
      "is_orthogonal: True\n",
      "translation_magnitude: 0.9999999999999999\n",
      "rotation_angle_deg: 3.6340013363105803\n",
      "\n",
      "3D Point Triangulation and Scene Visualisation:\n",
      "Triangulated 2530 points.\n",
      "Triangulated 1939 points.\n",
      "Triangulated 4282 points.\n",
      "Triangulated 3324 points.\n",
      "Triangulated 2413 points.\n",
      "Triangulated 827 points.\n",
      "Triangulated 1127 points.\n",
      "Triangulated 1213 points.\n",
      "Triangulated 1748 points.\n",
      "Triangulated 2310 points.\n",
      "\n",
      "\n",
      "Applying bundle adjustment...\n",
      "Starting bundle adjustment with 11 poses...\n",
      "Bundle adjustment completed.\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 2530\n",
      "Reprojection error: 2.17 pixels\n",
      "Baseline: 0.912\n",
      "Scale ratio: 24.4x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 1939\n",
      "Reprojection error: 2.00 pixels\n",
      "Baseline: 0.947\n",
      "Scale ratio: 127.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 4282\n",
      "Reprojection error: 3.06 pixels\n",
      "Baseline: 1.083\n",
      "Scale ratio: 16.1x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 3324\n",
      "Reprojection error: 0.74 pixels\n",
      "Baseline: 0.965\n",
      "Scale ratio: 188849.6x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 2413\n",
      "Reprojection error: 1.33 pixels\n",
      "Baseline: 0.841\n",
      "Scale ratio: 25.0x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 827\n",
      "Reprojection error: 1.41 pixels\n",
      "Baseline: 0.967\n",
      "Scale ratio: 16.9x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 1127\n",
      "Reprojection error: 0.83 pixels\n",
      "Baseline: 1.013\n",
      "Scale ratio: 53.7x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 1213\n",
      "Reprojection error: 5.85 pixels\n",
      "Baseline: 0.699\n",
      "Scale ratio: 57.4x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 1748\n",
      "Reprojection error: 8.12 pixels\n",
      "Baseline: 1.251\n",
      "Scale ratio: 7.2x\n",
      "\n",
      "Reconstruction Quality Metrics:\n",
      "Points: 2310\n",
      "Reprojection error: 1.60 pixels\n",
      "Baseline: 1.038\n",
      "Scale ratio: 7861.2x\n",
      "\n",
      "HARRIS Final Reconstruction Quality Summary:\n",
      "Average Reprojection Error: 2.71 pixels\n",
      "Average Scale Ratio: 19703.9x\n",
      "Total Reconstructed Points: 21713\n",
      "\n",
      "Total reconstructed points: 21713\n",
      "\n",
      "Visualizing point cloud.\n",
      "Controls: Mouse drag=rotate, Shift+drag=pan, Scroll=zoom, Ctrl+drag=Translate\n",
      "Press 'q' to close the window.\n",
      "Saved point cloud to reconstructions\\HARRIS_reconstruction.ply\n"
     ]
    }
   ],
   "source": [
    "# Loop through each detector type and function pair to process feature matching\n",
    "for detector_type, detector_func in ALGORITHM_NAME_FUNC_MAP.items():\n",
    "    print(f\"\\nRunning:{detector_type}, func:{detector_func.__name__}\")\n",
    "    match_results = match_features(Frames, detector_type, detector_func)\n",
    "\n",
    "    # Check if any valid matches were found\n",
    "    if not match_results:\n",
    "        raise RuntimeError(\"No valid matches found!\")\n",
    "    \n",
    "    if PLOT_FEATURE_DETECTOR_PAIRS:\n",
    "        display_feature_detector_pairs(Frames, match_results, detector_type)  \n",
    "\n",
    "    # Apply RANSAC filtering to each match result\n",
    "    ransac_results = [apply_ransac_filter(res, reproj_thresh=16) for res in match_results]\n",
    "\n",
    "    print(f\"Number of match results: {len(match_results)}\")\n",
    "\n",
    "    # Visualize matches before and after RANSAC filtering\n",
    "    for idx in range(len(ransac_results)):        \n",
    "        before = match_results[idx]\n",
    "        after = ransac_results[idx]\n",
    "        i, j = before.frame_pair\n",
    "        if PLOT_RANSAC_FILTERING:\n",
    "            print(f\"\\nVisualising Matches: Frame Pair ({i}, {j})\")\n",
    "            plot_before_after_ransac(Frames[i], Frames[j], before, after)\n",
    "  \n",
    "    # Fundamental Matrix Computation and Epipolar Geometry\n",
    "    print(\"\\nFundamental Matrix and Epipolar Geometry:\")\n",
    "    fundamental_results_list = []\n",
    "\n",
    "    # Process each RANSAC result to compute fundamental matrix\n",
    "    for idx in range(len(ransac_results)):\n",
    "        res = ransac_results[idx]\n",
    "        i, j = res.frame_pair\n",
    "        print(f\"\\nFundamental Matrix Computation for Frame Pair ({i}, {j})\")\n",
    "        imgs = [Frames[i], Frames[j]]\n",
    "        fundamental_result = process_fundamental_matrix(imgs, res.matches, res.kp1, res.kp2, visualise=PLOT_FUNDAMENTAL_MATRIX_COMPUTATION)\n",
    "        fundamental_results_list.append(fundamental_result)\n",
    "\n",
    "    # Camera Pose Estimation\n",
    "    print(\"\\nCamera Pose Estimation:\")\n",
    "    pose_estimations = []\n",
    "    frame_shape = (Frames[0].shape[0], Frames[0].shape[1])  # Get frame dimensions\n",
    "\n",
    "    # Process each fundamental matrix result and RANSAC result\n",
    "    for idx, (fundamental_data, res) in enumerate(zip(fundamental_results_list, ransac_results)):\n",
    "        print(f\"\\nPose Estimation for Frame Pair {res.frame_pair}\")\n",
    "        pose_result = run_pose_estimation_from_matches(\n",
    "            match_result = res, \n",
    "            frame_shape = frame_shape, \n",
    "            fundamental_matrix = fundamental_data['fundamental_matrix'],\n",
    "            visualise=PLOT_CAMERA_POSE_ESTIMATIONS\n",
    "        )\n",
    "        pose_estimations.append(pose_result)\n",
    "\n",
    "    # 3D Point Triangulation and Scene Visualisation\n",
    "    print(\"\\n3D Point Triangulation and Scene Visualisation:\")\n",
    "    reconstruct_scene(pose_estimations, ransac_results, Frames, detector_name=detector_type, use_bundle_adjustment=True, visualize=VISUALIZE_3D_RECONTRUCTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b949f",
   "metadata": {},
   "source": [
    "# 8.Evaluation and Analysis "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
